{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Address Matching with Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iniciar Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/15 15:29:27 WARN Utils: Your hostname, ubuntu resolves to a loopback address: 127.0.1.1; using 10.6.130.30 instead (on interface ens3)\n",
      "24/05/15 15:29:27 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/05/15 15:29:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/05/15 15:29:28 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n",
      "24/05/15 15:29:29 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.session import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"Adress Matching\")\n",
    "    .config(\"spark.executor.memory\", \"2g\")\n",
    "    .config(\"spark.driver.memory\", \"20g\")\n",
    "    .config(\"spark.driver.maxResultSize\", \"4g\")\n",
    "    .config(\"spark.local.dir\", \"/spark-tmp\") \\\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark.app.name: Adress Matching\n",
      "spark.executor.memory: 2g\n",
      "spark.driver.extraJavaOptions: -Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false\n",
      "spark.app.id: local-1715786969547\n",
      "spark.driver.port: 46085\n",
      "spark.executor.id: driver\n",
      "spark.driver.host: 10.6.130.30\n",
      "spark.app.submitTime: 1715786967743\n",
      "spark.driver.memory: 20g\n",
      "spark.rdd.compress: True\n",
      "spark.executor.extraJavaOptions: -Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false\n",
      "spark.local.dir: /spark-tmp\n",
      "spark.serializer.objectStreamReset: 100\n",
      "spark.master: local[*]\n",
      "spark.submit.pyFiles: \n",
      "spark.submit.deployMode: client\n",
      "spark.app.startTime: 1715786967964\n",
      "spark.ui.showConsoleProgress: true\n",
      "spark.driver.maxResultSize: 4g\n"
     ]
    }
   ],
   "source": [
    "conf = spark.sparkContext.getConf()\n",
    "for key, value in conf.getAll():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lectura de ficheros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El dataset continene exactamente 24 columnas las cuales, el ***, almacena información relevante el trabajo que realizan. En cambio, en este proyecto, se utilizarán las siguientes columnas pues contienen la información más relevante:\n",
    "- **uuid_idt**: contiene un identificador alfanumérico único para cada dirección.\n",
    "- **tvia**: contiene el tipo de vía, es decir, si es una calle, avenida, carretera, etc.\n",
    "- **nvia**: contiene el nombre de la vía.\n",
    "- **numer**: contiene el número de la vía.\n",
    "- **codmun**: está compuesto por un código numérico que identifica a un municipio, es decir, el código postal.\n",
    "- **nommun**: contiene el nombre del municipio.\n",
    "- **direccion**: contiene la dirección completa.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+----------+------------+--------------------+-----+------+------+--------------------+\n",
      "|            uuid_idt|  latitud|  longitud|        tvia|                nvia|numer|codmun|nommun|           direccion|\n",
      "+--------------------+---------+----------+------------+--------------------+-----+------+------+--------------------+\n",
      "|027C0FF8-B17B-11E...| 28.10351| -15.71037|       CALLE|        FUENTE SANTA|    5| 35001|Agaete|CALLE FUENTE SANT...|\n",
      "|028F2302-B17B-11E...| 28.10011| -15.69867|       CALLE|       CRUZ CHIQUITA|    2| 35001|Agaete|CALLE CRUZ CHIQUI...|\n",
      "|02918516-B17B-11E...| 28.10106| -15.70524|URBANIZACION|RESIDENCIAL PALMERAL|    9| 35001|Agaete|URBANIZACION RESI...|\n",
      "|03A306C0-7525-11E...| 28.10225| -15.69982|       CALLE|SEÑORITA MARIA MA...|    2| 35001|Agaete|CALLE SEÑORITA MA...|\n",
      "|03A306C0-7525-11E...| 28.10225| -15.69982|       CALLE|SRTA M MANRIQUE LARA|    2| 35001|Agaete|CALLE SRTA M MANR...|\n",
      "|03A306C0-7525-11E...| 28.10225| -15.69982|       CALLE| MARIA MANRIQUE LARA|    0| 35001|Agaete|CALLE MARIA MANRI...|\n",
      "|03A306C0-7525-11E...| 28.10225| -15.69982|       CALLE| MARIA MANRIQUE LARA|    2| 35001|Agaete|CALLE MARIA MANRI...|\n",
      "|03A306C0-7525-11E...| 28.10225| -15.69982|       CALLE|  ST M MANRIQUE LARA|    2| 35001|Agaete|CALLE ST M MANRIQ...|\n",
      "|03A306C0-7525-11E...| 28.10225| -15.69982|       CALLE|       MANRIQUE LARA|    2| 35001|Agaete|CALLE MANRIQUE LA...|\n",
      "|05C1069E-02BB-443...| 28.04658| -15.72287|       CALLE|        RISCO AGAETE|   19| 35001|Agaete|CALLE RISCO AGAET...|\n",
      "|05C1069E-02BB-443...| 28.04658| -15.72287|       CALLE|               RISCO|   19| 35001|Agaete|CALLE RISCO 19 AG...|\n",
      "|0767670E-D385-4FF...|28.104062|-15.704751|URBANIZACION|            ELTURMAN|   14| 35001|Agaete|URBANIZACION ELTU...|\n",
      "|0767670E-D385-4FF...|28.104062|-15.704751|       LUGAR|              TURMAN|   78| 35001|Agaete|LUGAR TURMAN 78 A...|\n",
      "|07D6EE06-78EE-4FC...| 28.10563| -15.70737|URBANIZACION|TURMAN C ALFREDO ...|  212| 35001|Agaete|URBANIZACION TURM...|\n",
      "|07E3A094-4FAF-4E9...| 28.09957| -15.71215|       OTROS|   ESTACION MARITIMA|    0| 35001|Agaete|ESTACION MARITIMA...|\n",
      "|09DB1BCD-BADF-4F6...| 28.10398| -15.71158|     AVENIDA|ALCALDE JOSE ARMA...|    7| 35001|Agaete|AVENIDA ALCALDE J...|\n",
      "|0DD7DD10-3553-411...| 28.10059| -15.70661|       PASEO|PALMERAL PINTOR G...|    6| 35001|Agaete|PASEO PALMERAL PI...|\n",
      "|0DD7DD10-3553-411...| 28.10059| -15.70661|       CALLE|PINTOR GARCIA ALV...|    8| 35001|Agaete|CALLE PINTOR GARC...|\n",
      "|0FBDCFF8-1183-4B0...| 28.09839| -15.70138|URBANIZACION|         CANDELARIAS|   17| 35001|Agaete|URBANIZACION CAND...|\n",
      "|0FBDCFF8-1183-4B0...| 28.09839| -15.70138|URBANIZACION|         CANDELARIAS|   18| 35001|Agaete|URBANIZACION CAND...|\n",
      "+--------------------+---------+----------+------------+--------------------+-----+------+------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1784217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Leer el archivo CSV y cargarlo en un DataFrame\n",
    "file = \"../data/raw_data/TFM_Direcciones.tab\"\n",
    "first_df = (\n",
    "    spark.read.option(\"delimiter\", \"\\t\")\n",
    "    .option(\"encoding\", \"windows-1252\")\n",
    "    .csv(file, header=True, inferSchema=True)\n",
    ")\n",
    "\n",
    "# Seleccionar solo las columnas deseadas\n",
    "selected_columns = [\n",
    "    \"uuid_idt\",\n",
    "    \"latitud\",\n",
    "    \"longitud\",\n",
    "    \"tvia\",\n",
    "    \"nvia\",\n",
    "    \"numer\",\n",
    "    \"codmun\",\n",
    "    \"nommun\",\n",
    "    \"direccion\",\n",
    "]\n",
    "first_df = first_df.select(selected_columns)\n",
    "\n",
    "# Mostrar el DataFrame\n",
    "first_df.show()\n",
    "first_df.schema\n",
    "print(first_df.count())\n",
    "size_df = first_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+----------+------+--------------------+--------------------+\n",
      "|            uuid_idt|  latitud|  longitud|codmun|              nommun|           direccion|\n",
      "+--------------------+---------+----------+------+--------------------+--------------------+\n",
      "|C0CF3B94-0AD1-11E...| 28.09315| -15.44562| 35016|LAS PALMAS DE GRA...|IGUAZU 42 0 LAS P...|\n",
      "|73B57C1B-3251-11E...|28.100397|-15.446634| 35016|LAS PALMAS DE GRA...|PARQUE CENTRAL BL...|\n",
      "|C70E3EC6-3EDA-11E...| 28.12209| -15.43886| 35016|LAS PALMAS DE GRA...|CONCEJAL GARCIA F...|\n",
      "|687BA81E-3251-11E...|28.128481|-15.508722| 35006|              ARUCAS|PEDRO MORALES DEN...|\n",
      "|74921989-3251-11E...|27.765042|-15.586671| 35019|SAN BARTOLOMÉ DE ...|ISLA LOBOS 19 0 S...|\n",
      "|6C0D19FF-3251-11E...|28.112275|-15.419253| 35016|LAS PALMAS DE GRA...|CALLE LEON Y CAST...|\n",
      "|6360FDCD-3251-11E...|28.116229|-15.443305| 35016|LAS PALMAS DE GRA...|VIGEN PILAR 45 4 ...|\n",
      "|758B3855-3251-11E...|28.091282|-15.460183| 35016|LAS PALMAS DE GRA...|CALLE PALMERA CAN...|\n",
      "|5D36841A-7A6B-11E...|  28.0939| -15.44672| 35016|LAS PALMAS DE GRA...|RUPERTO CHAPI 0 L...|\n",
      "|6C00DBF4-3251-11E...|28.035619|-15.498084| 35021|       SANTA BRÍGIDA|MANUEL HERNANDEZ ...|\n",
      "|733A49C5-3251-11E...|28.060993|-15.548180| 35027|               TEROR| HERRERIA 36 0 TEROR|\n",
      "|C0A2DCAF-0AD1-11E...| 28.02296| -15.51652| 35021|       SANTA BRÍGIDA|SUBIDA JOSE MEDER...|\n",
      "|785998D5-3251-11E...|28.101843|-15.433498| 35016|LAS PALMAS DE GRA...|CARMEN QUINTANA 6...|\n",
      "|6D52F776-3251-11E...|28.113051|-15.424184| 35016|LAS PALMAS DE GRA...|TOMAS MORALES 100...|\n",
      "|710F6DCB-3251-11E...|28.319675|-16.412695| 38020|              GÜÍMAR|CALLE CHACONA CAL...|\n",
      "|D721C91D-B743-11E...| 28.42103| -16.32089| 38038|SANTA CRUZ DE TEN...|AZAHAR CHORRILLO ...|\n",
      "|6FA00078-3251-11E...|28.154215|-15.423482| 35016|LAS PALMAS DE GRA...|BANDAMA 76 0 LAS ...|\n",
      "|72AB493C-3251-11E...|28.108815|-15.436455| 35016|LAS PALMAS DE GRA...|REALIDAD 39 0 LAS...|\n",
      "|6AD2FD7E-3251-11E...|28.962313|-13.566829| 35004|            ARRECIFE|AGUSTIN ESPINOSA ...|\n",
      "|65BA189F-3251-11E...|27.992807|-15.491055| 35031|VALSEQUILLO DE GR...|PILON 3 VALSEQUIL...|\n",
      "+--------------------+---------+----------+------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "630297\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import upper\n",
    "\n",
    "file = \"../data/raw_data/data-09022024.csv\"\n",
    "second_df = spark.read.option(\"header\", True).csv(file)\n",
    "\n",
    "selected_columns = [\"uuid_idt\", \"latitud\", \"longitud\", \"codmun\", \"nommun\", \"direccion\"]\n",
    "\n",
    "second_df = second_df.select(selected_columns)\n",
    "second_df = second_df.select(\n",
    "    upper(\"uuid_idt\").alias(\"uuid_idt\"),\n",
    "    \"latitud\",\n",
    "    \"longitud\",\n",
    "    \"codmun\",\n",
    "    upper(\"nommun\").alias(\"nommun\"),\n",
    "    \"direccion\",\n",
    ")\n",
    "second_df.show()\n",
    "first_df.schema\n",
    "print(second_df.count())\n",
    "size_df += second_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2414514\n"
     ]
    }
   ],
   "source": [
    "print(size_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+----------+\n",
      "|Número de direcciones asociadas|Frecuencia|\n",
      "+-------------------------------+----------+\n",
      "|                              1|    267275|\n",
      "|                              2|    115343|\n",
      "|                              3|     60757|\n",
      "|                              4|     35631|\n",
      "|                              5|     22207|\n",
      "|                              6|     14810|\n",
      "|                              7|     10142|\n",
      "|                              8|      7411|\n",
      "|                              9|      5414|\n",
      "|                       10 o más|     25539|\n",
      "+-------------------------------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+----------+\n",
      "|Número de direcciones asociadas|Frecuencia|\n",
      "+-------------------------------+----------+\n",
      "|                              1|    250632|\n",
      "|                              2|     51645|\n",
      "|                              3|     19692|\n",
      "|                              4|      9055|\n",
      "|                              5|      4702|\n",
      "|                              6|      2876|\n",
      "|                              7|      1772|\n",
      "|                              8|      1161|\n",
      "|                              9|       917|\n",
      "|                       10 o más|      4083|\n",
      "+-------------------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count\n",
    "\n",
    "\n",
    "def uuid_frecuency(dataframe):\n",
    "    values_under_10 = (\n",
    "        dataframe.groupBy(dataframe.uuid_idt)\n",
    "        .count()\n",
    "        .filter(\"count <= 9\")\n",
    "        .groupBy(\"count\")\n",
    "        .agg(count(\"*\").alias(\"Frecuencia\"))\n",
    "        .orderBy(\"count\")\n",
    "    )\n",
    "    values_under_10 = values_under_10.withColumnRenamed(\n",
    "        \"count\", \"Número de direcciones asociadas\"\n",
    "    )\n",
    "\n",
    "    values_over_10 = dataframe.groupBy(dataframe.uuid_idt).count().filter(\"count > 9\")\n",
    "    values_over_10 = spark.createDataFrame(\n",
    "        [[\"10 o más\", values_over_10.count()]],\n",
    "        [\"Número de direcciones asociadas\", \"Frecuencia\"],\n",
    "    )\n",
    "\n",
    "    values = values_under_10.union(values_over_10)\n",
    "    values.show()\n",
    "\n",
    "\n",
    "uuid_frecuency(first_df)\n",
    "uuid_frecuency(second_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unión de los dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+----------+------------+--------------------+-----+------+------+--------------------+\n",
      "|            uuid_idt|  latitud|  longitud|        tvia|                nvia|numer|codmun|nommun|           direccion|\n",
      "+--------------------+---------+----------+------------+--------------------+-----+------+------+--------------------+\n",
      "|027C0FF8-B17B-11E...| 28.10351| -15.71037|       CALLE|        FUENTE SANTA|    5| 35001|Agaete|CALLE FUENTE SANT...|\n",
      "|028F2302-B17B-11E...| 28.10011| -15.69867|       CALLE|       CRUZ CHIQUITA|    2| 35001|Agaete|CALLE CRUZ CHIQUI...|\n",
      "|02918516-B17B-11E...| 28.10106| -15.70524|URBANIZACION|RESIDENCIAL PALMERAL|    9| 35001|Agaete|URBANIZACION RESI...|\n",
      "|03A306C0-7525-11E...| 28.10225| -15.69982|       CALLE|SEÑORITA MARIA MA...|    2| 35001|Agaete|CALLE SEÑORITA MA...|\n",
      "|03A306C0-7525-11E...| 28.10225| -15.69982|       CALLE|SRTA M MANRIQUE LARA|    2| 35001|Agaete|CALLE SRTA M MANR...|\n",
      "|03A306C0-7525-11E...| 28.10225| -15.69982|       CALLE| MARIA MANRIQUE LARA|    0| 35001|Agaete|CALLE MARIA MANRI...|\n",
      "|03A306C0-7525-11E...| 28.10225| -15.69982|       CALLE| MARIA MANRIQUE LARA|    2| 35001|Agaete|CALLE MARIA MANRI...|\n",
      "|03A306C0-7525-11E...| 28.10225| -15.69982|       CALLE|  ST M MANRIQUE LARA|    2| 35001|Agaete|CALLE ST M MANRIQ...|\n",
      "|03A306C0-7525-11E...| 28.10225| -15.69982|       CALLE|       MANRIQUE LARA|    2| 35001|Agaete|CALLE MANRIQUE LA...|\n",
      "|05C1069E-02BB-443...| 28.04658| -15.72287|       CALLE|        RISCO AGAETE|   19| 35001|Agaete|CALLE RISCO AGAET...|\n",
      "|05C1069E-02BB-443...| 28.04658| -15.72287|       CALLE|               RISCO|   19| 35001|Agaete|CALLE RISCO 19 AG...|\n",
      "|0767670E-D385-4FF...|28.104062|-15.704751|URBANIZACION|            ELTURMAN|   14| 35001|Agaete|URBANIZACION ELTU...|\n",
      "|0767670E-D385-4FF...|28.104062|-15.704751|       LUGAR|              TURMAN|   78| 35001|Agaete|LUGAR TURMAN 78 A...|\n",
      "|07D6EE06-78EE-4FC...| 28.10563| -15.70737|URBANIZACION|TURMAN C ALFREDO ...|  212| 35001|Agaete|URBANIZACION TURM...|\n",
      "|07E3A094-4FAF-4E9...| 28.09957| -15.71215|       OTROS|   ESTACION MARITIMA|    0| 35001|Agaete|ESTACION MARITIMA...|\n",
      "|09DB1BCD-BADF-4F6...| 28.10398| -15.71158|     AVENIDA|ALCALDE JOSE ARMA...|    7| 35001|Agaete|AVENIDA ALCALDE J...|\n",
      "|0DD7DD10-3553-411...| 28.10059| -15.70661|       PASEO|PALMERAL PINTOR G...|    6| 35001|Agaete|PASEO PALMERAL PI...|\n",
      "|0DD7DD10-3553-411...| 28.10059| -15.70661|       CALLE|PINTOR GARCIA ALV...|    8| 35001|Agaete|CALLE PINTOR GARC...|\n",
      "|0FBDCFF8-1183-4B0...| 28.09839| -15.70138|URBANIZACION|         CANDELARIAS|   17| 35001|Agaete|URBANIZACION CAND...|\n",
      "|0FBDCFF8-1183-4B0...| 28.09839| -15.70138|URBANIZACION|         CANDELARIAS|   18| 35001|Agaete|URBANIZACION CAND...|\n",
      "+--------------------+---------+----------+------------+--------------------+-----+------+------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del dataframe ampliado:  2234214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 94:====================================>                     (5 + 3) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de uuid_idt nuevos del segundo dataframe:  148073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Seleccionar los UUID únicos del primer DataFrame\n",
    "unique_uuid_first_df = first_df.select(\"uuid_idt\").distinct()\n",
    "\n",
    "# Seleccionar los UUID únicos del segundo DataFrame\n",
    "unique_uuid_second_df = second_df.select(\"uuid_idt\").distinct()\n",
    "\n",
    "# Encontrar los UUID comunes\n",
    "uuid_comunes = unique_uuid_first_df.join(unique_uuid_second_df, \"uuid_idt\", \"inner\")\n",
    "# Da el mismo resultado: uuid_primero.intersect(uuid_segundo)\n",
    "\n",
    "# Juntar los UUID comunes con el primer DataFrame\n",
    "addresses_df = first_df.unionByName(\n",
    "    second_df.join(uuid_comunes, \"uuid_idt\", \"inner\"), allowMissingColumns=True\n",
    ")\n",
    "\n",
    "# Mostrar el resultado\n",
    "addresses_df.show()\n",
    "print(\"Tamaño del dataframe ampliado: \", addresses_df.count())\n",
    "\n",
    "# Si se quiere comprobar que la operación es correcta\n",
    "print(\n",
    "    \"Número de uuid_idt nuevos del segundo dataframe: \",\n",
    "    unique_uuid_second_df.subtract(uuid_comunes).count(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+----------+\n",
      "|Número de direcciones asociadas|Frecuencia|\n",
      "+-------------------------------+----------+\n",
      "|                              1|    211768|\n",
      "|                              2|    119917|\n",
      "|                              3|     70050|\n",
      "|                              4|     44000|\n",
      "|                              5|     28925|\n",
      "|                              6|     19650|\n",
      "|                              7|     14050|\n",
      "|                              8|     10331|\n",
      "|                              9|      7778|\n",
      "|                       10 o más|     38060|\n",
      "+-------------------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "uuid_frecuency(addresses_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "addresses_df = addresses_df.withColumn(\n",
    "    \"latitud\", col(\"latitud\").cast(\"float\")\n",
    ").withColumn(\"longitud\", col(\"longitud\").cast(\"float\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partición del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "addresses_df = addresses_df.repartition(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limpieza del dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tras visualizar el dataset, se observa que hay columnas que tienen valores desconocidos, representados con '_U', y camos vacíos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 180:============================>                           (8 + 8) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial dataset size: 2234214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(f\"Initial dataset size: {addresses_df.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Valores nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 203:=================================================>     (29 + 3) / 32]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+--------+------+------+------+------+------+---------+\n",
      "|uuid_idt|latitud|longitud|  tvia|  nvia| numer|codmun|nommun|direccion|\n",
      "+--------+-------+--------+------+------+------+------+------+---------+\n",
      "|       0|      0|       0|580357|460216|449997|     0|     0|        0|\n",
      "+--------+-------+--------+------+------+------+------+------+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "\n",
    "addresses_df.select(\n",
    "    [count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in addresses_df.columns]\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 221:===================================================>   (30 + 2) / 32]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+--------+----+----+------+------+------+---------+\n",
      "|uuid_idt|latitud|longitud|tvia|nvia| numer|codmun|nommun|direccion|\n",
      "+--------+-------+--------+----+----+------+------+------+---------+\n",
      "|       0|      0|       0|   0|   0|449997|     0|     0|        0|\n",
      "+--------+-------+--------+----+----+------+------+------+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "addresses_df = addresses_df.na.fill(\"_U\")\n",
    "\n",
    "addresses_df.select(\n",
    "    [count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in addresses_df.columns]\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracción del prefijo para eleminar valores '_U'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tras analizar el dataset, se observa que para una dirección, el tipo de vía (tvia) puede aparecer vacío, en cambio, en la columna 'direccion' aparece este valor. Por lo que se procederá a extraer el prefijo de la columna 'direccion' para rellenar los valores vacíos de la columna 'tvia' evitando así la perdida de información."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 234:===================================================>   (15 + 1) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unknown tvia before prefix extraction: 580357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f\"Unknown tvia before prefix extraction: {addresses_df.filter(addresses_df.tvia == '_U').count()}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 252:============================================>          (13 + 3) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'COLONIA', 'TRASERA', 'VEREDA', 'URBANIZACION', 'PORTALES', 'VIA PEATONAL', 'CENTRO COMERCIAL', 'POLIGONO', 'DC', 'ANGOSTA', 'PISTA', 'BULEVAR', 'CARRETERA', 'CARRERA', 'PASADIZO', 'PAGO', 'RU', 'CAMIN', 'RINCON', 'AMPLIACION', 'LUGAR', 'ACCESO', 'FINCA', 'CG', 'BARRIO', 'VIA', 'ESTRADA', 'EDIF', 'CORRAL', 'ESCALINATA', 'EDIFICIO', 'CONJUNTO', 'CRTA', 'PASEO ALTO', 'PASEO BAJO', 'PASAJE', 'PASEO', 'ESCALA', 'SERVENTIA', 'LLANO', 'ESCALERA', 'AUTOVIA', 'JARDIN', 'UB', 'AUTOPISTA', 'ALDEA', 'BLOQUES', 'PATIO', 'BARRA', 'ZONA', 'HOTEL', 'LOMO', 'TORRE', 'PARAJE', 'CALLEJON', 'MANZANA', 'BARRIADA', 'RONDA', 'CHALET', 'CAMNO', 'RAMBLA', 'OTROS', 'ALAMEDA', 'CSRIO', 'SOLAR', 'PARQUE', 'TRAV', 'RESIDENCIA', 'COOPERATIVA', 'PROL', 'SUBIDA', 'BCO', 'PLAYA', 'HL', 'SENDA', 'VILLAS', 'AGRUP', 'CALLE', 'BARRANCO', 'BRNCO', 'DISEMINADO', 'CRA', 'PLAZA', 'PRIVADA', 'VIVIENDAS', 'APARTAMENTOS', 'CMINO', 'RAMAL', 'AVENI', 'ATAJO', 'EXTRARRADIO', 'MONTAÑA', 'APARTADO DE CORREOS', 'MUELLE', 'BAJADA', 'SENDERO', 'PUENTE', 'PZTA', 'PASILLO', 'PLACETA', 'PROLONGACION', 'CALLEJUELA', 'LU', 'POBLADO', 'LADERA', 'CASERIO', 'TORRENTE', 'AVENIDA', 'GRUPO', 'RINCONADA', 'ACEQUIA', 'GLORIETA', 'VALLE', 'CALZADA', 'SB', 'CAS', 'MONTE', 'RAMBL', 'PARQ', 'PRJE', 'CC', 'PASO', 'POLIG', 'RESIDENCIAL', 'PLAZUELA', 'PARTICULAR', 'CUESTA', 'RUA', 'COMPLEJO', 'TRAVESIA', 'LOMA', 'SECTOR', 'C', 'CALLEJA', 'ROTONDA', 'TVERA', 'MERCADO', 'ENTRADA', 'PLAZOLETA', 'COSTANILLA', 'MUNICIPIO', 'ACERA', 'CAMINO', 'SIN DATOS DOMICILIAR', 'PASSEIG', 'TRANS', 'TRANSVERSAL', 'RIBERA', 'CALE'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "tvia_types = addresses_df.select(\"tvia\").distinct().collect()\n",
    "tvia_types = {t.tvia for t in tvia_types if t.tvia != \"_U\" and not t.tvia.isnumeric()}\n",
    "print(tvia_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_extract, when\n",
    "\n",
    "condition = (addresses_df.tvia == \"_U\") & (\n",
    "    regexp_extract(\"direccion\", r\"^(\\S+)\", 1).isin(tvia_types)\n",
    ")\n",
    "addresses_df = addresses_df.withColumn(\n",
    "    \"tvia\",\n",
    "    when(condition, regexp_extract(\"direccion\", r\"^(\\S+)\", 1)).otherwise(\n",
    "        addresses_df.tvia\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 271:===================================================>   (15 + 1) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unknown tvia after prefix extraction: 119382\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f\"Unknown tvia after prefix extraction: {addresses_df.filter(addresses_df.tvia == '_U').count()}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede observar que los tipos de vías que siguen siendo desconocidos es debido a que desde la columna de dirección no se ha podido extraer un prefijo que aparezca en la lista de los tipos de vías que tenemos en el dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limpieza de entradas con valor '_U'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tras realizar la limpieza del dataset y extraer el tipo de via de la columna 'direccion', se procederá a eliminar las entradas restantes que contengan valores '_U' en las columnas 'tvia' pues no aportan información relevante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 288:============================>                           (8 + 8) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unknown tvia before duplicated null cleaning: 119382\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f\"Unknown tvia before duplicated null cleaning: {addresses_df.filter(addresses_df.tvia == '_U').count()}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "addresses_df = addresses_df.filter(addresses_df.tvia != \"_U\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unknown tvia after duplicated null cleaning: 0\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f\"Unknown tvia after duplicated null cleaning: {addresses_df.filter(addresses_df.tvia == '_U').count()}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 306:=====================================>                 (11 + 5) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño final del dataframe: 2114832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(f\"Tamaño final del dataframe: {addresses_df.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aumento de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUMENTED_DATA = False\n",
    "MIN_FRENCUENCY = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frecuencia de uuid_idt antes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+----------+\n",
      "|Número de direcciones asociadas|Frecuencia|\n",
      "+-------------------------------+----------+\n",
      "|                              1|    220637|\n",
      "|                              2|    117018|\n",
      "|                              3|     67699|\n",
      "|                              4|     41588|\n",
      "|                              5|     27376|\n",
      "|                              6|     18221|\n",
      "|                              7|     12864|\n",
      "|                              8|      9686|\n",
      "|                              9|      7136|\n",
      "|                       10 o más|     34919|\n",
      "+-------------------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "uuid_frecuency(addresses_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aumento de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cambiar el orden de las palabras\n",
    "- Añadir errores ortográficos\n",
    "- Añadir sinónimos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "import random\n",
    "\n",
    "\n",
    "# Define la función UDF\n",
    "def switch_letters(direcction: str) -> str:\n",
    "    \"\"\"Intercambia letras adyacentes en una palabra.\n",
    "\n",
    "    Args:\n",
    "        direction: string with the direction\n",
    "    \"\"\"\n",
    "    words = direcction.split()\n",
    "    word_candidates = [word for word in words if word.isalpha() and len(word) >= 2]\n",
    "    if not word_candidates:\n",
    "        return direcction\n",
    "    word = random.choice(word_candidates)\n",
    "    pos = random.randint(0, len(word) - 2)\n",
    "    return \" \".join(\n",
    "        w if w != word else w[:pos] + w[pos + 1] + w[pos] + w[pos + 2 :] for w in words\n",
    "    )\n",
    "\n",
    "\n",
    "def update_direction_via(tvia: str, direction: str) -> str:\n",
    "    \"\"\"Actualiza el tipo de vía de la dirección\"\"\"\n",
    "    old_tvia = direction.split(\" \")[0]\n",
    "    if tvia.upper() in tvia_types:\n",
    "        direction = direction.replace(old_tvia, tvia)\n",
    "    return direction\n",
    "\n",
    "\n",
    "def select_via_type(tvia: str) -> list:\n",
    "    \"\"\"Selecciona tipos de vía alternativos sin duplicados\"\"\"\n",
    "    return random.choice(\n",
    "        [t for t in tvia_types if t.title() != tvia and t != tvia.lower()]\n",
    "    )\n",
    "\n",
    "\n",
    "unique_filtered_uuids = (\n",
    "    addresses_df.groupBy(\"uuid_idt\")\n",
    "    .count()\n",
    "    .filter(f\"count >= {MIN_FRENCUENCY}\")\n",
    "    .select(\"uuid_idt\")\n",
    "    .sample(0.5)\n",
    ")\n",
    "\n",
    "to_extend = addresses_df.join(unique_filtered_uuids, \"uuid_idt\")\n",
    "\n",
    "# Registra la función UDF con Spark\n",
    "spark.udf.register(\"switch_letters\", switch_letters, StringType())\n",
    "spark.udf.register(\"select_via_type\", select_via_type, StringType())\n",
    "spark.udf.register(\"update_direction_via\", update_direction_via, StringType())\n",
    "\n",
    "if (AUMENTED_DATA):\n",
    "    extended_ds = (\n",
    "        to_extend.withColumn(\"tvia\", udf(select_via_type, StringType())(\"tvia\"))\n",
    "        .withColumn(\n",
    "            \"direccion\", udf(update_direction_via, StringType())(\"tvia\", \"direccion\")\n",
    "        )\n",
    "        .withColumn(\"direccion\", udf(switch_letters, StringType())(\"direccion\"))\n",
    "    )\n",
    "\n",
    "    extended_ds.show(truncate=False)\n",
    "    print(extended_ds.count(), addresses_df.count())\n",
    "    addresses_df = addresses_df.union(extended_ds)\n",
    "    print(addresses_df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frecuencia de uuid_idt después"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+----------+\n",
      "|Número de direcciones asociadas|Frecuencia|\n",
      "+-------------------------------+----------+\n",
      "|                              1|    220637|\n",
      "|                              2|    117018|\n",
      "|                              3|     67699|\n",
      "|                              4|     41588|\n",
      "|                              5|     27376|\n",
      "|                              6|     18221|\n",
      "|                              7|     12864|\n",
      "|                              8|      9686|\n",
      "|                              9|      7136|\n",
      "|                       10 o más|     34919|\n",
      "+-------------------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "uuid_frecuency(addresses_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtros de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "MUNICIPIO = \"San Cristóbal de La Laguna\"\n",
    "\n",
    "filtered_uuids = (\n",
    "    addresses_df.groupBy(\"uuid_idt\")\n",
    "    .count()\n",
    "    .filter(f\"count >= {MIN_FRENCUENCY}\")\n",
    "    .select(\"uuid_idt\")\n",
    ")\n",
    "\n",
    "addresses_df = addresses_df.join(filtered_uuids, \"uuid_idt\").filter(\n",
    "    f\"nommun == '{MUNICIPIO}'\"\n",
    ")\n",
    "\n",
    "addresses_df = addresses_df.select(\"uuid_idt\", \"latitud\", \"longitud\", \"direccion\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## División de dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Obtener los uuids únicos y convertirlos a una lista de Python\n",
    "unique_uuids = [row[0] for row in addresses_df.select(\"uuid_idt\").distinct().collect()]\n",
    "\n",
    "# Crear DataFrame del 80%\n",
    "train_df = addresses_df.sampleBy(\n",
    "    \"uuid_idt\", fractions={uuid: 0.8 for uuid in unique_uuids}, seed=42\n",
    ")\n",
    "\n",
    "# Crear DataFrame del porcentaje restante restando el DataFrame del 80% al original\n",
    "test_df = addresses_df.subtract(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' from pyspark.sql.functions import rand\\n\\n# Obtener la cantidad mínima de filas por valor único en la columna \"uuid_idt\"\\nmin_count = addresses_df.groupBy(\"uuid_idt\").count().agg({\"count\": \"min\"}).collect()[0][0]\\n\\n# Crear DataFrames estratificados para el conjunto de entrenamiento y prueba\\ntrain_df_stratified = addresses_df.groupBy(\"uuid_idt\").agg(rand(seed=42).alias(\"rand\")).where(f\"rand <= 0.8\")\\ntest_df_stratified = addresses_df.groupBy(\"uuid_idt\").agg(rand(seed=42).alias(\"rand\")).where(f\"rand > 0.8\")\\n\\n# Unir las particiones estratificadas para obtener los DataFrames finales de entrenamiento y prueba\\ntrain_df = train_df_stratified.orderBy(\"uuid_idt\").limit(min_count * 0.8)\\ntest_df = test_df_stratified.orderBy(\"uuid_idt\").limit(min_count * 0.2)\\n\\n# Contar la cantidad de valores únicos en cada DataFrame\\ntrain_unique_count = train_df.select(\"uuid_idt\").distinct().count()\\ntest_unique_count = test_df.select(\"uuid_idt\").distinct().count()\\n\\nprint(\"Cantidad de valores únicos en el conjunto de entrenamiento:\", train_unique_count)\\nprint(\"Cantidad de valores únicos en el conjunto de prueba:\", test_unique_count)\\n '"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" from pyspark.sql.functions import rand\n",
    "\n",
    "# Obtener la cantidad mínima de filas por valor único en la columna \"uuid_idt\"\n",
    "min_count = addresses_df.groupBy(\"uuid_idt\").count().agg({\"count\": \"min\"}).collect()[0][0]\n",
    "\n",
    "# Crear DataFrames estratificados para el conjunto de entrenamiento y prueba\n",
    "train_df_stratified = addresses_df.groupBy(\"uuid_idt\").agg(rand(seed=42).alias(\"rand\")).where(f\"rand <= 0.8\")\n",
    "test_df_stratified = addresses_df.groupBy(\"uuid_idt\").agg(rand(seed=42).alias(\"rand\")).where(f\"rand > 0.8\")\n",
    "\n",
    "# Unir las particiones estratificadas para obtener los DataFrames finales de entrenamiento y prueba\n",
    "train_df = train_df_stratified.orderBy(\"uuid_idt\").limit(min_count * 0.8)\n",
    "test_df = test_df_stratified.orderBy(\"uuid_idt\").limit(min_count * 0.2)\n",
    "\n",
    "# Contar la cantidad de valores únicos en cada DataFrame\n",
    "train_unique_count = train_df.select(\"uuid_idt\").distinct().count()\n",
    "test_unique_count = test_df.select(\"uuid_idt\").distinct().count()\n",
    "\n",
    "print(\"Cantidad de valores únicos en el conjunto de entrenamiento:\", train_unique_count)\n",
    "print(\"Cantidad de valores únicos en el conjunto de prueba:\", test_unique_count)\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3814\n"
     ]
    }
   ],
   "source": [
    "print(addresses_df.count())\n",
    "print(train_df.count())\n",
    "print(test_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210\n"
     ]
    }
   ],
   "source": [
    "print(addresses_df.select(\"uuid_idt\").distinct().count())\n",
    "print(train_df.select(\"uuid_idt\").distinct().count())\n",
    "print(test_df.select(\"uuid_idt\").distinct().count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+----------+\n",
      "|Número de direcciones asociadas|Frecuencia|\n",
      "+-------------------------------+----------+\n",
      "|                              1|         1|\n",
      "|                              2|         1|\n",
      "|                       10 o más|       209|\n",
      "+-------------------------------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+----------+\n",
      "|Número de direcciones asociadas|Frecuencia|\n",
      "+-------------------------------+----------+\n",
      "|                              1|         1|\n",
      "|                              3|         2|\n",
      "|                              4|         4|\n",
      "|                              5|         8|\n",
      "|                              6|        11|\n",
      "|                              7|        10|\n",
      "|                              8|         9|\n",
      "|                              9|        20|\n",
      "|                       10 o más|       145|\n",
      "+-------------------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "uuid_frecuency(train_df)\n",
    "uuid_frecuency(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Escritura de los dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/14 17:00:55 WARN DAGScheduler: Broadcasting large task binary with size 28.3 MiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Escribir el resultado en un archivo parquet\n",
    "try:\n",
    "    train_df.write.format(\"parquet\").save(\n",
    "        \"../data/proccesed_data/train_dataframe2.parquet\"\n",
    "    )\n",
    "except:\n",
    "    print(\"No se pudo guardar el dataset de entrenamiento.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No se ha podido guardar el dataset de testeo.\n"
     ]
    }
   ],
   "source": [
    "# Escribir el resultado en un archivo parquet\n",
    "try:\n",
    "    test_df.write.format(\"parquet\").save(\n",
    "        \"../data/proccesed_data/test_dataframe2.parquet\"\n",
    "    )\n",
    "except:\n",
    "    print(\"No se ha podido guardar el dataset de testeo.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representación gráfica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['calle', 'sotavento', '118', 'antigua'], ['calle', 'fermina', '55', 'arrecife'], ['calle', 'b', 'villa', 'pirula', '26', 'la', 'oliva'], ['calle', 'bilbao', '55', 'arrecife'], ['calle', 'guanarteme', 'cruce', 'arinaga', '133', 'agüimes'], ['calle', 'dr', 'barnard', '3', 'ingenio'], ['calle', 'palmeral', '1', 'haria'], ['calle', 'verol', 'caleta', 'fuste', '25', 'antigua'], ['calle', 'diama', 'argana', 'alta', '41', 'arrecife'], ['calle', 'camino', 'volcan', 'macher', '7', 'arrecife']]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"direccion\", outputCol=\"words\")\n",
    "tokenized_sentences = tokenizer.transform(addresses_df).select(\"words\").collect()\n",
    "tokenized_sentences = [sentence.words for sentence in tokenized_sentences]\n",
    "print(tokenized_sentences[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+------------------+\n",
      "|     word|  count|         frequency|\n",
      "+---------+-------+------------------+\n",
      "|    calle|1163401| 9.279369890329013|\n",
      "|       de| 987371| 7.875341974077767|\n",
      "|      san| 472800| 3.771086739780658|\n",
      "|       la| 398688| 3.179964107676969|\n",
      "|    santa| 284912| 2.272478564307079|\n",
      "|     gran| 238890|1.9054037886340978|\n",
      "|  canaria| 227676|1.8159601196410768|\n",
      "|     cruz| 223350|1.7814556331006979|\n",
      "|   laguna| 211761|1.6890209371884346|\n",
      "|cristobal| 210151|1.6761794616151546|\n",
      "|   palmas| 197517|1.5754097706879362|\n",
      "|      las| 195800|1.5617148554336988|\n",
      "| tenerife| 159233|1.2700538384845463|\n",
      "|        0| 145890| 1.163629112662014|\n",
      "|  avenida| 124823|0.9955972083748754|\n",
      "|   camino| 109253|0.8714097706879362|\n",
      "|        1| 107738|0.8593260219341974|\n",
      "| tirajana| 105530| 0.841714855433699|\n",
      "|      los| 102542|0.8178823529411765|\n",
      "|carretera| 101610|0.8104486540378864|\n",
      "+---------+-------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "word_counter = dict()\n",
    "for sentence in tokenized_sentences:\n",
    "    for word in sentence:\n",
    "        if word not in word_counter:\n",
    "            word_counter[word] = 1\n",
    "        else:\n",
    "            word_counter[word] += 1\n",
    "\n",
    "word_counter = spark.createDataFrame(word_counter.items(), [\"word\", \"count\"]).orderBy(\n",
    "    \"count\", ascending=False\n",
    ")\n",
    "\n",
    "# Creo la columa frecuencia\n",
    "word_counter = word_counter.withColumn(\n",
    "    \"frequency\", word_counter[\"count\"] / word_counter.count()\n",
    ")\n",
    "word_counter.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='word'>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot\n",
    "\n",
    "# Imprimir las 20 palabras más frecuentes\n",
    "word_counter.toPandas().head(15).plot.bar(x=\"word\", y=\"frequency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representación con embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lectura del dataframe de entrenamiento y validación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    train_df = (\n",
    "        spark.read.format(\"parquet\")\n",
    "        .load(\"../data/proccesed_data/train_dataframe2.parquet\")\n",
    "        .repartition(32)\n",
    "    )\n",
    "except:\n",
    "    print(\"No se ha podido leer el dataset de entrenamiento.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    test_df = (\n",
    "        spark.read.format(\"parquet\")\n",
    "        .load(\"../data/proccesed_data/test_dataframe2.parquet\")\n",
    "        .repartition(32)\n",
    "    )\n",
    "except:\n",
    "    print(\"No se ha podido leer el dataset de testeo.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representación mediante _Word2Vec_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Word2Vec, Tokenizer\n",
    "\n",
    "# Tokenizar el texto\n",
    "tokenizer = Tokenizer(inputCol=\"direccion\", outputCol=\"words\")\n",
    "\n",
    "train_tokens = tokenizer.transform(train_df)\n",
    "test_tokens = tokenizer.transform(test_df)\n",
    "\n",
    "# Entrenar el modelo Word2Vec\n",
    "word2Vec = Word2Vec(vectorSize=200, minCount=0, inputCol=\"words\", outputCol=\"embedding\")\n",
    "word2vec_model = word2Vec.fit(train_tokens)\n",
    "\n",
    "train_embeddings_word2vec  = word2vec_model.transform(train_tokens)\n",
    "test_embeddings_word2vec  = word2vec_model.transform(test_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chroma DB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Escritura del modelo y del dataset con el embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/19 16:08:22 WARN TaskSetManager: Stage 181 contains a task of very large size (18046 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    word2vec_model.write().overwrite().save(\"../models/word2vec_model\")\n",
    "except:\n",
    "    print(\"No se ha podido guardar el modelo word2vec.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    train_embeddings_word2vec.write.format(\"parquet\").save(\n",
    "        \"../data/proccesed_data/train_embeddings_word2vec.parquet\"\n",
    "    )\n",
    "except:\n",
    "    print(\"No se ha podido guardar el dataset con embedding word2vec.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representación con _GPT_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import DenseVector, VectorUDT\n",
    "from pyspark.sql.functions import udf\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "def gtp_embedding(text, model=\"text-embedding-3-small\"):\n",
    "    client = OpenAI(api_key=\"***REMOVED***\")\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    embedding = client.embeddings.create(input=[text], model=model).data[0].embedding\n",
    "    return DenseVector(embedding)\n",
    "\n",
    "\n",
    "# Definir la función UDF y especificar el tipo de dato de retorno como VectorUDT()\n",
    "embedding_udf = udf(lambda text: gtp_embedding(text), VectorUDT())\n",
    "\n",
    "# Aplicar la función UDF al DataFrame y crear una nueva columna llamada embeddings\n",
    "gpt3_train_dense_vector = train_df.withColumn(\n",
    "    \"embedding\", embedding_udf(\"direccion\")\n",
    ")\n",
    "\n",
    "# Aplicar la función UDF al DataFrame y crear una nueva columna llamada embeddings\n",
    "gpt_test_dense_vector = test_df.withColumn(\n",
    "    \"embedding\", embedding_udf(\"direccion\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import ArrayType, FloatType\n",
    "from pyspark.sql.functions import udf\n",
    "from openai import OpenAI\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def normalize_l2(x):\n",
    "    x = np.array(x)\n",
    "    if x.ndim == 1:\n",
    "        norm = np.linalg.norm(x)\n",
    "        if norm == 0:\n",
    "            return x.tolist()  # Convertir a lista si es un vector unidimensional\n",
    "        return (x / norm).tolist()\n",
    "    else:\n",
    "        norm = np.linalg.norm(x, 2, axis=1, keepdims=True)\n",
    "        return (x / norm).tolist()  # Convertir a lista antes de devolver\n",
    "\n",
    "\n",
    "def get_embedding(text, model=\"text-embedding-3-small\"):\n",
    "    client = OpenAI(api_key=\"***REMOVED***\")\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    result = client.embeddings.create(input=[text], model=model).data[0].embedding[:256]\n",
    "    return normalize_l2(result)\n",
    "\n",
    "\n",
    "# Definir la función UDF y especificar el tipo de dato de retorno como ArrayType(FloatType())\n",
    "embedding_udf = udf(lambda text: get_embedding(text), ArrayType(FloatType()))\n",
    "\n",
    "gpt_test_list = test_df.withColumn(\"embedding\", embedding_udf(\"direccion\"))\n",
    "gpt_train_list = train_df.withColumn(\"embedding\", embedding_udf(\"direccion\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representación con _MiniLM_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from pyspark.ml.linalg import DenseVector, VectorUDT\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "@udf(returnType=VectorUDT())\n",
    "def minilm_embedding_dense_vector(text):\n",
    "    \"\"\"Calcula el embedding de un texto con MiniLM y devuelve el resultado en el formato especificado.\"\"\"\n",
    "    embedding = model.encode(text)\n",
    "    return DenseVector(embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import ArrayType, FloatType\n",
    "from pyspark.sql.functions import udf\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "\n",
    "@udf(returnType=ArrayType(FloatType()))\n",
    "def minilm_embedding_list(direccion: str):\n",
    "    \"\"\"Calcula el embedding de la direccion dada y lo convierte a una lista para evitar la serelización de Spark.\n",
    "\n",
    "    Args:\n",
    "        direccion: Direction to be represeted by the embedding model.\n",
    "\n",
    "    Returns:\n",
    "        A list with de embedding of the direction\n",
    "    \"\"\"\n",
    "    embedding = model.encode(direccion)\n",
    "    return embedding.tolist()\n",
    "\n",
    "\n",
    "minilm_train_embedding_list = train_df.withColumn(\n",
    "    \"embedding\", minilm_embedding_list(\"direccion\")\n",
    ")\n",
    "minilm_test_embedding_list = test_df.withColumn(\n",
    "    \"embedding\", minilm_embedding_list(\"direccion\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similitud de direcciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *TODO*:\n",
    "- Hacer todo por lotes. Aplicar el crossjoin por lotes y realizar las evaluaciones\n",
    "- Extraer muestras para realizar las evaluaciones\n",
    "- Buscar la manera de extraer la informacion para usar Chromadb, es posible aplicar la realizacion por lotes\n",
    "- Dos opciones: tener dos dataframes en chroma o tener uno pero añadir metadato para diferenciarlos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10/04\n",
    "- Limitar los datos a un municipio o isla para verlos resultados y comprobar que todo funciona (añadir como metadato el munipio y codigo postal)\n",
    "- Comprobar cuanto destacan las distancias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distancia del coseno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def cosine_similarity(v1, v2):\n",
    "    v1_np = np.array(v1)\n",
    "    v2_np = np.array(v2)\n",
    "    cos_sim = np.dot(v1_np, v2_np) / (np.linalg.norm(v1_np) * np.linalg.norm(v2_np))\n",
    "    return float(cos_sim)\n",
    "\n",
    "\n",
    "cosine_similarity_udf = udf(cosine_similarity, DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col, row_number, expr, max as spark_max\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "\n",
    "def get_top_matches(\n",
    "    test_df: DataFrame,\n",
    "    train_df: DataFrame,\n",
    "    embedding_col: str,\n",
    "    address_col: str,\n",
    "    top_n: int = 3,\n",
    ") -> DataFrame:\n",
    "    \"\"\"Return the n top results of the cosine similarity\"\"\"\n",
    "    test_df = test_df.alias(\"test_df\")\n",
    "    train_df = train_df.alias(\"train_df\")\n",
    "\n",
    "    cross_df = test_df.crossJoin(train_df)\n",
    "    cross_df = cross_df.withColumn(\n",
    "        \"cosine_similarity\",\n",
    "        cosine_similarity_udf(\n",
    "            col(f\"test_df.{embedding_col}\"), col(f\"train_df.{embedding_col}\")\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Crear una ventana para cada uuid_idt en test_df y ordenar por similitud del coseno\n",
    "    window = Window.partitionBy(col(f\"test_df.{address_col}\")).orderBy(\n",
    "        col(\"cosine_similarity\").desc()\n",
    "    )\n",
    "\n",
    "    cross_df = cross_df.withColumn(\"rank\", row_number().over(window))\n",
    "    top_n_df = cross_df.filter(col(\"rank\") <= top_n)\n",
    "\n",
    "    return top_n_df\n",
    "\n",
    "\n",
    "def evaluate_and_count_matches(\n",
    "    top_n_df: DataFrame, test_id_col: str, train_id_col: str, address_col: str\n",
    ") -> DataFrame:\n",
    "    top_n_df = top_n_df.withColumn(\"evaluation\", col(test_id_col) == col(train_id_col))\n",
    "    \n",
    "    # top_n_df.select(\"test_df.uuid_idt\", \"train_df.uuid_idt\", \"cosine_similarity\", \"evaluation\").show(truncate=False)\n",
    "\n",
    "    matches_by_address = top_n_df.groupBy(f\"test_df.{address_col}\").agg(\n",
    "        spark_max(expr(\"case when evaluation = true then 1 else 0 end\")).alias(\n",
    "            \"has_match\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Filtrar para obtener las direcciones con coincidencias\n",
    "    matches = matches_by_address.filter(\"has_match = 1\")\n",
    "\n",
    "    return matches.count()\n",
    "\n",
    "    \"\"\" top_n_df = top_n_df.withColumn(\"evaluation\", expr(f\"test_df.{test_id_col} = train_df.{train_id_col}\"))\n",
    "\n",
    "    # Agrupar por la dirección en test_df y contar si alguna de las mejores coincidencias acierta\n",
    "    matches_by_address = top_n_df.groupBy(col(f\"test_df.{address_col}\")).agg(\n",
    "        expr(\"sum(case when evaluation = true then 1 else 0 end)\").alias(\"match_count\")\n",
    "    )\n",
    "\n",
    "    # Filtrar para obtener las direcciones con coincidencias\n",
    "    matches = matches_by_address.filter(\"match_count > 0\")\n",
    "\n",
    "    return matches \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 611:=====================>                                   (3 + 5) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(train_embeddings_word2vec.count())\n",
    "print(test_embeddings_word2vec.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Obtener las mejores coincidencias\n",
    "best_results_df = get_top_matches(\n",
    "    test_embeddings_word2vec,\n",
    "    train_embeddings_word2vec,\n",
    "    \"embedding\",\n",
    "    \"direccion\",\n",
    "    top_n=3,\n",
    ")\n",
    "\n",
    "# Evaluar y contar las coincidencias\n",
    "matches_count = evaluate_and_count_matches(\n",
    "    best_results_df, \"test_df.uuid_idt\", \"train_df.uuid_idt\", \"direccion\"\n",
    ")\n",
    "\n",
    "# matches_count = matches_by_address.count()\n",
    "# print(f\"Number of addresses with matches: {matches_count}\")\n",
    "\n",
    "# 50 - 29min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/13 16:08:41 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/13 16:08:42 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/13 16:08:44 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/13 16:08:44 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/13 16:08:45 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/13 16:08:45 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/13 16:08:45 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/13 16:08:47 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/13 16:09:13 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/13 16:09:15 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/13 16:09:16 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/13 16:09:16 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/13 16:09:17 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/13 16:09:18 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/13 16:09:18 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/13 16:09:19 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/13 16:09:37 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/13 16:09:41 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/13 16:09:41 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/13 16:09:41 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/13 16:09:43 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/13 16:09:44 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/13 16:09:44 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/13 16:09:45 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/13 16:10:00 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/13 16:10:04 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/13 16:10:07 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/13 16:10:07 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/13 16:10:09 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/13 16:10:11 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/13 16:10:11 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/13 16:10:12 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/13 16:10:23 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/13 16:10:29 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/13 16:10:35 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/13 16:10:35 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/13 16:10:37 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/13 16:10:40 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/13 16:10:40 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/13 16:10:41 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/13 16:10:50 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/13 16:10:56 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/13 16:11:03 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/13 16:11:05 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/13 16:11:08 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/13 16:11:09 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/13 16:11:09 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/13 16:11:15 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/13 16:11:19 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/13 16:11:21 ERROR TaskMemoryManager: error while calling spill() on org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@6168b469\n",
      "java.io.IOException: No space left on device\n",
      "\tat java.base/java.io.FileOutputStream.writeBytes(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.write(FileOutputStream.java:354)\n",
      "\tat org.apache.spark.storage.TimeTrackingOutputStream.write(TimeTrackingOutputStream.java:59)\n",
      "\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)\n",
      "\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:127)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:225)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:178)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:323)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.write(UnsafeSorterSpillWriter.java:136)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.spillIterator(UnsafeExternalSorter.java:576)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.spill(UnsafeExternalSorter.java:231)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.trySpillAndAcquire(TaskMemoryManager.java:227)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.acquireExecutionMemory(TaskMemoryManager.java:190)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.allocatePage(TaskMemoryManager.java:317)\n",
      "\tat org.apache.spark.memory.MemoryConsumer.allocatePage(MemoryConsumer.java:117)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.acquireNewPageIfNecessary(UnsafeExternalSorter.java:433)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.allocateMemoryForRecordIfNecessary(UnsafeExternalSorter.java:452)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.insertRecord(UnsafeExternalSorter.java:487)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.insertRow(UnsafeExternalRowSorter.java:138)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:226)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "24/05/13 16:11:21 ERROR TaskMemoryManager: error while calling spill() on org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@78abef78\n",
      "java.io.IOException: No space left on device\n",
      "\tat java.base/java.io.FileOutputStream.writeBytes(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.write(FileOutputStream.java:354)\n",
      "\tat org.apache.spark.storage.TimeTrackingOutputStream.write(TimeTrackingOutputStream.java:59)\n",
      "\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)\n",
      "\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:127)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:225)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:178)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:323)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.write(UnsafeSorterSpillWriter.java:136)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.spillIterator(UnsafeExternalSorter.java:576)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.spill(UnsafeExternalSorter.java:231)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.trySpillAndAcquire(TaskMemoryManager.java:227)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.acquireExecutionMemory(TaskMemoryManager.java:190)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.allocatePage(TaskMemoryManager.java:317)\n",
      "\tat org.apache.spark.memory.MemoryConsumer.allocatePage(MemoryConsumer.java:117)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.acquireNewPageIfNecessary(UnsafeExternalSorter.java:433)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.allocateMemoryForRecordIfNecessary(UnsafeExternalSorter.java:452)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.insertRecord(UnsafeExternalSorter.java:487)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.insertRow(UnsafeExternalRowSorter.java:138)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:226)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "24/05/13 16:11:21 ERROR TaskMemoryManager: error while calling spill() on org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@53b138ff\n",
      "java.io.IOException: No space left on device\n",
      "\tat java.base/java.io.FileOutputStream.writeBytes(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.write(FileOutputStream.java:354)\n",
      "\tat org.apache.spark.storage.TimeTrackingOutputStream.write(TimeTrackingOutputStream.java:59)\n",
      "\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)\n",
      "\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:127)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:225)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:178)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:323)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.write(UnsafeSorterSpillWriter.java:136)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.spillIterator(UnsafeExternalSorter.java:576)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.spill(UnsafeExternalSorter.java:231)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.trySpillAndAcquire(TaskMemoryManager.java:227)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.acquireExecutionMemory(TaskMemoryManager.java:190)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.allocatePage(TaskMemoryManager.java:317)\n",
      "\tat org.apache.spark.memory.MemoryConsumer.allocatePage(MemoryConsumer.java:117)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.acquireNewPageIfNecessary(UnsafeExternalSorter.java:433)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.allocateMemoryForRecordIfNecessary(UnsafeExternalSorter.java:452)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.insertRecord(UnsafeExternalSorter.java:487)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.insertRow(UnsafeExternalRowSorter.java:138)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:226)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "24/05/13 16:11:21 ERROR TaskMemoryManager: error while calling spill() on org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@16115511\n",
      "java.io.IOException: No space left on device\n",
      "\tat java.base/java.io.FileOutputStream.writeBytes(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.write(FileOutputStream.java:354)\n",
      "\tat org.apache.spark.storage.TimeTrackingOutputStream.write(TimeTrackingOutputStream.java:59)\n",
      "\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)\n",
      "\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:127)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:225)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:178)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:323)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.write(UnsafeSorterSpillWriter.java:136)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.spillIterator(UnsafeExternalSorter.java:576)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.spill(UnsafeExternalSorter.java:231)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.trySpillAndAcquire(TaskMemoryManager.java:227)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.acquireExecutionMemory(TaskMemoryManager.java:190)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.allocatePage(TaskMemoryManager.java:317)\n",
      "\tat org.apache.spark.memory.MemoryConsumer.allocatePage(MemoryConsumer.java:117)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.acquireNewPageIfNecessary(UnsafeExternalSorter.java:433)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.allocateMemoryForRecordIfNecessary(UnsafeExternalSorter.java:452)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.insertRecord(UnsafeExternalSorter.java:487)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.insertRow(UnsafeExternalRowSorter.java:138)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:226)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "24/05/13 16:11:21 ERROR TaskMemoryManager: error while calling spill() on org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@78646693\n",
      "java.io.IOException: No space left on device\n",
      "\tat java.base/java.io.FileOutputStream.writeBytes(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.write(FileOutputStream.java:354)\n",
      "\tat org.apache.spark.storage.TimeTrackingOutputStream.write(TimeTrackingOutputStream.java:59)\n",
      "\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)\n",
      "\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:127)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:225)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:178)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:323)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.write(UnsafeSorterSpillWriter.java:136)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.spillIterator(UnsafeExternalSorter.java:576)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.spill(UnsafeExternalSorter.java:231)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.trySpillAndAcquire(TaskMemoryManager.java:227)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.acquireExecutionMemory(TaskMemoryManager.java:190)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.allocatePage(TaskMemoryManager.java:317)\n",
      "\tat org.apache.spark.memory.MemoryConsumer.allocatePage(MemoryConsumer.java:117)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.acquireNewPageIfNecessary(UnsafeExternalSorter.java:433)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.allocateMemoryForRecordIfNecessary(UnsafeExternalSorter.java:452)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.insertRecord(UnsafeExternalSorter.java:487)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.insertRow(UnsafeExternalRowSorter.java:138)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:226)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "24/05/13 16:11:21 ERROR TaskMemoryManager: error while calling spill() on org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@566f7734\n",
      "java.io.IOException: No space left on device\n",
      "\tat java.base/java.io.FileOutputStream.writeBytes(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.write(FileOutputStream.java:354)\n",
      "\tat org.apache.spark.storage.TimeTrackingOutputStream.write(TimeTrackingOutputStream.java:59)\n",
      "\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)\n",
      "\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:127)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:225)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:178)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:323)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.write(UnsafeSorterSpillWriter.java:136)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.spillIterator(UnsafeExternalSorter.java:576)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.spill(UnsafeExternalSorter.java:231)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.trySpillAndAcquire(TaskMemoryManager.java:227)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.acquireExecutionMemory(TaskMemoryManager.java:190)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.allocatePage(TaskMemoryManager.java:317)\n",
      "\tat org.apache.spark.memory.MemoryConsumer.allocatePage(MemoryConsumer.java:117)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.acquireNewPageIfNecessary(UnsafeExternalSorter.java:433)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.allocateMemoryForRecordIfNecessary(UnsafeExternalSorter.java:452)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.insertRecord(UnsafeExternalSorter.java:487)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.insertRow(UnsafeExternalRowSorter.java:138)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:226)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "24/05/13 16:11:21 ERROR TaskMemoryManager: error while calling spill() on org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@36108f35\n",
      "java.io.IOException: No space left on device\n",
      "\tat java.base/java.io.FileOutputStream.writeBytes(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.write(FileOutputStream.java:354)\n",
      "\tat org.apache.spark.storage.TimeTrackingOutputStream.write(TimeTrackingOutputStream.java:59)\n",
      "\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)\n",
      "\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:127)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:225)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:178)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:323)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.write(UnsafeSorterSpillWriter.java:136)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.spillIterator(UnsafeExternalSorter.java:576)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.spill(UnsafeExternalSorter.java:231)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.trySpillAndAcquire(TaskMemoryManager.java:227)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.acquireExecutionMemory(TaskMemoryManager.java:190)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.allocatePage(TaskMemoryManager.java:317)\n",
      "\tat org.apache.spark.memory.MemoryConsumer.allocatePage(MemoryConsumer.java:117)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.acquireNewPageIfNecessary(UnsafeExternalSorter.java:433)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.allocateMemoryForRecordIfNecessary(UnsafeExternalSorter.java:452)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.insertRecord(UnsafeExternalSorter.java:487)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.insertRow(UnsafeExternalRowSorter.java:138)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:226)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "24/05/13 16:11:21 ERROR DiskBlockObjectWriter: Exception occurred while manually close the output stream to file /tmp/blockmgr-3905b174-0116-4800-9fc7-bc475b36b175/1f/temp_shuffle_7508d488-4f06-40d3-939d-5cc57302505f, No space left on device\n",
      "24/05/13 16:11:22 ERROR Executor: Exception in task 2.0 in stage 1157.0 (TID 3746)\n",
      "org.apache.spark.memory.SparkOutOfMemoryError: error while calling spill() on org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@78646693 : No space left on device\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.trySpillAndAcquire(TaskMemoryManager.java:253)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.acquireExecutionMemory(TaskMemoryManager.java:190)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.allocatePage(TaskMemoryManager.java:317)\n",
      "\tat org.apache.spark.memory.MemoryConsumer.allocatePage(MemoryConsumer.java:117)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.acquireNewPageIfNecessary(UnsafeExternalSorter.java:433)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.allocateMemoryForRecordIfNecessary(UnsafeExternalSorter.java:452)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.insertRecord(UnsafeExternalSorter.java:487)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.insertRow(UnsafeExternalRowSorter.java:138)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:226)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "24/05/13 16:11:22 ERROR Executor: Exception in task 4.0 in stage 1157.0 (TID 3748)\n",
      "org.apache.spark.memory.SparkOutOfMemoryError: error while calling spill() on org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@36108f35 : No space left on device\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.trySpillAndAcquire(TaskMemoryManager.java:253)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.acquireExecutionMemory(TaskMemoryManager.java:190)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.allocatePage(TaskMemoryManager.java:317)\n",
      "\tat org.apache.spark.memory.MemoryConsumer.allocatePage(MemoryConsumer.java:117)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.acquireNewPageIfNecessary(UnsafeExternalSorter.java:433)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.allocateMemoryForRecordIfNecessary(UnsafeExternalSorter.java:452)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.insertRecord(UnsafeExternalSorter.java:487)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.insertRow(UnsafeExternalRowSorter.java:138)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:226)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "24/05/13 16:11:22 WARN TaskSetManager: Lost task 4.0 in stage 1157.0 (TID 3748) (10.6.130.30 executor driver): org.apache.spark.memory.SparkOutOfMemoryError: error while calling spill() on org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@36108f35 : No space left on device\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.trySpillAndAcquire(TaskMemoryManager.java:253)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.acquireExecutionMemory(TaskMemoryManager.java:190)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.allocatePage(TaskMemoryManager.java:317)\n",
      "\tat org.apache.spark.memory.MemoryConsumer.allocatePage(MemoryConsumer.java:117)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.acquireNewPageIfNecessary(UnsafeExternalSorter.java:433)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.allocateMemoryForRecordIfNecessary(UnsafeExternalSorter.java:452)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.insertRecord(UnsafeExternalSorter.java:487)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.insertRow(UnsafeExternalRowSorter.java:138)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:226)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "24/05/13 16:11:22 ERROR TaskSetManager: Task 4 in stage 1157.0 failed 1 times; aborting job\n",
      "24/05/13 16:11:22 ERROR Executor: Exception in task 6.0 in stage 1157.0 (TID 3750)\n",
      "org.apache.spark.memory.SparkOutOfMemoryError: error while calling spill() on org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@53b138ff : No space left on device\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.trySpillAndAcquire(TaskMemoryManager.java:253)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.acquireExecutionMemory(TaskMemoryManager.java:190)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.allocatePage(TaskMemoryManager.java:317)\n",
      "\tat org.apache.spark.memory.MemoryConsumer.allocatePage(MemoryConsumer.java:117)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.acquireNewPageIfNecessary(UnsafeExternalSorter.java:433)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.allocateMemoryForRecordIfNecessary(UnsafeExternalSorter.java:452)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.insertRecord(UnsafeExternalSorter.java:487)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.insertRow(UnsafeExternalRowSorter.java:138)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:226)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "24/05/13 16:11:22 ERROR Executor: Exception in task 0.0 in stage 1157.0 (TID 3744)\n",
      "java.io.IOException: No space left on device\n",
      "\tat java.base/java.io.FileOutputStream.writeBytes(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.write(FileOutputStream.java:354)\n",
      "\tat org.apache.spark.storage.TimeTrackingOutputStream.write(TimeTrackingOutputStream.java:59)\n",
      "\tat org.apache.spark.io.MutableCheckedOutputStream.write(MutableCheckedOutputStream.scala:43)\n",
      "\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)\n",
      "\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:127)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:225)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:178)\n",
      "\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)\n",
      "\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:127)\n",
      "\tat java.base/java.io.DataOutputStream.write(DataOutputStream.java:107)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.writeToStream(UnsafeRow.java:519)\n",
      "\tat org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$1.writeValue(UnsafeRowSerializer.scala:69)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:312)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:171)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "24/05/13 16:11:22 ERROR Executor: Exception in task 1.0 in stage 1157.0 (TID 3745)\n",
      "org.apache.spark.memory.SparkOutOfMemoryError: error while calling spill() on org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@16115511 : No space left on device\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.trySpillAndAcquire(TaskMemoryManager.java:253)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.acquireExecutionMemory(TaskMemoryManager.java:190)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.allocatePage(TaskMemoryManager.java:317)\n",
      "\tat org.apache.spark.memory.MemoryConsumer.allocatePage(MemoryConsumer.java:117)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.acquireNewPageIfNecessary(UnsafeExternalSorter.java:433)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.allocateMemoryForRecordIfNecessary(UnsafeExternalSorter.java:452)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.insertRecord(UnsafeExternalSorter.java:487)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.insertRow(UnsafeExternalRowSorter.java:138)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:226)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "24/05/13 16:11:22 ERROR Executor: Exception in task 5.0 in stage 1157.0 (TID 3749)\n",
      "org.apache.spark.memory.SparkOutOfMemoryError: error while calling spill() on org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@566f7734 : No space left on device\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.trySpillAndAcquire(TaskMemoryManager.java:253)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.acquireExecutionMemory(TaskMemoryManager.java:190)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.allocatePage(TaskMemoryManager.java:317)\n",
      "\tat org.apache.spark.memory.MemoryConsumer.allocatePage(MemoryConsumer.java:117)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.acquireNewPageIfNecessary(UnsafeExternalSorter.java:433)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.allocateMemoryForRecordIfNecessary(UnsafeExternalSorter.java:452)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.insertRecord(UnsafeExternalSorter.java:487)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.insertRow(UnsafeExternalRowSorter.java:138)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:226)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "24/05/13 16:11:22 ERROR Executor: Exception in task 7.0 in stage 1157.0 (TID 3751)\n",
      "org.apache.spark.memory.SparkOutOfMemoryError: error while calling spill() on org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@6168b469 : No space left on device\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.trySpillAndAcquire(TaskMemoryManager.java:253)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.acquireExecutionMemory(TaskMemoryManager.java:190)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.allocatePage(TaskMemoryManager.java:317)\n",
      "\tat org.apache.spark.memory.MemoryConsumer.allocatePage(MemoryConsumer.java:117)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.acquireNewPageIfNecessary(UnsafeExternalSorter.java:433)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.allocateMemoryForRecordIfNecessary(UnsafeExternalSorter.java:452)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.insertRecord(UnsafeExternalSorter.java:487)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.insertRow(UnsafeExternalRowSorter.java:138)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:226)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "24/05/13 16:11:22 ERROR Executor: Exception in task 3.0 in stage 1157.0 (TID 3747)\n",
      "org.apache.spark.memory.SparkOutOfMemoryError: error while calling spill() on org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@78abef78 : No space left on device\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.trySpillAndAcquire(TaskMemoryManager.java:253)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.acquireExecutionMemory(TaskMemoryManager.java:190)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.allocatePage(TaskMemoryManager.java:317)\n",
      "\tat org.apache.spark.memory.MemoryConsumer.allocatePage(MemoryConsumer.java:117)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.acquireNewPageIfNecessary(UnsafeExternalSorter.java:433)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.allocateMemoryForRecordIfNecessary(UnsafeExternalSorter.java:452)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.insertRecord(UnsafeExternalSorter.java:487)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.insertRow(UnsafeExternalRowSorter.java:138)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:226)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "24/05/13 16:11:22 WARN TaskSetManager: Lost task 2.0 in stage 1157.0 (TID 3746) (10.6.130.30 executor driver): org.apache.spark.memory.SparkOutOfMemoryError: error while calling spill() on org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@78646693 : No space left on device\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.trySpillAndAcquire(TaskMemoryManager.java:253)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.acquireExecutionMemory(TaskMemoryManager.java:190)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.allocatePage(TaskMemoryManager.java:317)\n",
      "\tat org.apache.spark.memory.MemoryConsumer.allocatePage(MemoryConsumer.java:117)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.acquireNewPageIfNecessary(UnsafeExternalSorter.java:433)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.allocateMemoryForRecordIfNecessary(UnsafeExternalSorter.java:452)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.insertRecord(UnsafeExternalSorter.java:487)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.insertRow(UnsafeExternalRowSorter.java:138)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:226)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "24/05/13 16:11:22 WARN TaskSetManager: Lost task 6.0 in stage 1157.0 (TID 3750) (10.6.130.30 executor driver): org.apache.spark.memory.SparkOutOfMemoryError: error while calling spill() on org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@53b138ff : No space left on device\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.trySpillAndAcquire(TaskMemoryManager.java:253)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.acquireExecutionMemory(TaskMemoryManager.java:190)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.allocatePage(TaskMemoryManager.java:317)\n",
      "\tat org.apache.spark.memory.MemoryConsumer.allocatePage(MemoryConsumer.java:117)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.acquireNewPageIfNecessary(UnsafeExternalSorter.java:433)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.allocateMemoryForRecordIfNecessary(UnsafeExternalSorter.java:452)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.insertRecord(UnsafeExternalSorter.java:487)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.insertRow(UnsafeExternalRowSorter.java:138)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:226)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "24/05/13 16:11:22 WARN TaskSetManager: Lost task 1.0 in stage 1157.0 (TID 3745) (10.6.130.30 executor driver): org.apache.spark.memory.SparkOutOfMemoryError: error while calling spill() on org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@16115511 : No space left on device\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.trySpillAndAcquire(TaskMemoryManager.java:253)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.acquireExecutionMemory(TaskMemoryManager.java:190)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.allocatePage(TaskMemoryManager.java:317)\n",
      "\tat org.apache.spark.memory.MemoryConsumer.allocatePage(MemoryConsumer.java:117)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.acquireNewPageIfNecessary(UnsafeExternalSorter.java:433)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.allocateMemoryForRecordIfNecessary(UnsafeExternalSorter.java:452)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.insertRecord(UnsafeExternalSorter.java:487)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.insertRow(UnsafeExternalRowSorter.java:138)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:226)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "24/05/13 16:11:22 WARN TaskSetManager: Lost task 0.0 in stage 1157.0 (TID 3744) (10.6.130.30 executor driver): java.io.IOException: No space left on device\n",
      "\tat java.base/java.io.FileOutputStream.writeBytes(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.write(FileOutputStream.java:354)\n",
      "\tat org.apache.spark.storage.TimeTrackingOutputStream.write(TimeTrackingOutputStream.java:59)\n",
      "\tat org.apache.spark.io.MutableCheckedOutputStream.write(MutableCheckedOutputStream.scala:43)\n",
      "\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)\n",
      "\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:127)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:225)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:178)\n",
      "\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)\n",
      "\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:127)\n",
      "\tat java.base/java.io.DataOutputStream.write(DataOutputStream.java:107)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.writeToStream(UnsafeRow.java:519)\n",
      "\tat org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$1.writeValue(UnsafeRowSerializer.scala:69)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:312)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:171)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "24/05/13 16:11:22 WARN TaskSetManager: Lost task 5.0 in stage 1157.0 (TID 3749) (10.6.130.30 executor driver): org.apache.spark.memory.SparkOutOfMemoryError: error while calling spill() on org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@566f7734 : No space left on device\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.trySpillAndAcquire(TaskMemoryManager.java:253)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.acquireExecutionMemory(TaskMemoryManager.java:190)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.allocatePage(TaskMemoryManager.java:317)\n",
      "\tat org.apache.spark.memory.MemoryConsumer.allocatePage(MemoryConsumer.java:117)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.acquireNewPageIfNecessary(UnsafeExternalSorter.java:433)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.allocateMemoryForRecordIfNecessary(UnsafeExternalSorter.java:452)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.insertRecord(UnsafeExternalSorter.java:487)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.insertRow(UnsafeExternalRowSorter.java:138)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:226)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "24/05/13 16:11:22 WARN TaskSetManager: Lost task 7.0 in stage 1157.0 (TID 3751) (10.6.130.30 executor driver): org.apache.spark.memory.SparkOutOfMemoryError: error while calling spill() on org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@6168b469 : No space left on device\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.trySpillAndAcquire(TaskMemoryManager.java:253)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.acquireExecutionMemory(TaskMemoryManager.java:190)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.allocatePage(TaskMemoryManager.java:317)\n",
      "\tat org.apache.spark.memory.MemoryConsumer.allocatePage(MemoryConsumer.java:117)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.acquireNewPageIfNecessary(UnsafeExternalSorter.java:433)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.allocateMemoryForRecordIfNecessary(UnsafeExternalSorter.java:452)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.insertRecord(UnsafeExternalSorter.java:487)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.insertRow(UnsafeExternalRowSorter.java:138)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:226)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "24/05/13 16:11:22 WARN TaskSetManager: Lost task 3.0 in stage 1157.0 (TID 3747) (10.6.130.30 executor driver): org.apache.spark.memory.SparkOutOfMemoryError: error while calling spill() on org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@78abef78 : No space left on device\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.trySpillAndAcquire(TaskMemoryManager.java:253)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.acquireExecutionMemory(TaskMemoryManager.java:190)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.allocatePage(TaskMemoryManager.java:317)\n",
      "\tat org.apache.spark.memory.MemoryConsumer.allocatePage(MemoryConsumer.java:117)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.acquireNewPageIfNecessary(UnsafeExternalSorter.java:433)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.allocateMemoryForRecordIfNecessary(UnsafeExternalSorter.java:452)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.insertRecord(UnsafeExternalSorter.java:487)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.insertRow(UnsafeExternalRowSorter.java:138)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:226)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "24/05/13 16:11:22 WARN TaskSetManager: Lost task 8.0 in stage 1157.0 (TID 3752) (10.6.130.30 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 4 in stage 1157.0 failed 1 times, most recent failure: Lost task 4.0 in stage 1157.0 (TID 3748) (10.6.130.30 executor driver): org.apache.spark.memory.SparkOutOfMemoryError: error while calling spill() on org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@36108f35 : No space left on device\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.trySpillAndAcquire(TaskMemoryManager.java:253)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.acquireExecutionMemory(TaskMemoryManager.java:190)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.allocatePage(TaskMemoryManager.java:317)\n",
      "\tat org.apache.spark.memory.MemoryConsumer.allocatePage(MemoryConsumer.java:117)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.acquireNewPageIfNecessary(UnsafeExternalSorter.java:433)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.allocateMemoryForRecordIfNecessary(UnsafeExternalSorter.java:452)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.insertRecord(UnsafeExternalSorter.java:487)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.insertRow(UnsafeExternalRowSorter.java:138)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:226)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:)\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1270.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 1157.0 failed 1 times, most recent failure: Lost task 4.0 in stage 1157.0 (TID 3748) (10.6.130.30 executor driver): org.apache.spark.memory.SparkOutOfMemoryError: error while calling spill() on org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@36108f35 : No space left on device\n\tat org.apache.spark.memory.TaskMemoryManager.trySpillAndAcquire(TaskMemoryManager.java:253)\n\tat org.apache.spark.memory.TaskMemoryManager.acquireExecutionMemory(TaskMemoryManager.java:190)\n\tat org.apache.spark.memory.TaskMemoryManager.allocatePage(TaskMemoryManager.java:317)\n\tat org.apache.spark.memory.MemoryConsumer.allocatePage(MemoryConsumer.java:117)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.acquireNewPageIfNecessary(UnsafeExternalSorter.java:433)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.allocateMemoryForRecordIfNecessary(UnsafeExternalSorter.java:452)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.insertRecord(UnsafeExternalSorter.java:487)\n\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.insertRow(UnsafeExternalRowSorter.java:138)\n\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:226)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\nCaused by: org.apache.spark.memory.SparkOutOfMemoryError: error while calling spill() on org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@36108f35 : No space left on device\n\tat org.apache.spark.memory.TaskMemoryManager.trySpillAndAcquire(TaskMemoryManager.java:253)\n\tat org.apache.spark.memory.TaskMemoryManager.acquireExecutionMemory(TaskMemoryManager.java:190)\n\tat org.apache.spark.memory.TaskMemoryManager.allocatePage(TaskMemoryManager.java:317)\n\tat org.apache.spark.memory.MemoryConsumer.allocatePage(MemoryConsumer.java:117)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.acquireNewPageIfNecessary(UnsafeExternalSorter.java:433)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.allocateMemoryForRecordIfNecessary(UnsafeExternalSorter.java:452)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.insertRecord(UnsafeExternalSorter.java:487)\n\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.insertRow(UnsafeExternalRowSorter.java:138)\n\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:226)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mevaluated_word2vec_cosine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mevaluation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mevaluation == \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTrue\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Address-matching/env_address_matching/lib/python3.10/site-packages/pyspark/sql/dataframe.py:1238\u001b[0m, in \u001b[0;36mDataFrame.count\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcount\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m   1216\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns the number of rows in this :class:`DataFrame`.\u001b[39;00m\n\u001b[1;32m   1217\u001b[0m \n\u001b[1;32m   1218\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1236\u001b[0m \u001b[38;5;124;03m    3\u001b[39;00m\n\u001b[1;32m   1237\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1238\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/Address-matching/env_address_matching/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/Address-matching/env_address_matching/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/Address-matching/env_address_matching/lib/python3.10/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1270.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 1157.0 failed 1 times, most recent failure: Lost task 4.0 in stage 1157.0 (TID 3748) (10.6.130.30 executor driver): org.apache.spark.memory.SparkOutOfMemoryError: error while calling spill() on org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@36108f35 : No space left on device\n\tat org.apache.spark.memory.TaskMemoryManager.trySpillAndAcquire(TaskMemoryManager.java:253)\n\tat org.apache.spark.memory.TaskMemoryManager.acquireExecutionMemory(TaskMemoryManager.java:190)\n\tat org.apache.spark.memory.TaskMemoryManager.allocatePage(TaskMemoryManager.java:317)\n\tat org.apache.spark.memory.MemoryConsumer.allocatePage(MemoryConsumer.java:117)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.acquireNewPageIfNecessary(UnsafeExternalSorter.java:433)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.allocateMemoryForRecordIfNecessary(UnsafeExternalSorter.java:452)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.insertRecord(UnsafeExternalSorter.java:487)\n\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.insertRow(UnsafeExternalRowSorter.java:138)\n\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:226)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\nCaused by: org.apache.spark.memory.SparkOutOfMemoryError: error while calling spill() on org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@36108f35 : No space left on device\n\tat org.apache.spark.memory.TaskMemoryManager.trySpillAndAcquire(TaskMemoryManager.java:253)\n\tat org.apache.spark.memory.TaskMemoryManager.acquireExecutionMemory(TaskMemoryManager.java:190)\n\tat org.apache.spark.memory.TaskMemoryManager.allocatePage(TaskMemoryManager.java:317)\n\tat org.apache.spark.memory.MemoryConsumer.allocatePage(MemoryConsumer.java:117)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.acquireNewPageIfNecessary(UnsafeExternalSorter.java:433)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.allocateMemoryForRecordIfNecessary(UnsafeExternalSorter.java:452)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.insertRecord(UnsafeExternalSorter.java:487)\n\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.insertRow(UnsafeExternalRowSorter.java:138)\n\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:226)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    }
   ],
   "source": [
    "evaluated_word2vec_cosine.select(\"evaluation\").filter(\"evaluation == 'True'\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/07 17:26:13 ERROR TaskMemoryManager: error while calling spill() on org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@7564a21\n",
      "java.io.IOException: No space left on device\n",
      "\tat java.base/java.io.FileOutputStream.writeBytes(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.write(FileOutputStream.java:354)\n",
      "\tat org.apache.spark.storage.TimeTrackingOutputStream.write(TimeTrackingOutputStream.java:59)\n",
      "\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)\n",
      "\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:127)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:225)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:178)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:323)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.write(UnsafeSorterSpillWriter.java:136)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.spillIterator(UnsafeExternalSorter.java:576)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.spill(UnsafeExternalSorter.java:231)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.trySpillAndAcquire(TaskMemoryManager.java:227)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.acquireExecutionMemory(TaskMemoryManager.java:190)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.allocatePage(TaskMemoryManager.java:317)\n",
      "\tat org.apache.spark.memory.MemoryConsumer.allocatePage(MemoryConsumer.java:117)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.acquireNewPageIfNecessary(UnsafeExternalSorter.java:433)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.allocateMemoryForRecordIfNecessary(UnsafeExternalSorter.java:452)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.insertRecord(UnsafeExternalSorter.java:487)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.insertRow(UnsafeExternalRowSorter.java:138)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:226)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "24/05/07 17:26:13 ERROR TaskMemoryManager: error while calling spill() on org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@7608744b\n",
      "java.io.IOException: No space left on device\n",
      "\tat java.base/java.io.FileOutputStream.writeBytes(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.write(FileOutputStream.java:354)\n",
      "\tat org.apache.spark.storage.TimeTrackingOutputStream.write(TimeTrackingOutputStream.java:59)\n",
      "\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)\n",
      "\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:127)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:225)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:178)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:323)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.write(UnsafeSorterSpillWriter.java:136)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.spillIterator(UnsafeExternalSorter.java:576)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.spill(UnsafeExternalSorter.java:231)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.trySpillAndAcquire(TaskMemoryManager.java:227)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.acquireExecutionMemory(TaskMemoryManager.java:190)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.allocatePage(TaskMemoryManager.java:317)\n",
      "\tat org.apache.spark.memory.MemoryConsumer.allocatePage(MemoryConsumer.java:117)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.acquireNewPageIfNecessary(UnsafeExternalSorter.java:433)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.allocateMemoryForRecordIfNecessary(UnsafeExternalSorter.java:452)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.insertRecord(UnsafeExternalSorter.java:487)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.insertRow(UnsafeExternalRowSorter.java:138)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:226)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "24/05/07 17:26:13 ERROR TaskMemoryManager: error while calling spill() on org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@61e5e343\n",
      "java.io.IOException: No space left on device\n",
      "\tat java.base/java.io.FileOutputStream.writeBytes(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.write(FileOutputStream.java:354)\n",
      "\tat org.apache.spark.storage.TimeTrackingOutputStream.write(TimeTrackingOutputStream.java:59)\n",
      "\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)\n",
      "\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:127)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:225)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:178)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:323)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.write(UnsafeSorterSpillWriter.java:136)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.spillIterator(UnsafeExternalSorter.java:576)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.spill(UnsafeExternalSorter.java:231)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.trySpillAndAcquire(TaskMemoryManager.java:227)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.acquireExecutionMemory(TaskMemoryManager.java:190)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.allocatePage(TaskMemoryManager.java:317)\n",
      "\tat org.apache.spark.memory.MemoryConsumer.allocatePage(MemoryConsumer.java:117)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.acquireNewPageIfNecessary(UnsafeExternalSorter.java:433)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.allocateMemoryForRecordIfNecessary(UnsafeExternalSorter.java:452)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.insertRecord(UnsafeExternalSorter.java:487)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.insertRow(UnsafeExternalRowSorter.java:138)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:226)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "24/05/07 17:26:13 ERROR TaskMemoryManager: error while calling spill() on org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@115785d6\n",
      "java.io.IOException: No space left on device\n",
      "\tat java.base/java.io.FileOutputStream.writeBytes(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.write(FileOutputStream.java:354)\n",
      "\tat org.apache.spark.storage.TimeTrackingOutputStream.write(TimeTrackingOutputStream.java:59)\n",
      "\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)\n",
      "\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:127)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:225)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:178)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:323)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.write(UnsafeSorterSpillWriter.java:136)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.spillIterator(UnsafeExternalSorter.java:576)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.spill(UnsafeExternalSorter.java:231)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.trySpillAndAcquire(TaskMemoryManager.java:227)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.acquireExecutionMemory(TaskMemoryManager.java:190)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.allocatePage(TaskMemoryManager.java:317)\n",
      "\tat org.apache.spark.memory.MemoryConsumer.allocatePage(MemoryConsumer.java:117)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.acquireNewPageIfNecessary(UnsafeExternalSorter.java:433)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.allocateMemoryForRecordIfNecessary(UnsafeExternalSorter.java:452)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.insertRecord(UnsafeExternalSorter.java:487)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.insertRow(UnsafeExternalRowSorter.java:138)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:226)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "24/05/07 17:26:13 ERROR TaskMemoryManager: error while calling spill() on org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@5f6b8641\n",
      "java.io.IOException: No space left on device\n",
      "\tat java.base/java.io.FileOutputStream.writeBytes(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.write(FileOutputStream.java:354)\n",
      "\tat org.apache.spark.storage.TimeTrackingOutputStream.write(TimeTrackingOutputStream.java:59)\n",
      "\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)\n",
      "\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:127)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:225)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:178)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:323)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.write(UnsafeSorterSpillWriter.java:136)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.spillIterator(UnsafeExternalSorter.java:576)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.spill(UnsafeExternalSorter.java:231)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.trySpillAndAcquire(TaskMemoryManager.java:227)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.acquireExecutionMemory(TaskMemoryManager.java:190)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.allocatePage(TaskMemoryManager.java:317)\n",
      "\tat org.apache.spark.memory.MemoryConsumer.allocatePage(MemoryConsumer.java:117)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.acquireNewPageIfNecessary(UnsafeExternalSorter.java:433)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.allocateMemoryForRecordIfNecessary(UnsafeExternalSorter.java:452)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.insertRecord(UnsafeExternalSorter.java:487)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.insertRow(UnsafeExternalRowSorter.java:138)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:226)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "24/05/07 17:26:13 ERROR TaskMemoryManager: error while calling spill() on org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@42e4b2\n",
      "java.io.IOException: No space left on device\n",
      "\tat java.base/java.io.FileOutputStream.writeBytes(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.write(FileOutputStream.java:354)\n",
      "\tat org.apache.spark.storage.TimeTrackingOutputStream.write(TimeTrackingOutputStream.java:59)\n",
      "\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)\n",
      "\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:127)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:225)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:178)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:323)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.write(UnsafeSorterSpillWriter.java:136)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.spillIterator(UnsafeExternalSorter.java:576)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.spill(UnsafeExternalSorter.java:231)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.trySpillAndAcquire(TaskMemoryManager.java:227)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.acquireExecutionMemory(TaskMemoryManager.java:190)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.allocatePage(TaskMemoryManager.java:317)\n",
      "\tat org.apache.spark.memory.MemoryConsumer.allocatePage(MemoryConsumer.java:117)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.acquireNewPageIfNecessary(UnsafeExternalSorter.java:433)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.allocateMemoryForRecordIfNecessary(UnsafeExternalSorter.java:452)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.insertRecord(UnsafeExternalSorter.java:487)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.insertRow(UnsafeExternalRowSorter.java:138)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:226)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "24/05/07 17:26:13 ERROR TaskMemoryManager: error while calling spill() on org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@9ea144f\n",
      "java.io.IOException: No space left on device\n",
      "\tat java.base/java.io.FileOutputStream.writeBytes(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.write(FileOutputStream.java:354)\n",
      "\tat org.apache.spark.storage.TimeTrackingOutputStream.write(TimeTrackingOutputStream.java:59)\n",
      "\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)\n",
      "\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:127)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:225)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:178)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:323)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.write(UnsafeSorterSpillWriter.java:136)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.spillIterator(UnsafeExternalSorter.java:576)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.spill(UnsafeExternalSorter.java:231)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.trySpillAndAcquire(TaskMemoryManager.java:227)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.acquireExecutionMemory(TaskMemoryManager.java:190)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.allocatePage(TaskMemoryManager.java:317)\n",
      "\tat org.apache.spark.memory.MemoryConsumer.allocatePage(MemoryConsumer.java:117)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.acquireNewPageIfNecessary(UnsafeExternalSorter.java:433)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.allocateMemoryForRecordIfNecessary(UnsafeExternalSorter.java:452)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.insertRecord(UnsafeExternalSorter.java:487)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.insertRow(UnsafeExternalRowSorter.java:138)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:226)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "24/05/07 17:26:13 ERROR Executor: Exception in task 4.0 in stage 1665.0 (TID 5306)\n",
      "org.apache.spark.memory.SparkOutOfMemoryError: error while calling spill() on org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@5f6b8641 : No space left on device\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.trySpillAndAcquire(TaskMemoryManager.java:253)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.acquireExecutionMemory(TaskMemoryManager.java:190)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.allocatePage(TaskMemoryManager.java:317)\n",
      "\tat org.apache.spark.memory.MemoryConsumer.allocatePage(MemoryConsumer.java:117)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.acquireNewPageIfNecessary(UnsafeExternalSorter.java:433)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.allocateMemoryForRecordIfNecessary(UnsafeExternalSorter.java:452)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.insertRecord(UnsafeExternalSorter.java:487)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.insertRow(UnsafeExternalRowSorter.java:138)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:226)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "24/05/07 17:26:13 WARN TaskSetManager: Lost task 4.0 in stage 1665.0 (TID 5306) (10.6.130.30 executor driver): org.apache.spark.memory.SparkOutOfMemoryError: error while calling spill() on org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@5f6b8641 : No space left on device\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.trySpillAndAcquire(TaskMemoryManager.java:253)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.acquireExecutionMemory(TaskMemoryManager.java:190)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.allocatePage(TaskMemoryManager.java:317)\n",
      "\tat org.apache.spark.memory.MemoryConsumer.allocatePage(MemoryConsumer.java:117)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.acquireNewPageIfNecessary(UnsafeExternalSorter.java:433)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.allocateMemoryForRecordIfNecessary(UnsafeExternalSorter.java:452)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.insertRecord(UnsafeExternalSorter.java:487)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.insertRow(UnsafeExternalRowSorter.java:138)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:226)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "24/05/07 17:26:13 ERROR TaskSetManager: Task 4 in stage 1665.0 failed 1 times; aborting job\n",
      "24/05/07 17:26:14 ERROR Executor: Exception in task 2.0 in stage 1665.0 (TID 5304)\n",
      "org.apache.spark.memory.SparkOutOfMemoryError: error while calling spill() on org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@9ea144f : No space left on device\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.trySpillAndAcquire(TaskMemoryManager.java:253)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.acquireExecutionMemory(TaskMemoryManager.java:190)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.allocatePage(TaskMemoryManager.java:317)\n",
      "\tat org.apache.spark.memory.MemoryConsumer.allocatePage(MemoryConsumer.java:117)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.acquireNewPageIfNecessary(UnsafeExternalSorter.java:433)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.allocateMemoryForRecordIfNecessary(UnsafeExternalSorter.java:452)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.insertRecord(UnsafeExternalSorter.java:487)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.insertRow(UnsafeExternalRowSorter.java:138)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:226)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "24/05/07 17:26:14 WARN TaskSetManager: Lost task 2.0 in stage 1665.0 (TID 5304) (10.6.130.30 executor driver): org.apache.spark.memory.SparkOutOfMemoryError: error while calling spill() on org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@9ea144f : No space left on device\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.trySpillAndAcquire(TaskMemoryManager.java:253)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.acquireExecutionMemory(TaskMemoryManager.java:190)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.allocatePage(TaskMemoryManager.java:317)\n",
      "\tat org.apache.spark.memory.MemoryConsumer.allocatePage(MemoryConsumer.java:117)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.acquireNewPageIfNecessary(UnsafeExternalSorter.java:433)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.allocateMemoryForRecordIfNecessary(UnsafeExternalSorter.java:452)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.insertRecord(UnsafeExternalSorter.java:487)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.insertRow(UnsafeExternalRowSorter.java:138)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:226)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "24/05/07 17:26:14 ERROR Executor: Exception in task 6.0 in stage 1665.0 (TID 5308)\n",
      "org.apache.spark.memory.SparkOutOfMemoryError: error while calling spill() on org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@115785d6 : No space left on device\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.trySpillAndAcquire(TaskMemoryManager.java:253)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.acquireExecutionMemory(TaskMemoryManager.java:190)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.allocatePage(TaskMemoryManager.java:317)\n",
      "\tat org.apache.spark.memory.MemoryConsumer.allocatePage(MemoryConsumer.java:117)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.acquireNewPageIfNecessary(UnsafeExternalSorter.java:433)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.allocateMemoryForRecordIfNecessary(UnsafeExternalSorter.java:452)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.insertRecord(UnsafeExternalSorter.java:487)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.insertRow(UnsafeExternalRowSorter.java:138)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:226)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "24/05/07 17:26:14 ERROR Executor: Exception in task 7.0 in stage 1665.0 (TID 5309)\n",
      "org.apache.spark.memory.SparkOutOfMemoryError: error while calling spill() on org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@61e5e343 : No space left on device\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.trySpillAndAcquire(TaskMemoryManager.java:253)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.acquireExecutionMemory(TaskMemoryManager.java:190)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.allocatePage(TaskMemoryManager.java:317)\n",
      "\tat org.apache.spark.memory.MemoryConsumer.allocatePage(MemoryConsumer.java:117)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.acquireNewPageIfNecessary(UnsafeExternalSorter.java:433)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.allocateMemoryForRecordIfNecessary(UnsafeExternalSorter.java:452)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.insertRecord(UnsafeExternalSorter.java:487)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.insertRow(UnsafeExternalRowSorter.java:138)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:226)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "24/05/07 17:26:14 WARN TaskSetManager: Lost task 6.0 in stage 1665.0 (TID 5308) (10.6.130.30 executor driver): org.apache.spark.memory.SparkOutOfMemoryError: error while calling spill() on org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@115785d6 : No space left on device\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.trySpillAndAcquire(TaskMemoryManager.java:253)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.acquireExecutionMemory(TaskMemoryManager.java:190)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.allocatePage(TaskMemoryManager.java:317)\n",
      "\tat org.apache.spark.memory.MemoryConsumer.allocatePage(MemoryConsumer.java:117)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.acquireNewPageIfNecessary(UnsafeExternalSorter.java:433)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.allocateMemoryForRecordIfNecessary(UnsafeExternalSorter.java:452)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.insertRecord(UnsafeExternalSorter.java:487)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.insertRow(UnsafeExternalRowSorter.java:138)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:226)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "24/05/07 17:26:14 WARN TaskSetManager: Lost task 7.0 in stage 1665.0 (TID 5309) (10.6.130.30 executor driver): org.apache.spark.memory.SparkOutOfMemoryError: error while calling spill() on org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@61e5e343 : No space left on device\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.trySpillAndAcquire(TaskMemoryManager.java:253)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.acquireExecutionMemory(TaskMemoryManager.java:190)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.allocatePage(TaskMemoryManager.java:317)\n",
      "\tat org.apache.spark.memory.MemoryConsumer.allocatePage(MemoryConsumer.java:117)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.acquireNewPageIfNecessary(UnsafeExternalSorter.java:433)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.allocateMemoryForRecordIfNecessary(UnsafeExternalSorter.java:452)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.insertRecord(UnsafeExternalSorter.java:487)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.insertRow(UnsafeExternalRowSorter.java:138)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:226)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "24/05/07 17:26:14 ERROR Executor: Exception in task 5.0 in stage 1665.0 (TID 5307)\n",
      "org.apache.spark.memory.SparkOutOfMemoryError: error while calling spill() on org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@7564a21 : No space left on device\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.trySpillAndAcquire(TaskMemoryManager.java:253)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.acquireExecutionMemory(TaskMemoryManager.java:190)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.allocatePage(TaskMemoryManager.java:317)\n",
      "\tat org.apache.spark.memory.MemoryConsumer.allocatePage(MemoryConsumer.java:117)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.acquireNewPageIfNecessary(UnsafeExternalSorter.java:433)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.allocateMemoryForRecordIfNecessary(UnsafeExternalSorter.java:452)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.insertRecord(UnsafeExternalSorter.java:487)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.insertRow(UnsafeExternalRowSorter.java:138)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:226)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "24/05/07 17:26:14 WARN TaskSetManager: Lost task 5.0 in stage 1665.0 (TID 5307) (10.6.130.30 executor driver): org.apache.spark.memory.SparkOutOfMemoryError: error while calling spill() on org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@7564a21 : No space left on device\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.trySpillAndAcquire(TaskMemoryManager.java:253)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.acquireExecutionMemory(TaskMemoryManager.java:190)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.allocatePage(TaskMemoryManager.java:317)\n",
      "\tat org.apache.spark.memory.MemoryConsumer.allocatePage(MemoryConsumer.java:117)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.acquireNewPageIfNecessary(UnsafeExternalSorter.java:433)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.allocateMemoryForRecordIfNecessary(UnsafeExternalSorter.java:452)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.insertRecord(UnsafeExternalSorter.java:487)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.insertRow(UnsafeExternalRowSorter.java:138)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:226)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "24/05/07 17:26:14 WARN TaskSetManager: Lost task 8.0 in stage 1665.0 (TID 5310) (10.6.130.30 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 4 in stage 1665.0 failed 1 times, most recent failure: Lost task 4.0 in stage 1665.0 (TID 5306) (10.6.130.30 executor driver): org.apache.spark.memory.SparkOutOfMemoryError: error while calling spill() on org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@5f6b8641 : No space left on device\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.trySpillAndAcquire(TaskMemoryManager.java:253)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.acquireExecutionMemory(TaskMemoryManager.java:190)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.allocatePage(TaskMemoryManager.java:317)\n",
      "\tat org.apache.spark.memory.MemoryConsumer.allocatePage(MemoryConsumer.java:117)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.acquireNewPageIfNecessary(UnsafeExternalSorter.java:433)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.allocateMemoryForRecordIfNecessary(UnsafeExternalSorter.java:452)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.insertRecord(UnsafeExternalSorter.java:487)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.insertRow(UnsafeExternalRowSorter.java:138)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:226)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/05/07 17:26:14 ERROR Executor: Exception in task 1.0 in stage 1665.0 (TID 5303)\n",
      "org.apache.spark.memory.SparkOutOfMemoryError: error while calling spill() on org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@7608744b : No space left on device\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.trySpillAndAcquire(TaskMemoryManager.java:253)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.acquireExecutionMemory(TaskMemoryManager.java:190)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.allocatePage(TaskMemoryManager.java:317)\n",
      "\tat org.apache.spark.memory.MemoryConsumer.allocatePage(MemoryConsumer.java:117)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.acquireNewPageIfNecessary(UnsafeExternalSorter.java:433)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.allocateMemoryForRecordIfNecessary(UnsafeExternalSorter.java:452)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.insertRecord(UnsafeExternalSorter.java:487)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.insertRow(UnsafeExternalRowSorter.java:138)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:226)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "24/05/07 17:26:14 WARN TaskSetManager: Lost task 1.0 in stage 1665.0 (TID 5303) (10.6.130.30 executor driver): org.apache.spark.memory.SparkOutOfMemoryError: error while calling spill() on org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@7608744b : No space left on device\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.trySpillAndAcquire(TaskMemoryManager.java:253)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.acquireExecutionMemory(TaskMemoryManager.java:190)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.allocatePage(TaskMemoryManager.java:317)\n",
      "\tat org.apache.spark.memory.MemoryConsumer.allocatePage(MemoryConsumer.java:117)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.acquireNewPageIfNecessary(UnsafeExternalSorter.java:433)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.allocateMemoryForRecordIfNecessary(UnsafeExternalSorter.java:452)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.insertRecord(UnsafeExternalSorter.java:487)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.insertRow(UnsafeExternalRowSorter.java:138)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:226)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "24/05/07 17:26:14 ERROR Executor: Exception in task 3.0 in stage 1665.0 (TID 5305)\n",
      "org.apache.spark.memory.SparkOutOfMemoryError: error while calling spill() on org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@42e4b2 : No space left on device\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.trySpillAndAcquire(TaskMemoryManager.java:253)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.acquireExecutionMemory(TaskMemoryManager.java:190)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.allocatePage(TaskMemoryManager.java:317)\n",
      "\tat org.apache.spark.memory.MemoryConsumer.allocatePage(MemoryConsumer.java:117)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.acquireNewPageIfNecessary(UnsafeExternalSorter.java:433)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.allocateMemoryForRecordIfNecessary(UnsafeExternalSorter.java:452)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.insertRecord(UnsafeExternalSorter.java:487)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.insertRow(UnsafeExternalRowSorter.java:138)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:226)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "24/05/07 17:26:14 WARN TaskSetManager: Lost task 3.0 in stage 1665.0 (TID 5305) (10.6.130.30 executor driver): org.apache.spark.memory.SparkOutOfMemoryError: error while calling spill() on org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@42e4b2 : No space left on device\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.trySpillAndAcquire(TaskMemoryManager.java:253)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.acquireExecutionMemory(TaskMemoryManager.java:190)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.allocatePage(TaskMemoryManager.java:317)\n",
      "\tat org.apache.spark.memory.MemoryConsumer.allocatePage(MemoryConsumer.java:117)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.acquireNewPageIfNecessary(UnsafeExternalSorter.java:433)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.allocateMemoryForRecordIfNecessary(UnsafeExternalSorter.java:452)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.insertRecord(UnsafeExternalSorter.java:487)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.insertRow(UnsafeExternalRowSorter.java:138)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:226)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "24/05/07 17:26:14 WARN TaskSetManager: Lost task 0.0 in stage 1665.0 (TID 5302) (10.6.130.30 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 4 in stage 1665.0 failed 1 times, most recent failure: Lost task 4.0 in stage 1665.0 (TID 5306) (10.6.130.30 executor driver): org.apache.spark.memory.SparkOutOfMemoryError: error while calling spill() on org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@5f6b8641 : No space left on device\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.trySpillAndAcquire(TaskMemoryManager.java:253)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.acquireExecutionMemory(TaskMemoryManager.java:190)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.allocatePage(TaskMemoryManager.java:317)\n",
      "\tat org.apache.spark.memory.MemoryConsumer.allocatePage(MemoryConsumer.java:117)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.acquireNewPageIfNecessary(UnsafeExternalSorter.java:433)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.allocateMemoryForRecordIfNecessary(UnsafeExternalSorter.java:452)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.insertRecord(UnsafeExternalSorter.java:487)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.insertRow(UnsafeExternalRowSorter.java:138)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:226)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:)\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1097.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 1665.0 failed 1 times, most recent failure: Lost task 4.0 in stage 1665.0 (TID 5306) (10.6.130.30 executor driver): org.apache.spark.memory.SparkOutOfMemoryError: error while calling spill() on org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@5f6b8641 : No space left on device\n\tat org.apache.spark.memory.TaskMemoryManager.trySpillAndAcquire(TaskMemoryManager.java:253)\n\tat org.apache.spark.memory.TaskMemoryManager.acquireExecutionMemory(TaskMemoryManager.java:190)\n\tat org.apache.spark.memory.TaskMemoryManager.allocatePage(TaskMemoryManager.java:317)\n\tat org.apache.spark.memory.MemoryConsumer.allocatePage(MemoryConsumer.java:117)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.acquireNewPageIfNecessary(UnsafeExternalSorter.java:433)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.allocateMemoryForRecordIfNecessary(UnsafeExternalSorter.java:452)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.insertRecord(UnsafeExternalSorter.java:487)\n\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.insertRow(UnsafeExternalRowSorter.java:138)\n\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:226)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\nCaused by: org.apache.spark.memory.SparkOutOfMemoryError: error while calling spill() on org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@5f6b8641 : No space left on device\n\tat org.apache.spark.memory.TaskMemoryManager.trySpillAndAcquire(TaskMemoryManager.java:253)\n\tat org.apache.spark.memory.TaskMemoryManager.acquireExecutionMemory(TaskMemoryManager.java:190)\n\tat org.apache.spark.memory.TaskMemoryManager.allocatePage(TaskMemoryManager.java:317)\n\tat org.apache.spark.memory.MemoryConsumer.allocatePage(MemoryConsumer.java:117)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.acquireNewPageIfNecessary(UnsafeExternalSorter.java:433)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.allocateMemoryForRecordIfNecessary(UnsafeExternalSorter.java:452)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.insertRecord(UnsafeExternalSorter.java:487)\n\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.insertRow(UnsafeExternalRowSorter.java:138)\n\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:226)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mevaluated_word2vec_cosine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mevaluation == \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTrue\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Address-matching/env_address_matching/lib/python3.10/site-packages/pyspark/sql/dataframe.py:1238\u001b[0m, in \u001b[0;36mDataFrame.count\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcount\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m   1216\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns the number of rows in this :class:`DataFrame`.\u001b[39;00m\n\u001b[1;32m   1217\u001b[0m \n\u001b[1;32m   1218\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1236\u001b[0m \u001b[38;5;124;03m    3\u001b[39;00m\n\u001b[1;32m   1237\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1238\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/Address-matching/env_address_matching/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/Address-matching/env_address_matching/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/Address-matching/env_address_matching/lib/python3.10/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1097.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 1665.0 failed 1 times, most recent failure: Lost task 4.0 in stage 1665.0 (TID 5306) (10.6.130.30 executor driver): org.apache.spark.memory.SparkOutOfMemoryError: error while calling spill() on org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@5f6b8641 : No space left on device\n\tat org.apache.spark.memory.TaskMemoryManager.trySpillAndAcquire(TaskMemoryManager.java:253)\n\tat org.apache.spark.memory.TaskMemoryManager.acquireExecutionMemory(TaskMemoryManager.java:190)\n\tat org.apache.spark.memory.TaskMemoryManager.allocatePage(TaskMemoryManager.java:317)\n\tat org.apache.spark.memory.MemoryConsumer.allocatePage(MemoryConsumer.java:117)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.acquireNewPageIfNecessary(UnsafeExternalSorter.java:433)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.allocateMemoryForRecordIfNecessary(UnsafeExternalSorter.java:452)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.insertRecord(UnsafeExternalSorter.java:487)\n\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.insertRow(UnsafeExternalRowSorter.java:138)\n\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:226)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\nCaused by: org.apache.spark.memory.SparkOutOfMemoryError: error while calling spill() on org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@5f6b8641 : No space left on device\n\tat org.apache.spark.memory.TaskMemoryManager.trySpillAndAcquire(TaskMemoryManager.java:253)\n\tat org.apache.spark.memory.TaskMemoryManager.acquireExecutionMemory(TaskMemoryManager.java:190)\n\tat org.apache.spark.memory.TaskMemoryManager.allocatePage(TaskMemoryManager.java:317)\n\tat org.apache.spark.memory.MemoryConsumer.allocatePage(MemoryConsumer.java:117)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.acquireNewPageIfNecessary(UnsafeExternalSorter.java:433)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.allocateMemoryForRecordIfNecessary(UnsafeExternalSorter.java:452)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.insertRecord(UnsafeExternalSorter.java:487)\n\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.insertRow(UnsafeExternalRowSorter.java:138)\n\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:226)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    }
   ],
   "source": [
    "evaluated_word2vec_cosine.filter(\"evaluation == 'True'\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 171:============================================>          (26 + 6) / 32]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+------------------------------------+-------------------------------------------------------------------------------+-----------------------------------------------------------------------+-----------------+\n",
      "|uuid_idt                            |uuid_idt2                           |direccion                                                                      |direccion2                                                             |cosine_similarity|\n",
      "+------------------------------------+------------------------------------+-------------------------------------------------------------------------------+-----------------------------------------------------------------------+-----------------+\n",
      "|05DF4E46-4F82-450B-ACF3-E2757D0AE09B|7438477F-3251-11E8-9F08-480FCF5217B3|ANGOSTA CARTA 10 SAN CRISTOBAL DE LA LAGUAN                                    |CRA CANDILA LGAUNA 21 SAN CRISTOBAL DE LA LGAUNA                       |0.91462225       |\n",
      "|33ECFF0C-9C99-4F93-BECD-3E7872E2DC03|685B7500-95B0-4048-BBFA-1E3D2884B428|APARTADO DE CORREOS AVDA MENCEEYS 295 SAN CRISTOBAL DE LA LAGUNA               |APARTADO DE CORREOS APARTADO CORREOS 659 SAN CRISTOBAL DE LA LAGUNA    |0.92651683       |\n",
      "|1946009D-6F04-4EB9-8882-3BDE351D99AE|6CBC519A-3251-11E8-BC74-480FCF5217B3|PLAZUELA D ALFONSO FERNANEDZ GARCIA 11 SAN CRISTOBAL DE LA LAGUNA              |CALLE IGNACIO PEREZ GARCIA 8 SAN CRISTOBAL DE LA LAGUNA                |0.8767058        |\n",
      "|3543EE18-7449-11EB-8771-6F70BC935088|61256FDD-3251-11E8-9DB0-480FCF5217B3|PLAZOLETA PANASCOS S BARTOLOME GENETO 33 SAN CRISTOBAL ED LA LAGUNA            |AVENIDA BARTOLOME CAIRASCO 7 SAN CRISTOBAL DE LA LAGUNA                |0.85955495       |\n",
      "|039FAD75-7525-11EB-A26D-7D5EFCBC1961|C0623070-0AD1-11EA-8401-7756D0C25261|VALLE CAMINO CAÑADA 109 SAN CRISTOBAL DE LA LAGUAN                             |CAMINO CAÑADA ORTIGAL 74 SAN CRISTOBAL DE LA LAGUNA                    |0.8677388        |\n",
      "|1B0B44DF-8652-42C2-A37E-7AF4D209210B|660AC9A5-3251-11E8-AB67-480FCF5217B3|PISTA AGUIRRE RIOS 12 SAN CRISTOBAL DE LA LGAUNA                               |CALLE CARMENEL BPILAR 9 SAN CRISTOBAL DE LA LAGUNA                     |0.78717923       |\n",
      "|35406E0E-7449-11EB-8771-6F70BC935088|6D65908E-3251-11E8-B903-480FCF5217B3|CSRIO JUSTICIA GUAAJRA 23 SAN CRISTOBAL DE LA LAGUNA                           |AMPLIACION CEBDAA 11 SAN CRISTOBAL DE LA LAGUNA                        |0.97899675       |\n",
      "|39F9AE5C-83CF-11EA-A806-0768FB2B34E1|636F3879-3251-11E8-AC36-480FCF5217B3|PARTICULAR VIRGEN SALETTE 25 SAN CRISTOBAL ED LA LAGUNA                        |CAMIN CREMONA 27 SAN CRISTOBAL ED LA LAGUNA                            |0.90635175       |\n",
      "|29651D6D-5160-487A-8176-E32CF027A6AF|481D68B3-8321-11EA-8991-9109B330B472|VIVIEDNAS GENERAL ESPERANZA 114 SAN CRISTOBAL DE LA LAGUNA                     |CARRETERA CTRAGRALBAJAMAR 14 SAN CRISTOBAL DE LA LAGUNA                |0.9321774        |\n",
      "|34CF828F-7449-11EB-8771-6F70BC935088|77487A61-3251-11E8-B785-480FCF5217B3|CORRAL LUCANAS VLALE GUERRA 0 SAN CRISTOBAL DE LA LAGUNA                       |MUNICIPIO CTRAGRALVALLE GUERRA 83 SAN CRISOTBAL DE LA LAGUNA           |0.88278407       |\n",
      "|34BE72D8-31A8-4684-814D-2F453A39B0FB|ACCDC40D-C2DD-479A-B3FB-BC0049A35F5B|MONTAÑA DOCTOR ESCOLASTICO AGUIAR SOTO 15 SAN CRISTOBAL DE LA LAGUAN           |URBANIZACION DR ANTONIO GONZALEZ GONZALEZ 13 SAN RCISTOBAL DE LA LAGUNA|0.64667183       |\n",
      "|39BB4635-BE4F-49F6-8680-7E0B95862B6D|636F3879-3251-11E8-AC36-480FCF5217B3|ZONA TEJINA BAJAMAR 4 SAN CRISTOBAL ED LA LAGUNA                               |CAMIN CREMONA 27 SAN CRISTOBAL ED LA LAGUNA                            |0.89648026       |\n",
      "|34CBDB57-7449-11EB-8771-6F70BC935088|6A8AC650-3251-11E8-B6AC-480FCF5217B3|PROLONGACION ADRE AGUA 3 SAN CRISTOBAL ED LA LAGUNA                            |BLOQUES MARTE 6 SAN CRISTOBAL ED LA LAGUNA                             |0.9030564        |\n",
      "|030CD241-C682-4119-96BA-2AB107F83014|65B14638-3251-11E8-BE7C-480FCF5217B3|EXTRARRADIO LUNA LLENA EXTRARRADIO TERCERA DERECHA 9 SAN CRISTOBAL DE AL LAGUNA|TRAV TANGANILLO 0 SAN CRISTOBAL DE AL LAGUNA                           |0.80556613       |\n",
      "|20D60BB5-C77B-443F-870B-495DA79409E9|7360B0CC-3251-11E8-9A67-480FCF5217B3|ACCESO BALTASAR NUÑEZ 17 JB 5 17 SAN CRISTOBAL DE LA LAGUNA                    |SUBIDA ANDENES 53 SAN CRISTOBAL DE LA LAGUNA                           |0.8413811        |\n",
      "|2250307D-5DDD-40D4-B4E2-209200A81CF4|63ECFC43-3251-11E8-B376-480FCF5217B3|ROTONDA COLEGIO PRINCESA TEJINA 0 ASN CRISTOBAL DE LA LAGUNA                   |ROTONDA SALERNO 3 SNA CRISTOBAL DE LA LAGUNA                           |0.85226595       |\n",
      "|34B18684-7449-11EB-8771-6F70BC935088|5FD5AF2B-3251-11E8-A2A7-480FCF5217B3|ESCALERA AGUIRRE IROS 44 SAN CRISTOBAL DE LA LAGUNA                            |CALLE ORTIGALA 17 SAN CRISTOBAL DE LA LAGUNA                           |0.95233494       |\n",
      "|31123F8D-7998-11EB-9335-3553A692CF72|6A8AC650-3251-11E8-B6AC-480FCF5217B3|CARRERA INDUSTRIAL MARCEROL 27 SAN CRISTOBAL ED LA LAGUNA                      |BLOQUES MARTE 6 SAN CRISTOBAL ED LA LAGUNA                             |0.8855191        |\n",
      "|34A9C410-7449-11EB-8771-6F70BC935088|6E35163F-3251-11E8-B930-480FCF5217B3|POLIGONO MERCEDES MAJUELOS TACO 0 SAN CRISTOBAL DE LA ALGUNA                   |CALLE SAN SEBASTIAN CNO HORNERA MAJUELOS 29 SAN CRISTOBAL DE LA LAGUNA |0.82562786       |\n",
      "|05A0F717-FE12-4087-8CC5-E718D630AD30|ED4F4D56-0C67-43DA-9DD3-275238008AF2|AVENIDA HURONCILOL TEJINA 0 SAN CRISTOBAL DE LA LAGUNA                         |SOLAR CHUMBEARS 7 SAN CRISTOBAL DE LA LAGUNA                           |0.92452323       |\n",
      "+------------------------------------+------------------------------------+-------------------------------------------------------------------------------+-----------------------------------------------------------------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "evaluated_word2vec_cosine.select(\n",
    "    \"uuid_idt\", \"uuid_idt2\", \"direccion\", \"direccion2\", \"cosine_similarity\"\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluated_word2vec_cosine.select(\"evaluation\").filter(\"evaluation == true\").count()\n",
    "# 1.000 2min\n",
    "# 10.000 error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accertion_ratio(evaluated_word2vec_cosine))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MiniLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "minilm_train_dense_vector = train_df.withColumn(\n",
    "    \"embedding\", minilm_embedding_dense_vector(col(\"direccion\"))\n",
    ")\n",
    "\n",
    "minilm_test_dense_vector = test_df.withColumn(\n",
    "    \"embedding\", minilm_embedding_dense_vector(col(\"direccion\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "minilm_train_dense_vector = train_df.withColumn(\"embedding\", minilm_embedding_dense_vector(\"direccion\"))\n",
    "\n",
    "minilm_test_dense_vector = test_df.withColumn(\"embedding\", minilm_embedding_dense_vector(\"direccion\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2088:==============> (8 + 1) / 9][Stage 2160:>               (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+---------+----------+------------+------------------------------------+-----+------+--------------------------+-----------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|uuid_idt                            |latitud  |longitud  |tvia        |nvia                                |numer|codmun|nommun                    |direccion                                                              |embedding                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |\n",
      "+------------------------------------+---------+----------+------------+------------------------------------+-----+------+--------------------------+-----------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|C075AB21-0AD1-11EA-8401-7756D0C25261|28.48311 |-16.31491 |PLAZA       |VICTOR ZURITA SOLER                 |102  |38023 |San Cristóbal de La Laguna|PLAZA VICTOR ZURITA SOLER 102 SAN CRISTOBAL DE LA LAGUNA               |[-0.05593755841255188,0.023558195680379868,-0.030977971851825714,-0.012365075759589672,-0.03384287655353546,-0.001311257015913725,-0.025311680510640144,0.04384683817625046,-0.023613378405570984,-0.036561232060194016,0.02455618418753147,-0.10064560174942017,0.033387355506420135,-0.01039414294064045,0.015515036880970001,-0.01278439350426197,-0.06197778880596161,0.008815311826765537,0.04864760860800743,-0.021803809329867363,-0.002294969977810979,-0.08246573060750961,-0.09819226711988449,0.03530706465244293,-0.06467067450284958,0.01808992400765419,-0.036707665771245956,0.02540895715355873,-0.03883760794997215,-0.056445859372615814,0.013159685768187046,0.06691581010818481,0.05154132843017578,0.022868415340781212,0.03699271380901337,0.028182201087474823,-0.02479478344321251,-0.04773253947496414,-0.034087084233760834,0.07343024015426636,-0.10177122056484222,-0.0026965278666466475,-0.018862973898649216,0.022565225139260292,0.03045284003019333,-0.041582558304071426,-0.006858654320240021,-0.035581398755311966,0.05469975620508194,0.018494734540581703,-0.07005831599235535,0.06994538754224777,-0.013964795507490635,0.07651442289352417,-0.020053116604685783,0.04886026307940483,0.07843731343746185,-0.05671757087111473,0.12557795643806458,-0.008847646415233612,0.1280318647623062,0.02566847950220108,-0.03592031076550484,-0.037627726793289185,-0.0414772555232048,-0.037313394248485565,-0.016408871859312057,-0.036814745515584946,-0.08391591906547546,-0.08226655423641205,0.10001606494188309,0.011791201308369637,0.03818422183394432,-0.021049873903393745,-0.01913396641612053,-0.06334544718265533,-0.03835540637373924,-0.017593372613191605,-0.04485626891255379,0.040705710649490356,0.0112691605463624,-0.030251091346144676,-0.0077687338925898075,-0.012234976515173912,0.013405553065240383,0.03419693559408188,-0.024663038551807404,-0.06016768515110016,0.05066995322704315,0.010850317776203156,-0.0795241966843605,-0.001908969134092331,-0.12973247468471527,-0.007940080948174,-0.03633861988782883,0.021363036707043648,-0.003645901568233967,0.014955423772335052,0.005120102781802416,0.06621959805488586,0.04532237350940704,0.03509572148323059,-0.05235295742750168,-0.011866048909723759,-0.04475909844040871,0.028544066473841667,0.0996684655547142,0.05002012848854065,-0.030348878353834152,0.01806737668812275,-0.0301742572337389,-0.022511987015604973,-0.013832478784024715,-0.0782967060804367,-9.90685890428722E-4,0.029274411499500275,0.007173287682235241,-0.01715826988220215,-0.0038266319315880537,-0.03231828659772873,0.05183609575033188,-0.04167234152555466,-0.04725763201713562,-0.09372575581073761,-0.0121668865904212,0.00217173108831048,0.022083310410380363,6.363386588594816E-33,-0.011340752243995667,0.04441056028008461,-0.008300283923745155,0.0980469286441803,0.06330259889364243,-0.016238942742347717,-0.008797436021268368,-0.004688088316470385,-0.04393348842859268,-0.07012589275836945,-0.05224541202187538,-0.042081594467163086,-0.07191964983940125,-0.04537436366081238,-0.009869779460132122,0.005858988501131535,0.058226294815540314,-0.098953016102314,-0.03617275506258011,-0.09865603595972061,0.03454238921403885,-0.006151298992335796,-0.019994698464870453,-0.011691083200275898,0.0016793962568044662,0.08682702481746674,-0.016160879284143448,-0.006354518234729767,0.004195110406726599,0.06238166242837906,-0.014797617681324482,-0.00712052546441555,0.05769208446145058,0.00593904685229063,0.07675258815288544,-0.04674261808395386,-8.516627713106573E-4,-0.001504950923845172,-0.05151260271668434,0.014948070049285889,0.07397768646478653,0.03544718027114868,-0.018565785139799118,0.040034763514995575,0.02884666435420513,0.03147396445274353,0.09014731645584106,0.05273285135626793,0.1248100996017456,0.004622187931090593,-0.11744924634695053,-0.04772132262587547,-0.1684601902961731,0.048174090683460236,0.04850608482956886,0.027573660016059875,-0.1004285141825676,-0.048638634383678436,-0.0529048852622509,0.059086691588163376,0.09904898703098297,0.11991802603006363,-0.03717136010527611,0.011400927789509296,0.017897963523864746,-0.06305643171072006,0.023745769634842873,0.04333166405558586,0.13249340653419495,-0.07007353752851486,-0.03372007980942726,0.0015595984878018498,0.08722078055143356,0.051734425127506256,-0.02877533994615078,0.012014213018119335,8.930929470807314E-4,0.022602789103984833,-0.054593853652477264,0.019649554044008255,-0.040468730032444,-0.03606779873371124,0.007904015481472015,0.1401429921388626,-0.006141105201095343,0.011966303922235966,0.08332695811986923,-0.029868191108107567,-0.041360702365636826,0.07679258286952972,-0.037893254309892654,-0.014088778756558895,0.10596615821123123,-0.08471810817718506,-0.04985795170068741,-6.256022812062462E-33,0.05500974878668785,-0.07352256029844284,-0.02111908048391342,-0.007210636045783758,-0.029800478368997574,0.029576042667031288,0.002158453455194831,-0.04086018353700638,-0.027272747829556465,-0.03917182981967926,-0.10161525756120682,0.0032513539772480726,0.11152090132236481,-0.048016250133514404,-0.005098198540508747,0.1144687682390213,0.08901537209749222,0.03167601302266121,-0.0890548899769783,3.0867179157212377E-4,0.001530165201984346,0.045596372336149216,-0.03920584172010422,0.003601674921810627,0.042502954602241516,0.04910886660218239,0.06267553567886353,0.05336722359061241,-0.10089462995529175,0.070661261677742,3.514362470014021E-5,-0.09156553447246552,-0.03512753173708916,0.07973548024892807,-0.03137069195508957,0.02019931934773922,0.05830610170960426,-0.017162349075078964,0.07344236224889755,0.04416573420166969,0.015525516122579575,-0.015229977667331696,0.006923812907189131,0.06264612078666687,0.033600952476263046,-0.026340849697589874,-0.04180547222495079,-0.06539328396320343,0.04650706797838211,-0.04912937432527542,0.06781158596277237,-0.0848030149936676,-0.07594794780015945,-0.008438600227236748,0.026303494349122047,-0.021785691380500793,-0.018796274438500404,0.0015642526559531689,0.002213675295934081,0.00941301416605711,0.050257183611392975,0.06171168386936188,-0.04914529621601105,0.09122132509946823,0.044531937688589096,-0.029402034357190132,0.05064532160758972,-0.019171398133039474,-0.051893819123506546,0.027455143630504608,-0.017428381368517876,-0.004062737803906202,-0.07508695125579834,0.08202024549245834,-0.06308198720216751,0.024798886850476265,-0.08135645091533661,0.06864731013774872,0.0047626434825360775,0.01712682843208313,-0.03257601708173752,-0.06712634116411209,-0.03399315103888512,-0.030381226912140846,-0.01049315370619297,0.015247643925249577,-0.013711264356970787,0.014843846671283245,0.08399519324302673,-0.06716043502092361,-7.75210908614099E-4,0.03151692822575569,-0.003450234653428197,-0.11557739228010178,-0.011472489684820175,-2.6269850295079777E-8,0.05680885165929794,0.03075326979160309,-0.10633032768964767,-0.025117216631770134,-0.0014916927320882678,-0.028787797316908836,0.02902422659099102,-0.013890596106648445,0.007930850610136986,0.037879034876823425,-0.009389090351760387,0.03282459080219269,0.0051073916256427765,0.029276300221681595,0.0035742332693189383,-0.009400456212460995,0.06691442430019379,0.058636803179979324,0.018528366461396217,-0.021059079095721245,0.06871410459280014,0.01726994849741459,-0.022720489650964737,-0.07251585274934769,-0.0272163525223732,0.00612653000280261,0.04049176722764969,-0.005333365872502327,0.021763553842902184,-0.07323212176561356,0.02034918963909149,0.02058650553226471,-0.059031520038843155,-0.09686363488435745,0.04168229550123215,0.06060207262635231,0.013797387480735779,-0.015274128876626492,-0.0905662477016449,-0.031691938638687134,0.08355686068534851,0.030773501843214035,-0.0702260285615921,-0.004496934823691845,0.009351974353194237,0.03191244974732399,-0.014127237722277641,-0.0679740384221077,0.013870438560843468,-0.04558093100786209,0.0017981621203944087,-0.012471281923353672,0.040619928389787674,0.09678950905799866,0.025067981332540512,0.011355657130479813,0.006818351801484823,0.01693277806043625,0.02253808081150055,0.0753825232386589,0.01718750409781933,0.06524177640676498,-0.10204480588436127,-0.022478923201560974]                         |\n",
      "|C481AD98-3EDA-11EB-845E-5FC45ADE36E3|28.46787 |-16.29529 |CALLE       |RECTOR JOSE CARLOS ALBERTO BETHANCOU|7    |38023 |San Cristóbal de La Laguna|CALLE RECTOR JOSE CARLOS ALBERTO BETHANCOU 7 SAN CRISTOBAL DE LA LAGUNA|[-0.022167548537254333,0.027170097455382347,-0.06687594950199127,0.03451745957136154,-0.08962924778461456,0.011657742783427238,-0.01224856823682785,0.054577767848968506,-0.006171553395688534,-0.009617825038731098,0.026852363720536232,-0.06297853589057922,-0.0026770569384098053,-0.029888834804296494,-0.03497452288866043,-0.02818267047405243,-0.06709431856870651,0.011782287620007992,0.07897397875785828,-0.012437079101800919,0.047887105494737625,0.0539010688662529,-0.06231139600276947,0.05034026876091957,-0.039014462381601334,-0.0551740936934948,0.00467729801312089,0.016700757667422295,-0.03724254295229912,-0.008061083033680916,0.02445181831717491,0.1460658609867096,0.12384674698114395,-0.021778788417577744,0.057318225502967834,0.012810613960027695,0.0045265695080161095,-0.08063416928052902,-0.0245005264878273,0.04804295301437378,-0.10230398923158646,0.04202475771307945,-0.051521699875593185,0.0226163137704134,-0.013182875700294971,-0.0490635447204113,0.023501064628362656,-0.008357372134923935,0.10630867630243301,-0.007669937796890736,-0.05763886123895645,0.015095042996108532,0.014067435637116432,0.048880964517593384,-0.032458093017339706,0.05346287041902542,0.058961011469364166,-0.03794557973742485,0.03454736992716789,0.04584593325853348,0.0442739762365818,0.08162498474121094,-0.10445468127727509,0.012256867252290249,-0.02600492164492607,0.007509271148592234,-0.05840562283992767,-0.054412029683589935,0.010187690146267414,-0.08581440895795822,0.03176274523139,-0.0329168364405632,0.017979085445404053,-0.02894076332449913,-0.003950123209506273,0.02597607672214508,-0.04531034082174301,-0.021689383313059807,-0.05636978894472122,-0.011689047329127789,-0.03584693744778633,-0.08704981207847595,0.009204728528857231,-0.09168248623609543,-0.023818587884306908,0.06307094544172287,0.02082536183297634,-0.04658330976963043,0.04856684431433678,0.08046669512987137,0.00660354970023036,0.02711116336286068,-0.03553406894207001,-0.049078334122896194,0.019513240084052086,0.03050846792757511,-0.019741157069802284,-0.060493506491184235,-0.0258677639067173,0.12186067551374435,-0.04354492202401161,0.06523038446903229,-0.0030785861890763044,0.016627805307507515,-0.04140301048755646,0.019708026200532913,0.046685703098773956,0.018235351890325546,-0.051779672503471375,-0.024978501722216606,-0.02129516564309597,-0.0363006517291069,-0.050373826175928116,-0.03004845418035984,0.03147474303841591,0.004805251955986023,0.11199969053268433,-0.019453711807727814,-0.09105284512042999,-0.08044738322496414,0.06081239506602287,-0.05499361827969551,-0.08508072048425674,-0.020447654649615288,-0.03112664259970188,-0.04791094735264778,0.030714981257915497,7.014038874822276E-33,0.056943077594041824,-0.006723142694681883,0.024432683363556862,0.04212679713964462,0.03254673257470131,-0.0033113781828433275,-0.05548318475484848,-0.0012930582743138075,-0.011281043291091919,-0.03848283365368843,0.017756151035428047,-0.05866635590791702,0.02028932236135006,-0.029064973816275597,0.024093151092529297,0.0811009556055069,-0.003442683955654502,-0.048124514520168304,-0.010844631120562553,0.031339406967163086,0.02458079159259796,0.028289591893553734,0.02377072535455227,-0.012316812761127949,0.07011154294013977,0.1338072121143341,-0.02170005440711975,0.003006385173648596,0.06257116794586182,0.04134891554713249,0.012106594629585743,-0.004861265420913696,0.0065315719693899155,-0.0027524924371391535,0.10795880854129791,0.032072514295578,0.005137174855917692,-0.027098355814814568,-0.07756360620260239,0.00240912358276546,0.06935647875070572,0.01662779413163662,0.028070621192455292,0.02282044105231762,0.055680058896541595,0.019988950341939926,0.03679550439119339,0.045893821865320206,0.12394928187131882,-0.044978171586990356,-0.0560338981449604,-0.020578259602189064,-0.11788874119520187,-0.03756749629974365,0.08750183135271072,-0.018967868760228157,-0.030995512381196022,0.046079572290182114,-0.010305014438927174,0.01944313943386078,0.07044235616922379,0.005153990816324949,-0.015479174442589283,0.07441583275794983,0.007467532530426979,-0.08312210440635681,-0.032422348856925964,0.031148621812462807,0.11820193380117416,-0.12407416850328445,-0.08971372246742249,-0.016246594488620758,0.09037154167890549,0.01845247857272625,-0.038167353719472885,-0.018554214388132095,-0.07917152345180511,-0.06450437009334564,0.004536516033113003,0.06741990149021149,-0.024973800405859947,-0.039103083312511444,0.006267516408115625,0.04447363317012787,0.07569737732410431,0.05024678632616997,0.034713950008153915,0.029390906915068626,0.004976337775588036,0.05146408826112747,-0.08825913816690445,0.07095012068748474,0.04508848488330841,0.032464127987623215,0.01574123650789261,-8.697436416644042E-33,0.05088113993406296,-0.07504379004240036,0.01857493817806244,0.012692115269601345,-0.009772755205631256,-0.002465088851749897,-0.06825986504554749,-0.008435416035354137,-0.033011484891176224,-0.12303712964057922,-0.045189183205366135,-0.017426664009690285,0.05947607383131981,-0.0799684152007103,0.024537736549973488,0.12113794684410095,0.06964027881622314,-0.07468888908624649,-0.14725123345851898,0.018492167815566063,0.03584008663892746,0.05815489590167999,-0.06085319072008133,-0.06361521780490875,0.00471830740571022,-0.0460406169295311,0.09119867533445358,0.05494053661823273,-0.012126723304390907,0.05949326977133751,-0.05493062734603882,-0.037110552191734314,-0.05591612309217453,0.05389941856265068,-0.06082841753959656,0.09626085311174393,0.042680997401475906,0.02399931661784649,-0.00934040267020464,0.08609973639249802,0.03898797929286957,-0.028223713859915733,-0.003968865144997835,-0.06919790804386139,-0.00462163845077157,-0.017848968505859375,-0.006681805010885,-0.037618737667798996,0.021611178293824196,-0.07867635786533356,0.015876103192567825,-0.06576285511255264,-0.07597804814577103,0.008623315021395683,0.09359094500541687,-0.011801132932305336,0.016531486064195633,-0.012800004333257675,0.01991141028702259,-0.031914252787828445,0.0992148220539093,0.017521949484944344,-0.022363992407917976,0.027654509991407394,-0.01810469478368759,-0.02792520262300968,-0.0024454225786030293,-0.006172809284180403,0.006287776865065098,0.0026099521201103926,0.058297935873270035,-0.03474365174770355,-0.05919930338859558,0.04845821112394333,-0.07111837714910507,0.04926126450300217,-0.07871124148368835,0.05517900362610817,-0.028969211503863335,0.05373484268784523,-0.015337285585701466,-0.05626221001148224,-0.0937681570649147,0.03940584138035774,0.06169217824935913,0.03812723606824875,0.032960034906864166,-0.044121284037828445,0.0913383811712265,-0.09053671360015869,-0.022731633856892586,0.010559980757534504,0.05856875702738762,-0.12361716479063034,-0.05127689614892006,-2.943114907338895E-8,0.029835496097803116,-0.04128635302186012,-0.03129881992936134,0.01859668269753456,0.07556687295436859,-0.056569818407297134,-0.00969848595559597,0.008025312796235085,-0.05699337273836136,0.0768156424164772,-0.03411547839641571,0.03706234320998192,-0.00435155164450407,0.010812753811478615,0.039610959589481354,0.0015703084645792842,0.05268746614456177,0.03463694080710411,0.004279928747564554,-0.019860954955220222,-0.013097494840621948,-0.04761963337659836,-0.04695751518011093,-0.040468838065862656,-0.06003030017018318,-0.010809065774083138,-0.004884852562099695,0.06117037311196327,-0.005190761759877205,5.902614211663604E-4,0.014716994017362595,-0.006554439198225737,-0.05501417815685272,-0.12688034772872925,0.02993537299335003,0.0547739714384079,0.002438546856865287,-0.0522833876311779,0.00836141500622034,-0.05757414549589157,0.03169529139995575,-0.018293051049113274,0.009528916329145432,0.04532995820045471,0.01582495868206024,-0.01641000807285309,-0.004375912714749575,-0.017607292160391808,0.05254979804158211,-0.009088867343962193,0.036476604640483856,-0.03103303536772728,0.06194606050848961,-0.016920192167162895,0.05709254369139671,0.03926210477948189,0.06705604493618011,-0.07132638245820999,-0.04587678611278534,0.04869134724140167,-0.04661073535680771,-0.0585395023226738,-0.10845997929573059,-0.0664350762963295]                                           |\n",
      "|C0E2B765-0AD1-11EA-8401-7756D0C25261|28.46712 |-16.2845  |CALLE       |VIRGEN SALETTE                      |1    |38023 |San Cristóbal de La Laguna|CALLE VIRGEN SALETTE 1 SAN CRISTOBAL DE LA LAGUNA                      |[-0.02877645753324032,0.027160530909895897,0.007802125532180071,-0.007998518645763397,-0.12910282611846924,0.018476000055670738,0.035560980439186096,0.07619236409664154,0.01901884190738201,-0.07299196720123291,0.03917917609214783,-0.1317022442817688,0.0019363004248589277,-0.0667378380894661,9.238257771357894E-4,-0.08169322460889816,0.003447755007073283,0.033233676105737686,0.06411188840866089,0.053481943905353546,0.034028567373752594,-0.002118993317708373,-0.07173126935958862,0.014065220952033997,-0.02689162828028202,-0.05739365890622139,-0.032166335731744766,0.09079749882221222,-0.04273277893662453,-0.036631982773542404,0.04489165544509888,0.14198073744773865,0.078553207218647,0.039870619773864746,0.04951038211584091,0.02665957808494568,-0.018488073721528053,-0.10362324118614197,-0.04485267400741577,0.0975833535194397,-0.0878196507692337,0.018102088943123817,-0.07889563590288162,-0.0029626067262142897,0.003717281622812152,-0.003825217252597213,-0.002598449354991317,0.08084217458963394,0.008335115388035774,-0.04657615348696709,-0.03844427689909935,0.009238913655281067,0.013058282434940338,0.10811260342597961,-0.06086617335677147,0.0844915360212326,0.05521078407764435,-0.032621949911117554,0.06783434748649597,0.037973422557115555,0.06027060002088547,0.010147866792976856,-0.028208689764142036,-0.055826034396886826,-0.10931264609098434,0.032179687172174454,-0.018779119476675987,-0.04621878266334534,-0.06918185949325562,-0.007817034609615803,-0.013212790712714195,0.0025747905019670725,-0.02517108991742134,-0.02138013392686844,-0.06242699921131134,0.039760589599609375,-0.10538380593061447,-0.08955001831054688,-0.026568422093987465,-0.045010533183813095,-0.014122884720563889,-0.0831419974565506,-0.004000975284725428,-0.038857050240039825,0.019414830952882767,0.06154131516814232,0.059257082641124725,-0.055225569754838943,0.08699556440114975,-0.005251334514468908,-0.1069013923406601,0.04032643139362335,-0.14256833493709564,-0.08443780243396759,-0.053122665733098984,0.01879081130027771,-0.005502115935087204,-0.04096531122922897,0.0452333465218544,0.05346154794096947,-0.0381053127348423,0.010427407920360565,-0.04472515359520912,0.0033126308117061853,-0.026453325524926186,-0.016281608492136,0.06660792231559753,-0.001352861407212913,-0.06352623552083969,-0.013679021038115025,-0.03560158610343933,-0.015470246784389019,0.004324445966631174,-0.06259621679782867,0.02771383337676525,0.04997220262885094,0.06822164356708527,-0.038493577390909195,-0.015243060886859894,-0.05945301055908203,0.08620063215494156,-0.006716521922498941,-0.04219333454966545,-0.07289402186870575,-0.05133412033319473,-0.06407316029071808,0.08581355959177017,4.968872660959381E-33,-0.0290329959243536,-0.06588199734687805,0.034316059201955795,0.06630386412143707,0.07231516391038895,0.02177976444363594,-0.057431139051914215,-0.0018394890939816833,-0.041554778814315796,-0.005208318587392569,-0.01969219371676445,-0.10664566606283188,0.007756970822811127,-0.05722241476178169,0.04047299921512604,0.056744541972875595,0.06138879433274269,-0.0816715806722641,-0.026619285345077515,-0.04454841464757919,3.739783714991063E-4,0.018860789015889168,0.023816846311092377,0.03652307763695717,-0.07178299129009247,0.015017173252999783,-1.0114897304447368E-4,-0.013883011415600777,0.036246463656425476,0.011809398420155048,8.017169311642647E-4,-0.00726501876488328,0.0539979450404644,0.00998265016824007,0.020216669887304306,-0.03521408140659332,-0.006502761971205473,-0.017039697617292404,-0.018728135153651237,0.018333444371819496,0.038795460015535355,0.047744493931531906,0.006544813513755798,-0.02937597781419754,0.025124909356236458,0.07154490053653717,-0.004343374166637659,0.03220521658658981,0.13951899111270905,0.004140041768550873,-0.037463702261447906,0.017316408455371857,-0.08070899546146393,0.047461576759815216,0.009280016645789146,0.0055182743817567825,-0.02793802320957184,0.009533894248306751,-0.06458043307065964,0.0033234695438295603,0.07080895453691483,0.05746730417013168,0.012022629380226135,0.0012529219966381788,-0.052817005664110184,-0.03162508085370064,0.0220604557543993,0.013635432347655296,0.07000744342803955,-0.08218470960855484,-0.04248043894767761,0.015697350725531578,0.08341237157583237,0.05237436294555664,-0.01995699852705002,-0.020657865330576897,1.811551337596029E-4,0.0038897886406630278,0.004797073546797037,-0.0012608750257641077,-0.049910109490156174,0.009325562044978142,0.04923690855503082,0.0815841406583786,0.05457757040858269,0.006571446545422077,0.05837547779083252,-0.037930089980363846,-0.016897741705179214,0.03949587047100067,-0.01903442107141018,0.05037461593747139,0.03183581680059433,-0.04198550805449486,0.01940181851387024,-5.400222884387512E-33,0.034832313656806946,-0.04519101604819298,0.0064886934123933315,0.013987796381115913,-0.02859952114522457,0.07545669376850128,-0.05611811578273773,0.006102818064391613,-0.004049229901283979,-0.013744969852268696,-0.10155491530895233,-0.03292183578014374,0.06671234220266342,-0.09509661048650742,0.05963334068655968,0.12356583029031754,0.1140943244099617,-0.03706398978829384,-0.05121858790516853,0.01408908236771822,0.0028796419501304626,0.05286342278122902,-0.0076036881655454636,0.042602669447660446,-0.025258075445890427,-0.012186343781650066,0.1064310222864151,0.11288663744926453,-0.023640085011720657,-0.009659826755523682,-0.006050093099474907,-0.06312543153762817,0.005035710986703634,0.08536721020936966,0.011298373341560364,0.025369983166456223,0.08540811389684677,-0.014643198810517788,0.10977847874164581,0.03176471218466759,-0.053181242197752,-0.04480251669883728,0.005411924794316292,-0.005211496260017157,-0.02629263885319233,-0.004528374876827002,-0.034390974789857864,0.022787857800722122,0.008862574584782124,-0.11718876659870148,0.01950477994978428,-0.03260708600282669,-0.07844210416078568,-0.006856068037450314,0.01642542891204357,-0.03834664449095726,0.017226768657565117,-0.0502217672765255,-0.007524474058300257,0.04312644153833389,0.1228971779346466,0.1020723283290863,-0.007500472012907267,0.01842493563890457,0.022332539781928062,-0.011807710863649845,-0.016597580164670944,-0.03405481204390526,0.011042100377380848,0.018217064440250397,0.040216829627752304,-0.012912334874272346,-0.08885472267866135,0.032184477895498276,-0.1145356222987175,0.012387423776090145,-0.10794676840305328,0.036988019943237305,-0.007381652947515249,0.05662662163376808,-0.04311380535364151,-0.052499596029520035,-0.05455087870359421,0.005751630291342735,0.036256857216358185,0.036093734204769135,-0.009021855890750885,-0.01133143063634634,0.03336735814809799,-0.07053913176059723,-0.02486795373260975,0.014113925397396088,0.00231070164591074,-0.07392551749944687,-0.007499147672206163,-2.3442842334020497E-8,0.0999768003821373,-0.038900744169950485,0.014569451101124287,0.014377299696207047,0.049825478345155716,-0.09159336239099503,0.011104870587587357,0.036184847354888916,0.07767552137374878,0.04213982820510864,-0.06424081325531006,-0.05098035931587219,0.04950512945652008,0.04752727970480919,-0.05082911252975464,0.08095023781061172,0.08118827641010284,0.03481404110789299,-0.029964016750454903,-0.00970044918358326,0.044825732707977295,-0.07639271765947342,-0.03539305180311203,-0.11047829687595367,-0.03165373206138611,3.710140590555966E-4,-9.826994501054287E-4,0.04641168564558029,0.002634294331073761,-0.06061972305178642,0.07592544704675674,0.008467059582471848,-0.007180529180914164,-0.0393027737736702,-0.024481898173689842,0.04598274827003479,-0.02972368337213993,0.02808339148759842,-0.029229160398244858,-0.03067537397146225,0.10548217594623566,-0.030654801055788994,0.010471646673977375,0.024056335911154747,-0.004693583119660616,0.04858443886041641,0.006035837344825268,-0.038112521171569824,0.09437072277069092,0.0029887608252465725,0.003077933331951499,0.03610958158969879,0.0682772696018219,0.04729650542140007,-0.033658094704151154,0.02806473709642887,0.04992946609854698,0.022429952397942543,0.01983012817800045,0.04148391634225845,-0.05231606960296631,0.015289613045752048,-0.0901491567492485,-0.003610118292272091]                              |\n",
      "|67842392-3251-11E8-B06D-480FCF5217B3|28.478521|-16.308836|CALLE       |ESC AGUIAR SOTO RES DRAGO FASE II   |30   |38023 |San Cristóbal de La Laguna|CALLE ESC AGUIAR SOTO RES DRAGO FASE II 30 SAN CRISTOBAL DE LA LAGUNA  |[-0.05304202437400818,-0.03305121138691902,-0.024299923330545425,0.06264889985322952,-0.04121530428528786,-0.0028910506516695023,0.008402279578149319,0.06759113818407059,0.022066764533519745,-0.013726279139518738,0.03841707482933998,-0.10259344428777695,-0.03589184209704399,0.013471710495650768,-0.018421834334731102,-0.046088483184576035,-0.04412459954619408,0.02007312886416912,0.016565458849072456,0.048577722162008286,0.07329419255256653,-0.036451518535614014,-0.08130356669425964,0.07961928099393845,-0.030793428421020508,-0.04719274863600731,-0.019044166430830956,0.06425261497497559,0.014263778924942017,-0.07248439639806747,0.01746826246380806,0.1440955251455307,0.043408166617155075,-0.0022111989092081785,-0.008905393071472645,0.004622754640877247,0.027806267142295837,-0.11730863153934479,-9.15295269805938E-4,0.004795155953615904,-0.06557647138834,-0.006521081086248159,-0.04386511445045471,-0.009134192951023579,-0.01967700384557247,-0.010313624516129494,0.027647335082292557,0.0666862204670906,0.008707687258720398,0.023750105872750282,-0.05920397862792015,-0.019023347645998,-0.08188115060329437,0.04535618796944618,-0.06590796262025833,0.0608016774058342,0.08724101632833481,-0.02595403790473938,0.11056657880544662,0.03383028879761696,0.044044867157936096,-0.01568710431456566,-0.075035460293293,0.040876615792512894,-0.06692146509885788,-0.019338935613632202,-0.043819133192300797,-0.019414184615015984,-0.037562161684036255,-0.08875462412834167,0.0402987077832222,-0.026816120371222496,0.010640369728207588,0.020847197622060776,0.008785853162407875,0.04306595399975777,-0.09050949662923813,0.024700164794921875,-0.03096415475010872,0.01909567229449749,0.04457294940948486,-0.04308776929974556,0.03090544603765011,-0.03981184959411621,-0.012711883522570133,0.03448743373155594,0.01714802160859108,-0.05353570356965065,0.052154868841171265,0.02365828864276409,-0.0262600090354681,0.03109733574092388,-0.07544457912445068,-0.04384618252515793,-0.03599550947546959,0.01594582386314869,0.0017291123513132334,-0.03211553394794464,0.01120584923774004,0.04475070908665657,0.007478265557438135,0.045224517583847046,-0.015577309764921665,0.017065484076738358,-0.07044193148612976,-0.04075342416763306,0.07783252000808716,-0.044459354132413864,-0.09642937034368515,0.05475069582462311,-0.026793353259563446,-0.030307382345199585,0.0014462944818660617,-0.08997341245412827,0.011790222488343716,0.030034862458705902,0.039027996361255646,-0.024661920964717865,0.02791748382151127,-0.06220794841647148,-0.0034923702478408813,-0.005952952895313501,-0.035665638744831085,-0.09390970319509506,0.03503008931875229,-0.08885496854782104,0.030096285045146942,1.0519677473713182E-32,-0.0057122171856462955,-0.01889454573392868,-0.013365862891077995,0.052903637290000916,0.060027774423360825,-0.02623244747519493,-0.034214746206998825,-0.047291528433561325,0.0057778917253017426,-0.043115872889757156,0.008217450231313705,-0.04266124591231346,-8.623668691143394E-4,-0.04472782090306282,0.029865549877285957,0.0695459172129631,0.02556643635034561,-0.04736986383795738,-0.03157681226730347,-0.039266087114810944,-0.017439041286706924,-0.037126075476408005,0.020469071343541145,-0.055792879313230515,-0.011187507770955563,0.050432972609996796,-0.04320531710982323,-0.018763283267617226,-0.04237368330359459,0.0670521929860115,0.04202951490879059,-0.005191506817936897,-0.01675846427679062,0.0016232877969741821,0.06947633624076843,-0.05654667690396309,0.03399455547332764,0.004362894222140312,-0.039288394153118134,0.0020374651066958904,0.043059349060058594,0.04577043280005455,-0.0019730848725885153,0.03766850754618645,0.04296104982495308,-0.022939056158065796,0.08044857531785965,0.04663034901022911,0.07728574424982071,0.007722414564341307,-0.0885334461927414,-0.027072463184595108,-0.03676370903849602,0.025996096432209015,0.09571211040019989,-0.009696711786091328,-0.0607931949198246,-0.00420651538297534,-0.04073159024119377,-0.004258651286363602,0.0730554535984993,0.09148342907428741,-0.02619776874780655,-0.0023680299054831266,0.029473690316081047,-0.03617530316114426,-0.005853450391441584,0.05876274034380913,0.14398230612277985,-0.014311770908534527,-0.0152540672570467,-0.04930251091718674,0.09737429022789001,0.0401754267513752,-0.0295366533100605,-0.04153946042060852,0.014608513563871384,-0.06850721687078476,-0.02701769582927227,0.013463116250932217,-0.05021406337618828,-0.006547864992171526,0.03913761302828789,0.027389628812670708,0.07618533074855804,0.005889753811061382,0.04015069454908371,-0.007139051798731089,-0.008139782585203648,0.03851379454135895,-0.0199040025472641,0.034672416746616364,0.025431886315345764,-0.027112126350402832,0.03365618363022804,-9.505944369875488E-33,0.10368603467941284,-0.0262571070343256,-9.97790601104498E-4,0.007415460422635078,-0.03471556305885315,0.003183184890076518,-0.05584755912423134,-0.02099709026515484,0.07946498692035675,-0.07556919753551483,-0.1345197856426239,-0.001845975173637271,0.11048337817192078,-0.044270943850278854,0.08354107290506363,0.06689032912254333,0.09379757195711136,-0.11997391283512115,-0.06207479536533356,-0.005280209705233574,0.0500061959028244,0.022502902895212173,-0.0357278436422348,0.00896513182669878,0.0069884974509477615,-0.034905169159173965,0.02836676314473152,0.036010369658470154,-0.04830663278698921,0.010243461467325687,0.04065760225057602,-0.12778261303901672,-0.002078291494399309,0.07042238861322403,-0.05482925474643707,-0.0833076611161232,0.11993621289730072,0.036188121885061264,0.023909755051136017,0.00190546119119972,0.028695063665509224,-0.029309915378689766,0.00995431374758482,-0.020029393956065178,-0.017385881394147873,-0.007664868142455816,-0.005319694522768259,-0.010610425844788551,0.02837902307510376,-0.1275579184293747,0.05260734260082245,-0.08520234376192093,-0.11828947067260742,0.003835906507447362,0.0804886668920517,-0.06249282509088516,0.0762321949005127,-0.08115895837545395,-0.0576801598072052,-0.004127660766243935,0.09068227559328079,0.037263110280036926,-0.07234840095043182,-0.0331091582775116,0.043390367180109024,0.019683346152305603,0.0048935143277049065,-0.00649687135592103,-0.00689086876809597,0.07462704181671143,-0.027016304433345795,-0.015032791532576084,-0.07162453979253769,0.042274896055459976,-0.019426479935646057,-0.014293200336396694,-0.05762112885713577,0.16517779231071472,0.06688763946294785,0.08628037571907043,0.018143977969884872,-0.03126529976725578,-0.06559749692678452,-0.01524142362177372,0.048935532569885254,0.023413825780153275,-0.08804310113191605,-0.024678410962224007,0.041758447885513306,-0.08382441848516464,0.013894075527787209,0.03047681786119938,0.026913434267044067,-0.04053603112697601,-0.01741514354944229,-3.2206699529524485E-8,0.06355366855859756,-0.01587008684873581,0.005795245058834553,0.031244337558746338,0.007516976445913315,-0.05838368460536003,-0.01762707717716694,0.042283978313207626,-0.0057401941157877445,0.0610981322824955,-0.048705872148275375,-0.030154472216963768,0.10536720603704453,0.09575670212507248,0.004564924165606499,0.0158464964479208,0.12942463159561157,0.06078546494245529,-0.05642933398485184,-0.0863116905093193,0.11242149770259857,-0.08125502616167068,0.007559387478977442,-0.054267700761556625,-0.05145026743412018,-0.030497543513774872,-0.038636527955532074,-0.03171428665518761,-0.037032101303339005,-0.03494258597493172,-0.006986327935010195,-0.006292686332017183,-0.03313026204705238,-0.0573883093893528,-0.014481253921985626,0.07071252167224884,0.010302860289812088,0.024423634633421898,-0.06130707263946533,0.016720116138458252,0.09884537756443024,0.05840027332305908,-0.01091157365590334,0.016467273235321045,0.1131342351436615,0.049809664487838745,-0.019562900066375732,-0.04373061656951904,-0.014240146614611149,-0.0071836207062006,-0.00404930068179965,-0.04811500012874603,0.0797470360994339,0.10882172733545303,0.0037372864317148924,0.002465241588652134,0.002441917546093464,-0.02033263072371483,-0.03481347858905792,0.05400483310222626,0.04129929468035698,0.05027439072728157,-0.08376419544219971,-0.015992561355233192]                                 |\n",
      "|C0CA35F2-0AD1-11EA-8401-7756D0C25261|28.4955  |-16.31458 |CALLE       |BARRANCO GONZALIANO                 |1    |38023 |San Cristóbal de La Laguna|CALLE BARRANCO GONZALIANO 1 SAN CRISTOBAL DE LA LAGUNA                 |[-0.02602846547961235,0.026375075802206993,-0.061660442501306534,0.027348557487130165,-0.1299242079257965,-0.010270791128277779,0.046677105128765106,0.0574943944811821,0.02355528622865677,-0.08042366057634354,0.03942561522126198,-0.14489798247814178,-0.025499491021037102,0.009694610722362995,0.013466562144458294,-0.05147429183125496,-0.011058483272790909,-0.014003537595272064,0.08451241999864578,0.02684369497001171,0.03616093471646309,0.0016103544039651752,-0.052437737584114075,0.08588811010122299,-0.0355021096765995,-0.026622986420989037,0.0020935102365911007,0.06843896955251694,-0.0364338755607605,-0.01980663277208805,0.041045963764190674,0.15110351145267487,0.12282782047986984,-0.009351835586130619,-0.008714986965060234,0.07408195734024048,-0.010626498609781265,-0.09061860293149948,0.006047217175364494,0.06290248036384583,-0.09254882484674454,0.009354001842439175,-0.043150395154953,-0.018753429874777794,5.111928912810981E-4,-0.017536651343107224,-0.007402060553431511,0.06908021867275238,0.05999794602394104,-0.039530977606773376,-0.047263145446777344,0.03313593193888664,-0.006259523797780275,0.04820288345217705,-0.012649706564843655,0.05530151352286339,0.05020787566900253,-0.019915597513318062,0.1014224961400032,0.06879464536905289,0.04591232165694237,-0.006630691234022379,-0.06049783527851105,0.032644324004650116,-0.047641366720199585,0.06261307001113892,-0.06824904680252075,-0.04542635753750801,-0.06902486830949783,0.001234462484717369,0.03985399752855301,-0.02693880908191204,0.021095475181937218,-0.04041937738656998,-0.0484667643904686,0.05826684832572937,-0.0598377026617527,0.00957884918898344,-0.0530334934592247,-0.012641883455216885,-0.03504233807325363,-0.07856019586324692,-0.010633491910994053,-0.0526701919734478,-0.03754808381199837,0.0730840340256691,-0.012060481123626232,-0.026508718729019165,0.05081099271774292,0.007000486832112074,-0.012174678966403008,0.03848361223936081,-0.06397039443254471,-0.057765621691942215,-0.006718624383211136,0.018429039046168327,-0.0010792354587465525,-0.04515362158417702,0.039633262902498245,0.04202652350068092,0.011153441853821278,0.019954195246100426,-0.013573977164924145,0.010959049686789513,-0.023872099816799164,0.004994141403585672,0.1053101047873497,-0.0010574358748272061,-0.016120463609695435,-0.0083007225766778,-0.03560474514961243,-0.04376570135354996,-0.013671725988388062,-0.07036883383989334,-0.02060963585972786,0.03363559767603874,0.09628386050462723,-0.027861498296260834,-0.04021705687046051,-0.031933654099702835,0.05747625231742859,-0.04363534227013588,-0.042357467114925385,-0.05465100333094597,-0.022954663261771202,-0.06066769361495972,0.03874816372990608,7.103233916794732E-33,0.03932870551943779,-0.05451812967658043,0.02806549333035946,0.06234237179160118,0.0999670997262001,0.025912223383784294,-0.08423829823732376,-0.06622789055109024,-0.02713756076991558,1.146085633081384E-4,0.012557596899569035,-0.08952052891254425,-0.026011168956756592,-0.05130381137132645,0.04427995905280113,0.06615951657295227,-0.002538625616580248,-0.08990729600191116,-0.0620831735432148,-0.006202888675034046,0.004148452077060938,-0.03482523933053017,0.044440560042858124,-0.009478247724473476,-0.00455978699028492,0.09605849534273148,0.007453927304595709,-0.04574255645275116,-0.015834985300898552,0.027013296261429787,-0.026411091908812523,-0.005524849519133568,0.020693683996796608,0.02824721857905388,0.05661642178893089,-0.02442360855638981,-0.013187289237976074,-0.012455553747713566,-0.05281752720475197,0.01950766146183014,0.05002696439623833,-0.002005366375669837,-0.038987476378679276,0.027524735778570175,0.040406934916973114,-0.012176144868135452,0.07385328412055969,0.049302466213703156,0.11900224536657333,0.04214450344443321,-0.06319434940814972,-0.007907816208899021,-0.12545254826545715,0.024379432201385498,0.004769257735460997,0.002701563062146306,-0.07092997431755066,0.012556355446577072,0.0029569584876298904,-0.0018450937932357192,0.07831481099128723,0.06499570608139038,0.023616723716259003,0.003979784902185202,0.017911359667778015,0.02358625829219818,0.030086562037467957,0.036571647971868515,0.10395887494087219,-0.10069654881954193,0.003984870854765177,-0.05112431198358536,0.06613220274448395,0.09882818907499313,-0.05328090488910675,-0.01181444339454174,-0.012220117263495922,0.003933081869035959,-0.016074681654572487,0.043056923896074295,-0.02497965656220913,0.010265619494020939,0.011401107534766197,0.05880235508084297,0.04083063453435898,0.09957044571638107,0.046949323266744614,-0.052964016795158386,-0.057232294231653214,0.052094001322984695,-0.0345965214073658,0.045846011489629745,0.04896879196166992,-0.06985529512166977,0.0066770268604159355,-6.996022219844016E-33,0.08758053183555603,-0.09298688918352127,0.03619300574064255,-0.047317881137132645,-0.014789523556828499,-0.01646980457007885,-0.04609325900673866,0.027143970131874084,-0.006382354069501162,0.02602798119187355,-0.07657255977392197,-0.008794216439127922,0.08077047765254974,-0.10559424012899399,0.04672443866729736,0.13499470055103302,0.12840516865253448,-0.05183061584830284,-0.09329505264759064,-0.006033606827259064,0.016185013577342033,-0.005861340556293726,-0.05486683547496796,-0.011421902105212212,0.01619858853518963,0.02910119853913784,0.06937047839164734,0.059946320950984955,-0.036796968430280685,0.011114931665360928,-0.0761561244726181,-0.06787067651748657,-0.035487815737724304,0.057319629937410355,-0.005304309073835611,0.0048776655457913876,0.023730816319584846,0.0581279881298542,0.06778417527675629,5.131080251885578E-5,-0.024022886529564857,-0.03360665217041969,0.020978674292564392,-0.03229865804314613,-7.256793323904276E-4,-0.007403136231005192,-0.05941537395119667,-6.03661232162267E-4,-0.016874969005584717,-0.11742214858531952,0.023289434611797333,0.02145799621939659,-0.1008329838514328,0.053739335387945175,0.077602818608284,-0.041566912084817886,0.019695809110999107,-0.05188317969441414,-0.04431978613138199,-0.029221458360552788,0.05028872936964035,0.0012853292282670736,-0.032065875828266144,0.019627133384346962,0.05407695844769478,-0.017734592780470848,-0.05607834830880165,-0.05983521789312363,-0.024183664470911026,0.05080059915781021,0.016301682218909264,-0.05146967992186546,-0.08981246501207352,0.04040361940860748,-0.13745886087417603,0.055644914507865906,-0.0970320999622345,0.05982428416609764,0.013643399812281132,0.094063401222229,0.054926443845033646,-0.02852492779493332,-0.07801863551139832,0.0410611554980278,0.09008481353521347,0.012770905159413815,0.0068907481618225574,0.028647836297750473,0.03895232826471329,-0.07265827804803848,0.012703503482043743,0.043768174946308136,-0.025947576388716698,-0.09296245127916336,-0.002421601442620158,-2.653622566128888E-8,0.06421671062707901,-0.03160889074206352,-0.003027841215953231,0.01981893926858902,0.06787607818841934,-0.042964231222867966,-0.01786522939801216,-0.005049442406743765,-0.024643195793032646,0.01652371510863304,-0.04504670947790146,-0.059818826615810394,0.03340219333767891,0.028569800779223442,-0.04240776598453522,0.05267719179391861,0.05690671131014824,0.048895761370658875,-0.002007066272199154,-0.018991347402334213,0.017084065824747086,-0.09805265069007874,-0.046490564942359924,-0.0733046755194664,-0.03656025975942612,-0.022685373201966286,-0.02460789680480957,0.055953171104192734,0.013812444172799587,-0.06953252851963043,0.030200058594346046,-0.02150433510541916,-0.02186768129467964,-0.05682884529232979,0.00838976725935936,0.044688235968351364,-0.07750965654850006,-0.012014930136501789,-0.048450879752635956,-0.043532952666282654,0.06061974912881851,0.04999007284641266,0.025914577767252922,0.033658456057310104,-0.0394320972263813,0.006131207104772329,-0.02747412957251072,-0.04067254811525345,0.05197577923536301,0.015393330715596676,0.0025075029116123915,0.005357011687010527,0.11170434951782227,0.0437593087553978,0.043057817965745926,0.029124850407242775,0.05290929973125458,0.022343672811985016,-0.05656731501221657,0.14167238771915436,-0.025121137499809265,0.009013097733259201,-0.09198451787233353,-0.0016460998449474573]                      |\n",
      "|6F510D00-3251-11E8-B9BD-480FCF5217B3|28.495062|-16.312155|URBANIZACION|TORRELAGUNA                         |23   |38023 |San Cristóbal de La Laguna|URBANIZACION TORRELAGUNA 23 SAN CRISTOBAL DE LA LAGUNA                 |[0.07257053256034851,0.031206384301185608,-0.013811328448355198,0.04843785613775253,-0.04156806692481041,0.011929398402571678,5.064951255917549E-4,0.02102097123861313,-0.06953354924917221,-0.027105150744318962,0.025168152526021004,-0.16259686648845673,0.003756178542971611,0.008350914344191551,0.05225702002644539,7.794908597134054E-4,-0.034353069961071014,0.01750195026397705,0.06624191254377365,0.002251188736408949,0.01767672784626484,-0.05515465885400772,-0.06756466627120972,0.05780380219221115,0.019843850284814835,0.06483055651187897,-0.008292779326438904,0.05321957916021347,-0.035793766379356384,-0.05108991637825966,0.01890750788152218,0.15448033809661865,0.097859226167202,-0.010565133765339851,0.04619261249899864,0.026897525414824486,-0.05253058671951294,-0.08461418747901917,0.0038561297114938498,0.0639495775103569,-0.049153976142406464,-0.008329245261847973,-0.02693408913910389,0.020130155608057976,-9.720268426463008E-4,-0.05099307745695114,0.04047735780477524,0.028540784493088722,0.04154982790350914,-0.056536417454481125,-0.05077863112092018,0.039053965359926224,-0.02901713363826275,0.08221016824245453,-0.05804482474923134,0.00937203411012888,0.03791685029864311,-0.014469539746642113,0.1192077025771141,-0.025726092979311943,0.08902207016944885,0.031067026779055595,-0.06905277073383331,-0.0025880380999296904,0.02183070406317711,-0.004031417425721884,-0.031169787049293518,-0.021991444751620293,0.03247508406639099,-0.09540513157844543,0.08195663243532181,-0.01951056346297264,0.03500458970665932,-0.00785667635500431,-0.016204137355089188,-0.04322531819343567,-0.005797767546027899,-0.007160567678511143,-0.05560950189828873,-0.010520766489207745,-0.04392388463020325,-0.03401617705821991,-0.014208043925464153,-0.04063061252236366,-0.0691298097372055,0.028559774160385132,-0.03234093636274338,-0.03464597091078758,0.04041237384080887,0.024447431787848473,0.012489511631429195,0.12196926027536392,-0.09943389892578125,-0.040466830134391785,-0.03768518194556236,0.04052381590008736,-0.06353183090686798,-0.10725005716085434,0.06547294557094574,0.0535566471517086,0.0064917257986962795,0.03068212792277336,-0.03493352234363556,-0.01220499537885189,-0.024270055815577507,-0.038873132318258286,0.08243799209594727,-0.011014068499207497,-0.09397497028112411,0.035660143941640854,-0.04367923364043236,-0.056948207318782806,-0.01578567735850811,-0.04835160821676254,-0.024599257856607437,0.048245590180158615,0.004073672462254763,0.016896875575184822,-0.030179455876350403,-0.007197481580078602,0.03565235435962677,-0.0818643569946289,-0.08496948331594467,-0.06865626573562622,-0.009440825320780277,-0.006714469287544489,-0.0023837480694055557,2.8705685923095497E-33,-0.02636081539094448,-0.042326681315898895,-0.01645238883793354,0.10580534487962723,0.06747864931821823,-0.020723484456539154,-0.01835951954126358,-0.043376900255680084,-0.03284391760826111,-0.05594705045223236,0.01140026655048132,-0.12014762312173843,-0.04954385757446289,-0.027840277180075645,0.1447695940732956,0.05064992234110832,-0.018490178510546684,-0.06551594287157059,-0.04627903178334236,0.004133452661335468,0.008911007083952427,0.044908687472343445,-0.047089118510484695,-0.06646397709846497,-0.03623859956860542,0.037474531680345535,0.03548023849725723,-0.006983827333897352,-0.02759632095694542,0.0590374656021595,-0.027090325951576233,0.040605444461107254,0.0039629219099879265,0.009261432103812695,0.06540253013372421,9.6606690203771E-4,-0.03200475126504898,0.024497557431459427,-0.05299847573041916,0.05444212257862091,0.02634979970753193,0.00120752293150872,0.01592789776623249,0.008690392598509789,0.07082154601812363,0.06132116541266441,0.08984874933958054,-0.010537193156778812,0.07736789435148239,-0.0018800051184371114,0.006229817867279053,-0.0015933832619339228,-0.1320437341928482,0.023248467594385147,0.010143420659005642,0.056244201958179474,-0.08281168341636658,-0.04914017394185066,-0.049763135612010956,0.03933549299836159,0.045326050370931625,0.043491631746292114,0.020605269819498062,0.012345867231488228,0.03439394384622574,-0.012853361666202545,0.08098175376653671,0.09972438216209412,0.19032585620880127,-0.03692151978611946,-0.05372275039553642,-0.052308861166238785,0.09718374162912369,0.04823780804872513,-0.05889755114912987,-0.05915036424994469,-0.03891439363360405,-0.001525803585536778,-0.029144706204533577,0.04452430084347725,-0.06840772926807404,-0.014767568558454514,0.009364607743918896,0.0649285539984703,0.08344718813896179,0.04330941662192345,0.05589626729488373,0.005353341810405254,0.011446719989180565,0.02553117647767067,-0.051276467740535736,0.04043598845601082,0.03695879876613617,-0.055215224623680115,0.003887646598741412,-3.8022907613682344E-33,0.0560319647192955,-0.06745530664920807,-0.0037873287219554186,-0.04903662949800491,-0.05559065565466881,0.01200657244771719,-0.08338312059640884,-0.033273059874773026,-0.008744185790419579,0.007422809489071369,-0.12662282586097717,-0.029117615893483162,0.10175349563360214,-0.08792296051979065,0.011137024499475956,0.07416830211877823,0.13056105375289917,-0.0688612163066864,-0.11723213642835617,0.02313498966395855,-0.04911917448043823,0.006003913469612598,-0.07296980172395706,-0.00913246814161539,0.021627778187394142,-0.02214866317808628,-0.005083625670522451,0.04709069803357124,0.008796358481049538,0.02924417145550251,-0.016906775534152985,-0.0738738402724266,-0.04395664483308792,0.04855676367878914,-0.018392637372016907,0.045616742223501205,0.09349611401557922,-0.034736838191747665,0.022053474560379982,0.03879166021943092,-0.017529940232634544,-0.04711269959807396,-7.413344574160874E-4,0.006714287213981152,-0.020845705643296242,0.03148477151989937,-0.00981815718114376,-0.029309984296560287,-0.030390165746212006,-0.09455810487270355,0.12065912038087845,-0.03821069747209549,-0.10298321396112442,-0.019797440618276596,0.05403143912553787,-0.04551420733332634,-0.008924278430640697,0.05417269840836525,-0.02430706098675728,-0.011183970607817173,0.08675029873847961,0.02263672463595867,-0.05158954858779907,0.034789662808179855,-0.0021679357159882784,-0.019456250593066216,0.016955561935901642,-0.04663556069135666,0.014971869066357613,0.011489136144518852,0.006491940934211016,-0.021058669313788414,-0.06644131243228912,0.09240394830703735,-0.08781208842992783,0.0023735451977699995,-0.011295093223452568,0.09944570064544678,0.05064013600349426,0.00203156191855669,0.013633591122925282,-0.021682148799300194,-0.0854773297905922,-0.015561575070023537,0.0447712205350399,-0.02929305098950863,-0.028394456952810287,-0.03523371368646622,0.02349630557000637,0.009056408889591694,-0.011746433563530445,0.04178960248827934,-0.051968131214380264,-0.05074380338191986,-0.013667210936546326,-2.1241328695964512E-8,0.05990470573306084,0.0013681596610695124,-0.07181969285011292,0.0656774714589119,0.02585667185485363,-0.0487004816532135,0.015919577330350876,0.10220672935247421,2.4406243755947798E-4,0.04301271587610245,-1.964946131920442E-4,-0.017018139362335205,0.05676128342747688,0.05079766362905502,-0.07554394751787186,-0.00440027704462409,0.07704989612102509,0.05232839286327362,-0.014917311258614063,-0.022130822762846947,0.055402614176273346,-0.03462079539895058,-0.06553258746862411,-0.015270575881004333,-0.020679105073213577,-0.003014026675373316,-0.023259587585926056,0.033668361604213715,0.02311735786497593,-0.00442897155880928,0.028694961220026016,-0.05298081785440445,-0.014965375885367393,-0.04324081540107727,0.020552681758999825,0.08796001225709915,0.042129650712013245,-0.015917889773845673,-0.0059137470088899136,-0.06907723844051361,0.07015100121498108,-0.01824011653661728,-0.03076593577861786,0.009009606204926968,0.012348068878054619,-0.05998144671320915,-0.026688391342759132,9.507364011369646E-4,0.028947507962584496,-0.02921261638402939,-0.0012587163364514709,-0.00862353015691042,0.03195128217339516,0.1021173745393753,0.08831404149532318,0.008626436814665794,0.022494029253721237,-0.019619597122073174,-0.015409592539072037,0.08438633382320404,0.016439473256468773,0.04096062853932381,-0.07884320616722107,-0.024730663746595383]                |\n",
      "|60B14E46-3251-11E8-A485-480FCF5217B3|28.489205|-16.315972|CALLE       |MENCEY BENCOMO                      |37   |38023 |San Cristóbal de La Laguna|CALLE MENCEY BENCOMO 37 SAN CRISTOBAL DE LA LAGUNA                     |[-0.022518007084727287,-0.022737596184015274,-0.028975144028663635,-0.002693278482183814,-0.13934233784675598,0.002414643531665206,0.07214397192001343,0.03031587041914463,-0.0077532632276415825,-0.038841988891363144,0.005590320099145174,-0.139387309551239,0.013384656049311161,-0.06105537712574005,-0.02980489656329155,-0.010600236244499683,0.008096394129097462,0.0029664018657058477,0.05854594707489014,0.021382415667176247,0.011367899365723133,-0.015525243245065212,-0.10452084988355637,0.056984882801771164,-0.05369904264807701,-0.056400198489427567,-0.014227613806724548,0.08510880172252655,-0.06025446951389313,-0.014239614829421043,0.041282329708337784,0.19989196956157684,0.12203255295753479,0.004141897428780794,0.06128117814660072,0.012417174875736237,-0.014791149646043777,-0.10896287113428116,-0.06116738170385361,0.08678882569074631,-0.0957886278629303,-0.018064290285110474,-0.08956311643123627,0.009595758281648159,-0.01437161024659872,-0.05668359622359276,0.02087428793311119,0.02122526988387108,0.07822492718696594,-0.011008495464920998,-0.037387773394584656,0.033888667821884155,-0.008994294330477715,0.08037585765123367,-0.019561199471354485,0.08435309678316116,0.06270916014909744,0.0025950525887310505,0.08685964345932007,0.04423785209655762,0.044887933880090714,-0.005149295087903738,-0.04927368462085724,0.009608341380953789,-0.04955478012561798,0.07769148051738739,-0.028773751109838486,-0.08592277020215988,-0.07212123274803162,-0.051928434520959854,0.06657186895608902,-0.005846645683050156,-0.04842623323202133,-0.025954008102416992,0.03590990975499153,0.040357887744903564,-0.07763545215129852,-0.04745516926050186,-0.03442944958806038,-0.055956821888685226,-0.025777464732527733,-0.06092756614089012,0.05783393234014511,-0.0029459616634994745,0.005285114981234074,0.04279886558651924,0.012213140726089478,-0.053007062524557114,0.03762790188193321,-0.0018105813069269061,-0.10681052505970001,0.028083985671401024,-0.15048906207084656,-0.053585484623909,-0.03158164396882057,-0.009021124802529812,0.017541930079460144,-0.032865025103092194,-0.017255259677767754,0.05734836310148239,0.00945234950631857,0.06558038294315338,-0.07851994782686234,0.019348055124282837,-0.010924527421593666,0.018116896972060204,0.1120641678571701,0.015354764647781849,-0.01648441143333912,-0.015602188184857368,-0.0511004813015461,0.02350994385778904,-0.012463019229471684,-0.05298689752817154,0.029701294377446175,-0.012750188820064068,0.10313688218593597,0.014638432301580906,-0.004887835122644901,-0.08408194780349731,0.03200225532054901,0.005286690779030323,-0.08037547767162323,-0.051838427782058716,-0.027884136885404587,-0.025832638144493103,0.03885174170136452,6.084550512372138E-33,0.011689601466059685,-0.005896544083952904,0.059166014194488525,0.08572901785373688,0.07206102460622787,0.007190323434770107,-0.05225851386785507,-0.014173805713653564,0.003972195088863373,-0.09149154275655746,0.0010223984718322754,-0.10307789593935013,-0.014214763417840004,-0.0978158637881279,0.0756070539355278,0.07368694245815277,0.00840063951909542,-0.08263201266527176,-0.05894632264971733,-0.003197131911292672,-0.0033051816280931234,0.026615511626005173,-0.014914607629179955,-0.025965265929698944,-0.006136290729045868,0.043009091168642044,0.04038270190358162,-3.800208332904731E-6,0.03218764439225197,0.02332155779004097,-0.04098975658416748,0.017702262848615646,0.07651520520448685,0.011844094842672348,0.05156659707427025,-0.04226727783679962,-0.010042699053883553,-0.004160277079790831,-0.07994844019412994,0.059991028159856796,0.04929714277386665,0.07619331032037735,0.0015183037612587214,-0.062193237245082855,-0.009163495153188705,0.011524009518325329,0.07482795417308807,0.053656239062547684,0.14254993200302124,0.013494271785020828,-0.03003339096903801,0.0313592404127121,-0.08431944996118546,0.01014934852719307,0.02515597641468048,0.004767070524394512,-0.0712655633687973,-0.02802054025232792,-0.013222520239651203,0.01991666853427887,0.1467801183462143,0.037533193826675415,0.05636340379714966,0.024711621925234795,0.00957651436328888,7.957202033139765E-4,0.007488673087209463,0.013887795619666576,0.08702114224433899,-0.056196097284555435,-0.04941834136843681,-0.008708207868039608,0.11672556400299072,0.03191691264510155,-0.004380702972412109,0.0013284366577863693,-0.0086899409070611,-0.02011287584900856,-0.001954212784767151,-0.010066620074212551,0.03362899646162987,-0.027977708727121353,0.02155407890677452,0.04963403567671776,0.08385083824396133,0.00829462893307209,0.05389570817351341,-0.0591583326458931,-0.04224831610918045,0.054962508380413055,-0.05276832357048988,0.009451009333133698,0.029363363981246948,-0.030754080042243004,-0.00983209628611803,-6.0753133308265826E-33,0.04240860044956207,-0.04256953299045563,0.049051184207201004,0.0056589520536363125,0.011665646918118,-0.0027233269065618515,0.003953605890274048,0.04338228702545166,0.022275900468230247,-0.0081427451223135,-0.04619792848825455,-0.05146874487400055,0.11777910590171814,-0.09693466871976852,0.038441237062215805,0.13347475230693817,0.08054786175489426,-0.039834652096033096,-0.0893140360713005,-0.03110339865088463,-0.017727553844451904,0.03399912267923355,-0.03050665184855461,-0.009212649427354336,-0.005504715722054243,-0.033004507422447205,0.051910802721977234,0.053294770419597626,-4.2870914330706E-4,0.014402605593204498,-0.03502538800239563,-0.07203716784715652,-0.07457285374403,0.07058321684598923,-0.04488328844308853,0.04830893501639366,0.04464667662978172,0.010135422460734844,0.09089481085538864,0.005582756828516722,-0.05856868997216225,-0.05776214227080345,-0.0023076971992850304,-0.019761180505156517,0.0018963773036375642,-0.007287910673767328,-0.027282094582915306,-0.0023078271187841892,0.010258084163069725,-0.08266562968492508,0.03270338475704193,-0.03825939819216728,-0.1259315311908722,0.04683338850736618,0.0457167811691761,-0.05398555099964142,0.04198645427823067,-0.050874169915914536,-0.018315015360713005,-0.018642783164978027,0.042803697288036346,0.06755045801401138,-0.024529092013835907,0.009225581772625446,-0.022705860435962677,0.039778828620910645,-0.04263560101389885,-0.025980720296502113,-0.006267355754971504,0.028043093159794807,0.03528057783842087,-0.021288523450493813,-0.08752771466970444,0.05257639288902283,-0.1016458198428154,0.015423009172081947,-0.06002608314156532,0.07429371029138565,-0.0037660272791981697,0.06396505981683731,-4.8283260548487306E-4,-0.06936383992433548,-0.0537145659327507,0.055833715945482254,0.028253518044948578,0.01740245334804058,-0.0071547930128872395,-0.006763732060790062,0.04109644144773483,-0.06393928825855255,0.019378283992409706,0.014737285673618317,-0.011709330603480339,-0.04483693465590477,-0.046795401722192764,-2.45794691267065E-8,0.07339579612016678,-0.020044729113578796,-0.036771759390830994,-0.02503456175327301,0.059598639607429504,-0.05578023940324783,0.0026174213271588087,0.022654535248875618,0.05380009487271309,0.03567120432853699,-0.02748943865299225,-0.03973829001188278,0.017553621903061867,0.035120632499456406,-0.04590555652976036,0.06624989211559296,0.07733090966939926,0.0012636712053790689,0.011110343039035797,-0.015690069645643234,0.018227461725473404,-0.10681354254484177,-0.022345222532749176,-0.029087776318192482,-0.013886521570384502,-0.022915232926607132,-0.0037840595468878746,0.030085334554314613,-0.009000442922115326,-0.08104196190834045,0.013546077534556389,-0.001043883734382689,-0.05289226025342941,-0.06991836428642273,0.032905854284763336,0.03770142421126366,-0.06509564071893692,0.024849912151694298,-0.02105456218123436,-0.01588733308017254,0.07674891501665115,0.010331390425562859,0.020741714164614677,0.05904824286699295,0.030360346660017967,0.0029089900199323893,-0.014970364980399609,-0.043302375823259354,0.03551279008388519,0.0020738900639116764,0.013633818365633488,-0.01727740280330181,0.12408977746963501,0.024913884699344635,0.019870437681674957,0.04183654859662056,0.045682355761528015,0.027745751664042473,0.03252552077174187,0.06669694930315018,-0.014712412841618061,0.023380497470498085,-0.09373940527439117,-0.038732755929231644]|\n",
      "|C480010C-3EDA-11EB-845E-5FC45ADE36E3|28.47962 |-16.30707 |CALLE       |VERDELLADA FASE I                   |13   |38023 |San Cristóbal de La Laguna|CALLE VERDELLADA FASE I 13 SAN CRISTOBAL DE LA LAGUNA                  |[-0.048655442893505096,0.0362715981900692,-0.03846408426761627,-0.023870687931776047,-0.12403041124343872,0.007871498353779316,0.05242438614368439,0.05924875661730766,0.06415310502052307,-0.06541293114423752,0.02628912962973118,-0.137825146317482,-0.026071615517139435,-0.0631900504231453,-0.018274124711751938,-0.03354868292808533,-0.06947100907564163,-0.007766636088490486,0.012093956582248211,0.039146870374679565,0.0215500108897686,-0.014762178994715214,-0.07067375630140305,0.058707065880298615,-0.05161623656749725,-0.01465430948883295,0.0019236039370298386,0.024792546406388283,0.00865374319255352,-0.023944567888975143,0.0931626409292221,0.17239868640899658,0.09849009662866592,-4.710573994088918E-4,0.03633064031600952,0.019158529117703438,0.0316014289855957,-0.1398553103208542,0.009039445780217648,0.08308509737253189,-0.1090516522526741,0.033584754914045334,-0.08583106100559235,-0.02975580096244812,0.0018890311475843191,-0.06607761979103088,0.02395514026284218,0.023707149550318718,0.036317963153123856,-0.01912522129714489,-0.04000731185078621,-0.010671788826584816,-0.021444611251354218,0.07855045795440674,-0.006338836159557104,0.03407946228981018,0.05477069690823555,0.014324427582323551,0.11361391097307205,0.05267922207713127,0.03956858813762665,0.004855589009821415,-0.07525182515382767,0.015070305205881596,-0.10929634422063828,0.008211280219256878,-0.019903089851140976,-0.08246416598558426,-0.0772765502333641,-0.030903389677405357,0.0030572526156902313,0.006257573142647743,-0.01589551754295826,0.021679671481251717,-0.035763930529356,0.0725715309381485,-0.07117407023906708,-0.06665926426649094,-0.03659677878022194,-0.00392827158793807,-0.011384647339582443,-0.035403862595558167,0.029210595414042473,-0.06529965251684189,0.018056917935609818,-0.001740963663905859,0.017704004421830177,-0.005329646170139313,0.04596221446990967,0.03669946268200874,-0.038292694836854935,0.032878577709198,-0.06862041354179382,-0.03968975692987442,-0.035480134189128876,0.007516978308558464,0.016739346086978912,-0.08767116069793701,-0.003435532795265317,0.060242731124162674,-0.00761678209528327,0.05649629235267639,-0.04501887783408165,0.03313793987035751,-0.06814442574977875,0.01758233644068241,0.0803050547838211,9.301982936449349E-4,-0.06362281739711761,0.018004050478339195,-0.03248037397861481,-0.045002300292253494,-0.02754083462059498,-0.07873962819576263,-0.011496240273118019,0.031524814665317535,0.07847139984369278,-0.013454657979309559,0.006378798745572567,-0.06641865521669388,0.010191545821726322,-0.00836016796529293,-0.04819571599364281,-0.009886158630251884,-0.009863656014204025,-0.07848300784826279,-0.003481008345261216,4.912535992170269E-33,0.030009480193257332,-0.01348549872636795,4.663505533244461E-4,0.024021679535508156,0.06533993780612946,-0.007725679315626621,-0.03941308334469795,-0.03969188034534454,-0.019072310999035835,-0.05053658410906792,0.005413946229964495,-0.105052649974823,-0.037783585488796234,-0.06648973375558853,0.07664976269006729,0.11757820844650269,0.01286124438047409,-0.10272222012281418,-0.08350895345211029,-0.012095740996301174,0.004698879085481167,-2.2667109442409128E-4,0.058439601212739944,-0.0440852977335453,-0.014442495070397854,0.041052188724279404,-0.011933776549994946,-0.004151105415076017,-0.015173545107245445,0.043871231377124786,0.014050492085516453,-0.05372660607099533,0.00628693075850606,0.018192457035183907,0.04764283820986748,-0.019848451018333435,0.05145550146698952,-0.05419306829571724,-0.0459582656621933,-0.002277602441608906,0.08956381678581238,0.03292025998234749,0.03485601395368576,0.03603968769311905,0.08493375033140182,0.04753673076629639,0.06578315049409866,0.03306514024734497,0.10534515231847763,-0.0043266527354717255,-0.09959171712398529,-0.007281694561243057,-0.08429650962352753,-0.004999870900064707,-6.99392578098923E-4,0.026433654129505157,-0.06430874019861221,0.007145388051867485,-0.008775236085057259,0.041879232972860336,0.08891032636165619,-0.038972605019807816,0.04799749702215195,0.015047559514641762,-0.03163532912731171,-0.015633264556527138,0.01606041006743908,0.0601285845041275,0.12032253295183182,-0.06810811907052994,-0.03735710680484772,-0.015002123080193996,0.08053906261920929,0.03478458896279335,-0.0036413511261343956,-0.024529239162802696,0.018033994361758232,-0.0464046485722065,-0.02873091585934162,0.045330990105867386,-0.008752909488976002,0.01876453310251236,0.022617243230342865,0.018323669210076332,0.0786811038851738,0.019231708720326424,0.03424212336540222,-0.010338201187551022,-0.029251033440232277,0.05989563465118408,-0.04334468021988869,0.019088568165898323,0.05539208650588989,-0.064637690782547,-0.0013877054443582892,-5.159333602519301E-33,0.055315516889095306,-0.052423231303691864,0.032245974987745285,-0.04016915708780289,-0.007951632142066956,-0.0026271382812410593,0.00891018845140934,0.04589429125189781,0.027093641459941864,-0.03760654851794243,-0.08395326882600784,-0.06978000700473785,0.07523801922798157,-0.08869078010320663,0.08852936327457428,0.1010049507021904,0.08512382954359055,-0.04538843408226967,-0.05779255926609039,0.009088924154639244,-0.036493200808763504,0.03967819735407829,-0.0618688203394413,0.026311278343200684,0.025405796244740486,-0.03616214171051979,0.05983344465494156,0.03524167463183403,-0.008633341640233994,0.012367489747703075,0.014153221622109413,-0.08918378502130508,-0.006591312121599913,0.07347842305898666,-0.04296647012233734,0.03347999230027199,0.0626782551407814,-0.034600768238306046,0.0053909290581941605,0.021052682772278786,0.023252416402101517,-0.05013734847307205,-0.03183054178953171,-0.03226572275161743,-0.04601103812456131,0.009270207956433296,-0.006960452068597078,0.0177578404545784,-2.0447521819733083E-4,-0.06432271748781204,0.06002187356352806,-0.04690287634730339,-0.06746876984834671,0.050515588372945786,0.08295426517724991,-0.08700652420520782,0.07032310217618942,-0.049285639077425,-0.031051894649863243,-0.01532803289592266,0.12199526280164719,0.03904840722680092,-0.03726838901638985,0.007815495133399963,0.03027716465294361,0.0017184583703055978,-0.02824321575462818,-0.045979682356119156,-0.004861651454120874,0.07209255546331406,0.023482121527194977,-0.017989574000239372,-0.1328950673341751,0.04875717684626579,-0.10301095247268677,0.002061668550595641,-0.08873451501131058,0.06998270750045776,0.008893044665455818,0.06676696240901947,0.01455321628600359,-0.05180295184254646,-0.07128341495990753,0.014927275478839874,0.07229720056056976,0.02134603261947632,-0.010656763799488544,-0.016964608803391457,0.028012070804834366,-0.061184946447610855,0.03667832165956497,0.04006623849272728,0.02357696369290352,-0.04520203545689583,-0.03831757977604866,-2.2498795715364395E-8,0.08653264492750168,-0.058330636471509933,0.013833378441631794,0.03745521605014801,0.052468378096818924,-0.0892433226108551,0.003217574441805482,0.012609175406396389,-0.032971739768981934,0.05230776220560074,-0.024170909076929092,-0.049582403153181076,0.0835888460278511,1.7856300109997392E-4,-0.029530012980103493,0.0629878044128418,0.13048361241817474,0.006401887163519859,0.006083550397306681,-0.05276796966791153,0.05759851261973381,-0.05216264724731445,-0.020243825390934944,-0.03596742823719978,-0.012700778432190418,-0.0046968539245426655,-0.020006980746984482,0.0533057264983654,-0.010150805115699768,-0.02425176091492176,0.027661602944135666,-0.029570844024419785,-0.01798110641539097,-0.08552315831184387,-0.05667911842465401,0.06376224756240845,-0.014881161972880363,-0.02967114932835102,-0.029072068631649017,-0.04080190882086754,0.09865757822990417,0.04501742124557495,0.02153467759490013,0.018751677125692368,0.0181855708360672,0.06403780728578568,-0.003938621375709772,-0.0664505809545517,0.05162759870290756,0.02246955968439579,-6.534024723805487E-4,-0.023452747613191605,0.10923238843679428,0.06470043957233429,0.048287827521562576,0.003545786254107952,0.02015303261578083,0.004340490326285362,-0.01397537998855114,0.11159830540418625,0.06385219842195511,0.048911530524492264,-0.05383796989917755,-0.07894427329301834]                                                |\n",
      "|6AFEC962-3251-11E8-ACD4-480FCF5217B3|28.457675|-16.316643|CAMINO      |PEDREGALELLOS VALLES                |55   |38023 |San Cristóbal de La Laguna|CAMINO PEDREGALELLOS VALLES 55 SAN CRISTOBAL DE LA LAGUNA              |[0.03404127433896065,0.04432011768221855,0.005634095054119825,0.0328240729868412,-0.041628398001194,-0.008096878416836262,0.039388518780469894,0.040688712149858475,0.031579528003931046,-0.03557249903678894,0.04292852059006691,-0.13719786703586578,-0.0010893939761444926,-0.03299983963370323,0.03985198959708214,0.00844061654061079,-0.060065627098083496,0.011474810540676117,0.03260229155421257,0.05633705109357834,0.028509940952062607,-0.058529771864414215,-0.0646970346570015,0.1188189685344696,-0.07245079427957535,0.02735230326652527,-0.013021619990468025,0.11970576643943787,-0.04853127896785736,-0.03340936452150345,-0.04799782112240791,0.11255212128162384,0.11296788603067398,-0.017367135733366013,0.07503379136323929,-0.0012780651450157166,-0.04275873675942421,-0.05259174108505249,-0.02039409801363945,0.07340258359909058,-0.08779922127723694,0.026162004098296165,0.00648688105866313,0.03951152786612511,0.02945500984787941,-0.0021416177041828632,0.07943931221961975,0.031043346971273422,0.05283648520708084,-0.09213633090257645,-0.11899559944868088,-0.0010130925802513957,0.014526251703500748,0.04899117723107338,-0.11205323040485382,0.007605904247611761,-0.045498695224523544,-0.07253214716911316,0.1062338575720787,-0.06943961977958679,0.07860902696847916,0.020972615107893944,-0.08410344272851944,0.02382712997496128,-0.06806806474924088,0.04178320989012718,-0.009896142408251762,-0.07410647720098495,0.004634496755897999,-0.05060490965843201,0.004338262137025595,0.019835343584418297,-0.02031608484685421,0.002893197350203991,0.014417928643524647,0.01776726357638836,-0.0290033258497715,-0.07454019039869308,-0.09548191726207733,-0.08888693153858185,1.3596673670690507E-4,-0.04333138465881348,-0.014475067146122456,0.011685221455991268,0.028590446338057518,0.04990135878324509,0.02600942552089691,0.03657064586877823,0.06913561373949051,-0.032611504197120667,-0.053587328642606735,0.014414533972740173,-0.12946002185344696,-0.04276254400610924,0.009111553430557251,0.05892639607191086,-0.024645235389471054,-0.08566662669181824,0.07078628242015839,0.029669342562556267,0.05490819737315178,-0.004581208806484938,-0.029027657583355904,0.0033348598517477512,-2.1548585209529847E-4,0.07859781384468079,0.08764680474996567,0.04902325198054314,-0.023198401555418968,0.0024462388828396797,-0.06380514800548553,-0.02827996388077736,0.019254367798566818,-0.03397117182612419,-0.020293712615966797,0.03752526268362999,-1.5299908409360796E-4,0.030527448281645775,-0.032242510467767715,0.012232323177158833,0.034679096192121506,-0.09248263388872147,-0.07445666939020157,-0.06235445663332939,0.00513828918337822,-0.02267371490597725,0.01180439256131649,3.951195038890709E-33,-0.022389788180589676,-0.03315265476703644,0.04974775388836861,0.07127989083528519,0.11339327692985535,-0.04242447763681412,-0.011465886607766151,-0.0950346291065216,-0.0690883994102478,-0.05915028601884842,-0.038647349923849106,-0.09599202871322632,-0.0393022745847702,-0.054724108427762985,0.06728774309158325,0.07330968976020813,0.005481400061398745,-0.09106145054101944,-0.05522158741950989,-0.0057167112827301025,-0.03977227583527565,0.0026349227409809828,-0.023554451763629913,0.004746375139802694,-0.021299991756677628,0.0810762420296669,0.020835459232330322,-0.026190420612692833,-0.021011553704738617,0.05280202999711037,0.004173108376562595,-0.016566377133131027,-0.004858906846493483,0.019417567178606987,0.02313542738556862,-0.005497653037309647,-0.024524815380573273,0.008264883421361446,-0.015191514045000076,0.057089872658252716,0.039405666291713715,0.031965222209692,-0.02023344114422798,0.03366962820291519,0.04131946712732315,0.03846576437354088,0.054526444524526596,0.05397626757621765,0.057117100805044174,0.04286016896367073,-0.013972677290439606,0.011342485435307026,-0.13666173815727234,-0.026572318747639656,-0.011223568581044674,-0.0014049039455130696,-0.09690216183662415,-0.024829372763633728,-0.08725473284721375,0.06926590949296951,0.07966916263103485,0.06651273369789124,0.01677325740456581,-0.035018522292375565,0.004774321801960468,-0.026938429102301598,0.042738184332847595,0.06874463707208633,0.08589654415845871,-0.010984613560140133,-0.0631544291973114,-0.02809598110616207,0.06112652271986008,-0.0055696191266179085,-0.018278516829013824,-0.005179358646273613,-0.11549855768680573,-0.07254867255687714,0.0128274941816926,0.10409624129533768,-0.1125522181391716,0.0012543871998786926,-0.018194114789366722,0.0616765059530735,0.004020349122583866,0.019278476014733315,0.03648838400840759,0.02901935949921608,-0.004553273785859346,0.038411904126405716,-0.041197214275598526,0.02614913508296013,0.072526715695858,-0.012367508374154568,0.0045516714453697205,-4.221946285373622E-33,0.012925324961543083,0.01117792073637247,0.005047962069511414,0.00681103253737092,-0.06911294907331467,0.003401194466277957,-0.022927874699234962,-0.0021687797270715237,0.05181031674146652,-0.03252608701586723,-0.11386708170175552,-0.04492698982357979,0.046122536063194275,-0.07309694588184357,0.018524175509810448,0.08077481389045715,0.07757768034934998,-0.027418557554483414,-0.09337454289197922,0.004217903595417738,0.009838784113526344,0.03683832287788391,0.007096600253134966,0.002662965329363942,0.05970091000199318,0.05521510913968086,0.030473953112959862,0.08397853374481201,-0.015332890674471855,-0.015722457319498062,-0.002968017477542162,-0.07307583093643188,0.018388625234365463,0.01899903267621994,-0.06667499244213104,0.04060027748346329,0.04495217651128769,-0.027430452406406403,0.05443790555000305,0.049018606543540955,0.005722240544855595,-0.054451536387205124,0.016870900988578796,-0.02155906893312931,-0.03457871824502945,0.032197512686252594,0.03700265288352966,-9.325483115389943E-4,0.00796789675951004,-0.10220707952976227,0.07266773283481598,-0.021939022466540337,-0.1088113933801651,0.04576859623193741,0.08530243486166,0.0016676010563969612,-0.06725897639989853,0.07962603867053986,-0.018332775682210922,-0.018881577998399734,0.05785715952515602,0.04661962762475014,-0.07384751737117767,0.0342058390378952,0.03198917955160141,0.008266081102192402,0.036324817687273026,-0.042176809161901474,-0.0362105555832386,0.04434406757354736,-0.0029104414861649275,0.004768166225403547,-0.07395141571760178,0.0394473671913147,-0.08305560797452927,0.03402535244822502,-0.018131310120224953,0.016069289296865463,0.036065541207790375,0.005172514822334051,-0.025075538083910942,-0.08442920446395874,-0.06228823587298393,0.04013814032077789,0.004227976314723492,0.013427087105810642,-0.028874346986413002,-0.03249458596110344,0.05575910210609436,0.008690351620316505,0.030237434431910515,0.006972676608711481,0.041477981954813004,-0.0018035980174317956,-0.02862890437245369,-2.1454507503904097E-8,0.08250243216753006,-4.5859545934945345E-4,-0.09161309897899628,0.050555404275655746,0.012406193651258945,-0.03902005776762962,0.03028770722448826,0.09379325807094574,5.389050929807127E-4,0.045104652643203735,0.051610395312309265,-0.016758982092142105,0.11338995397090912,0.02398524060845375,-0.00654648058116436,0.02313627488911152,0.05726158618927002,0.0245367344468832,-0.004429767839610577,-0.05906463414430618,-0.007329676300287247,-0.07993753999471664,-0.07289493083953857,-0.01871620863676071,-0.04009127616882324,-0.05823074281215668,0.017341332510113716,-0.02919822558760643,0.01301889680325985,-0.04138431325554848,-0.01455446146428585,-0.04928011819720268,0.02198760397732258,-0.04400618001818657,0.06993228197097778,0.03683197870850563,-0.11241410672664642,-0.014447170309722424,-0.020661769434809685,-0.03506516292691231,0.1019401028752327,-0.0164489783346653,-0.011862123385071754,-0.010393849574029446,0.02039911411702633,0.01765795797109604,0.008648112416267395,-0.02531040459871292,-0.007755296304821968,-0.021099112927913666,-0.016705812886357307,0.004519879352301359,0.09932965785264969,0.10353275388479233,-0.014223475009202957,0.04665452241897583,0.08913379162549973,-0.02691211737692356,-0.02325744926929474,0.03979048505425453,-0.057353898882865906,0.05822887644171715,-0.0975605845451355,-0.07077383995056152]                                          |\n",
      "|68D1FF76-3251-11E8-9C5F-480FCF5217B3|28.471379|-16.320648|CALLE       |FERIA 1 TRANSVERAL BALDIOS          |20   |38023 |San Cristóbal de La Laguna|CALLE FERIA 1 TRANSVERAL BALDIOS 20 SAN CRISTOBAL DE LA LAGUNA         |[0.00894140638411045,0.03727513551712036,-0.024635398760437965,0.029845019802451134,-0.13851597905158997,0.011013899929821491,0.031165972352027893,0.0666772648692131,0.02527626045048237,-0.10442699491977692,0.037222277373075485,-0.11377142369747162,-0.021761968731880188,-0.05137770250439644,-0.03841201588511467,-0.05372599884867668,-0.07066371291875839,0.002440736163407564,0.04321448877453804,0.03355042263865471,0.07880641520023346,-0.01737372577190399,-0.07812514901161194,0.05354556441307068,-0.07005005329847336,-0.025979220867156982,-0.006457071285694838,0.041533198207616806,-0.06508158892393112,-0.09459047764539719,0.02882971242070198,0.16376052796840668,0.10351142287254333,-0.037302929908037186,-0.008306935429573059,0.01668878272175789,-0.020248861983418465,-0.0978495329618454,0.027212748304009438,0.057997312396764755,-0.07628852128982544,-0.02975326217710972,-0.06520280987024307,-0.022212214767932892,-0.04915944114327431,-0.05012061446905136,0.048595037311315536,0.06285104155540466,0.05318520963191986,-0.004630875773727894,-0.06442567706108093,-0.010016919113695621,-0.01890707202255726,0.08857844769954681,-0.026138851419091225,0.05608582869172096,0.10513744503259659,-0.001706196228042245,0.03777666389942169,0.052325937896966934,0.08052157610654831,0.005061621777713299,-0.09483348578214645,-0.006547718774527311,-0.06326192617416382,0.040436357259750366,-0.044414032250642776,-0.08409474045038223,-0.10118971019983292,-0.0653078481554985,0.0391664020717144,0.009230419993400574,0.036951035261154175,4.327945352997631E-4,-0.0559016615152359,0.050191085785627365,-0.015071481466293335,-0.04517177864909172,-0.007272706367075443,-0.007624245714396238,-0.03480503708124161,-0.04554872587323189,0.03676510602235794,-0.04618721082806587,0.005336595233529806,0.08208347856998444,-1.8913480744231492E-4,0.01203641202300787,0.01702033542096615,0.006816780660301447,-0.06546789407730103,0.06354422122240067,-0.06269614398479462,-0.04925989732146263,0.007224823348224163,0.031189363449811935,-0.006986883468925953,-0.0672636553645134,0.04348686337471008,0.018373969942331314,0.019669445231556892,0.048802874982357025,-0.0406283475458622,0.007881850004196167,-0.09496533125638962,0.013173502869904041,0.03625348210334778,0.013912230730056763,-0.06973715126514435,-0.0030976533889770508,-0.02152089774608612,-0.04136674851179123,0.017709864303469658,-0.0895816907286644,-0.07939215004444122,0.015551741234958172,0.07158686965703964,-0.06234041973948479,-0.006766446866095066,-0.05981951579451561,0.046297430992126465,-0.025129936635494232,-0.027661139145493507,-0.05698411539196968,-0.051619239151477814,-0.01353215891867876,0.048254843801259995,6.440784810072802E-33,-0.020018571987748146,-0.005280999932438135,-0.00927744247019291,0.06606937199831009,0.05774462968111038,-0.03363681212067604,-0.0359911173582077,-0.062455154955387115,-0.03738519921898842,-0.059708841145038605,-0.06358781456947327,-0.06782267242670059,-0.050467874854803085,-0.0890020951628685,0.06905674189329147,0.03848729655146599,0.04070848971605301,-0.05319436639547348,-0.01059684157371521,-0.03866918757557869,-0.0036372789181768894,0.07083048671483994,0.02353922836482525,-0.022094184532761574,0.00708845816552639,0.07946085929870605,-0.027779733762145042,-0.06408128887414932,-0.024777909740805626,0.034802887588739395,0.01875181682407856,-0.006935532670468092,0.002787135075777769,-0.03582353517413139,0.04196590557694435,-0.027450736612081528,0.024212870746850967,-0.006367475725710392,-0.09590966999530792,0.020718375220894814,0.06380521506071091,0.0663502961397171,0.00488610053434968,0.019422760233283043,0.05253465101122856,0.0013035278534516692,0.0038871837314218283,0.06391354650259018,0.08915140479803085,0.05078643187880516,-0.07835757732391357,0.016896340996026993,-0.07881597429513931,-0.005155656952410936,0.04114031791687012,0.021953484043478966,-0.05590314790606499,0.013782655820250511,-0.03579556569457054,0.05602147430181503,0.10263655334711075,0.061526820063591,0.016631316393613815,0.018992234021425247,-0.001959397690370679,0.019058555364608765,0.008336527273058891,0.05519428104162216,0.11072508245706558,0.00443204864859581,-0.06907010078430176,-0.013777017593383789,0.11262625455856323,0.06729811429977417,0.037919141352176666,0.0015578942839056253,-0.03089747205376625,-0.07898703217506409,-0.020121922716498375,0.026190584525465965,-1.863629586296156E-4,-0.015938468277454376,0.0202837772667408,0.010889554396271706,0.06685243546962738,0.10083025693893433,0.03604252636432648,-0.013467079028487206,-0.041762035340070724,0.061603251844644547,-0.01572304591536522,0.04514472559094429,-0.012072207406163216,-0.013135189190506935,-2.1500041475519538E-4,-7.351151552854907E-33,0.03482568636536598,-0.11139101535081863,0.04022042825818062,-0.00782401766628027,-0.030413970351219177,0.05206477269530296,-0.05355261638760567,-0.018095683306455612,-0.007956678047776222,0.032350052148103714,-0.029497232288122177,-0.00594364246353507,0.08750558644533157,-0.08784805983304977,-0.018311265856027603,0.08824945241212845,0.09746160358190536,-0.032658059149980545,-0.07690198719501495,0.0036783707328140736,-0.03934895619750023,0.09112671762704849,-0.07957690954208374,0.021774152293801308,0.017503203824162483,-0.008998257108032703,0.040702831000089645,0.051994964480400085,0.021225126460194588,0.034465573728084564,-2.717502065934241E-4,-0.0740746557712555,0.025408228859305382,0.0631948709487915,-0.04946282505989075,0.029443221166729927,0.08885776251554489,-0.0033692598808556795,0.11176738888025284,0.03486834838986397,-0.058299869298934937,-0.04942592233419418,0.06822443008422852,0.013348079286515713,-0.03731781616806984,-0.05716346949338913,-0.09259498864412308,-0.015761228278279305,0.01525049190968275,-0.06423263251781464,0.09674728661775589,-0.048082295805215836,-0.10714326798915863,0.016719631850719452,0.03526996076107025,-0.08154738694429398,0.0020513362251222134,-0.0460127592086792,-0.030269522219896317,-6.71751971822232E-4,0.06289057433605194,-0.0042050741612911224,-0.020534474402666092,0.025222761556506157,0.05758805200457573,-0.00837599765509367,-0.05881742015480995,-0.011745307594537735,0.02377217449247837,0.0426887683570385,0.0176165122538805,-0.010486360639333725,-0.061268020421266556,0.052868109196424484,-0.03829324617981911,0.024711668491363525,-0.06686466932296753,0.06466931104660034,0.026000866666436195,0.09542953222990036,0.005781792104244232,-0.05917300283908844,-0.07547897100448608,0.025383973494172096,0.07517125457525253,-0.029840486124157906,-0.014614739455282688,0.014483913779258728,0.016698572784662247,-0.04440367966890335,-0.020033197477459908,0.07501773536205292,-0.043065328150987625,-0.08697741478681564,-0.02885533683001995,-2.6795532903634012E-8,0.09539571404457092,-0.03824251890182495,0.001763074309565127,0.030931811779737473,0.03632419556379318,-0.06057117134332657,-0.008798330090939999,-0.020311418920755386,0.0063458881340920925,0.02664967067539692,-0.0779491737484932,0.024721821770071983,0.06936415284872055,0.06071854382753372,-0.053430378437042236,0.020899763330817223,0.0888756662607193,0.1003144383430481,0.014819948002696037,-0.03463996201753616,-0.01289878599345684,-0.054438989609479904,-0.06109888479113579,-0.07667781412601471,-0.038945917040109634,-0.06467430293560028,0.007057953625917435,0.015501211397349834,0.04667961597442627,-0.05346584692597389,0.020625483244657516,0.013011377304792404,-0.01503800880163908,-0.07488065958023071,-0.021325303241610527,0.06917709857225418,-0.047903724014759064,0.04636326804757118,-0.055834997445344925,-0.023987237364053726,0.09605402499437332,-0.010342581197619438,-0.01709769479930401,0.004300452768802643,-0.011502828449010849,-0.012576284818351269,-0.014175794087350368,-0.02554655261337757,-0.011708118952810764,-0.020984450355172157,0.0066804285161197186,0.015312258154153824,0.12122572958469391,0.075030118227005,0.004280213266611099,0.04399784654378891,0.06559594720602036,-0.0028146831318736076,0.02058657445013523,0.08517472445964813,-0.007640887517482042,0.012891851365566254,-0.0283729936927557,-0.013402832671999931]                       |\n",
      "+------------------------------------+---------+----------+------------+------------------------------------+-----+------+--------------------------+-----------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "minilm_test_dense_vector.limit(10).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = minilm_test_dense_vector.filter(\"direccion == 'PLAZA VICTOR ZURITA SOLER 102 SAN CRISTOBAL DE LA LAGUNA'\")\n",
    "x = x.union(minilm_test_dense_vector.filter(\"direccion == 'CALLE RECTOR JOSE CARLOS ALBERTO BETHANCOU 7 SAN CRISTOBAL DE LA LAGUNA'\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluated_minilm_cosine = calculate_cosine_similarity(\n",
    "    minilm_test_dense_vector, minilm_train_dense_vector.limit(1000)\n",
    ")\n",
    "# evaluated_minilm_cosine.select(\"uuid_idt\", \"direccion\", \"uuid_idt2\", \"direccion2\").show(10, truncate=False)\n",
    "# print(evaluated_minilm_cosine.count())\n",
    "evaluated_minilm_cosine = filter_3_best(evaluated_minilm_cosine)\n",
    "# print(evaluated_minilm_cosine.count())\n",
    "evaluated_minilm_cosine = evaluated_minilm_cosine.withColumn(\n",
    "    \"evaluation\", evaluated_minilm_cosine.uuid_idt == evaluated_minilm_cosine.uuid_idt2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3608:====================================================> (31 + 1) / 32]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------+---------+-----+--------------------+-----+------+--------------------+--------------------+--------------------+-----------------+\n",
      "|           uuid_idt2|          direccion2|          embedding2|            uuid_idt| latitud| longitud| tvia|                nvia|numer|codmun|              nommun|           direccion|           embedding|cosine_similarity|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------+---------+-----+--------------------+-----+------+--------------------+--------------------+--------------------+-----------------+\n",
      "|C075AB21-0AD1-11E...|PLAZA VICTOR ZURI...|[-0.0559375584125...|C075AB21-0AD1-11E...|28.48311|-16.31491|PLAZA| VICTOR ZURITA SOLER|  102| 38023|San Cristóbal de ...|PLAZA VICTOR ZURI...|[-0.0559375584125...|              1.0|\n",
      "|C075AB21-0AD1-11E...|PLAZA VICTOR ZURI...|[-0.0551847442984...|C075AB21-0AD1-11E...|28.48311|-16.31491|PLAZA| VICTOR ZURITA SOLER|  102| 38023|San Cristóbal de ...|PLAZA VICTOR ZURI...|[-0.0559375584125...|        0.9841254|\n",
      "|C075AB21-0AD1-11E...|PLAZA VICTOR ZURI...|[-0.0451515354216...|C075AB21-0AD1-11E...|28.48311|-16.31491|PLAZA| VICTOR ZURITA SOLER|  102| 38023|San Cristóbal de ...|PLAZA VICTOR ZURI...|[-0.0559375584125...|       0.97861826|\n",
      "|C481AD98-3EDA-11E...|CALLE RECTOR JOSE...|[-0.0221675485372...|C481AD98-3EDA-11E...|28.46787|-16.29529|CALLE|RECTOR JOSE CARLO...|    7| 38023|San Cristóbal de ...|CALLE RECTOR JOSE...|[-0.0221675485372...|              1.0|\n",
      "|6984FA11-3251-11E...|CALLE RECTOR JOSE...|[-0.0104976007714...|C481AD98-3EDA-11E...|28.46787|-16.29529|CALLE|RECTOR JOSE CARLO...|    7| 38023|San Cristóbal de ...|CALLE RECTOR JOSE...|[-0.0221675485372...|        0.9872102|\n",
      "|601B85A2-3251-11E...|CALLE RECTOR JOSE...|[-0.0181576758623...|C481AD98-3EDA-11E...|28.46787|-16.29529|CALLE|RECTOR JOSE CARLO...|    7| 38023|San Cristóbal de ...|CALLE RECTOR JOSE...|[-0.0221675485372...|        0.9847812|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------+---------+-----+--------------------+-----+------+--------------------+--------------------+--------------------+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "evaluated_minilm_cosine.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/07 17:06:38 ERROR DiskBlockObjectWriter: Exception occurred while manually close the output stream to file /tmp/blockmgr-456644b8-bf20-45a0-900a-fa5d2d088506/16/temp_shuffle_7c2cf1c0-0dcd-4a9b-a676-01afec15ea2e, No space left on device\n",
      "24/05/07 17:06:40 ERROR Executor: Exception in task 0.0 in stage 1390.0 (TID 4376)\n",
      "java.io.IOException: No space left on device\n",
      "\tat java.base/java.io.FileOutputStream.writeBytes(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.write(FileOutputStream.java:354)\n",
      "\tat org.apache.spark.storage.TimeTrackingOutputStream.write(TimeTrackingOutputStream.java:59)\n",
      "\tat org.apache.spark.io.MutableCheckedOutputStream.write(MutableCheckedOutputStream.scala:43)\n",
      "\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)\n",
      "\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:127)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:225)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:178)\n",
      "\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)\n",
      "\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:127)\n",
      "\tat java.base/java.io.DataOutputStream.write(DataOutputStream.java:107)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.writeToStream(UnsafeRow.java:519)\n",
      "\tat org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$1.writeValue(UnsafeRowSerializer.scala:69)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:312)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:171)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "24/05/07 17:06:41 WARN TaskSetManager: Lost task 0.0 in stage 1390.0 (TID 4376) (10.6.130.30 executor driver): java.io.IOException: No space left on device\n",
      "\tat java.base/java.io.FileOutputStream.writeBytes(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.write(FileOutputStream.java:354)\n",
      "\tat org.apache.spark.storage.TimeTrackingOutputStream.write(TimeTrackingOutputStream.java:59)\n",
      "\tat org.apache.spark.io.MutableCheckedOutputStream.write(MutableCheckedOutputStream.scala:43)\n",
      "\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)\n",
      "\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:127)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:225)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:178)\n",
      "\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)\n",
      "\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:127)\n",
      "\tat java.base/java.io.DataOutputStream.write(DataOutputStream.java:107)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.writeToStream(UnsafeRow.java:519)\n",
      "\tat org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$1.writeValue(UnsafeRowSerializer.scala:69)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:312)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:171)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "24/05/07 17:06:41 ERROR TaskSetManager: Task 0 in stage 1390.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o908.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1390.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1390.0 (TID 4376) (10.6.130.30 executor driver): java.io.IOException: No space left on device\n\tat java.base/java.io.FileOutputStream.writeBytes(Native Method)\n\tat java.base/java.io.FileOutputStream.write(FileOutputStream.java:354)\n\tat org.apache.spark.storage.TimeTrackingOutputStream.write(TimeTrackingOutputStream.java:59)\n\tat org.apache.spark.io.MutableCheckedOutputStream.write(MutableCheckedOutputStream.scala:43)\n\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)\n\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:127)\n\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:225)\n\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:178)\n\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)\n\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:127)\n\tat java.base/java.io.DataOutputStream.write(DataOutputStream.java:107)\n\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.writeToStream(UnsafeRow.java:519)\n\tat org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$1.writeValue(UnsafeRowSerializer.scala:69)\n\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:312)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:171)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\nCaused by: java.io.IOException: No space left on device\n\tat java.base/java.io.FileOutputStream.writeBytes(Native Method)\n\tat java.base/java.io.FileOutputStream.write(FileOutputStream.java:354)\n\tat org.apache.spark.storage.TimeTrackingOutputStream.write(TimeTrackingOutputStream.java:59)\n\tat org.apache.spark.io.MutableCheckedOutputStream.write(MutableCheckedOutputStream.scala:43)\n\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)\n\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:127)\n\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:225)\n\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:178)\n\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)\n\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:127)\n\tat java.base/java.io.DataOutputStream.write(DataOutputStream.java:107)\n\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.writeToStream(UnsafeRow.java:519)\n\tat org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$1.writeValue(UnsafeRowSerializer.scala:69)\n\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:312)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:171)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mevaluated_minilm_cosine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mevaluation == True\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Address-matching/env_address_matching/lib/python3.10/site-packages/pyspark/sql/dataframe.py:1238\u001b[0m, in \u001b[0;36mDataFrame.count\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcount\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m   1216\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns the number of rows in this :class:`DataFrame`.\u001b[39;00m\n\u001b[1;32m   1217\u001b[0m \n\u001b[1;32m   1218\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1236\u001b[0m \u001b[38;5;124;03m    3\u001b[39;00m\n\u001b[1;32m   1237\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1238\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/Address-matching/env_address_matching/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/Address-matching/env_address_matching/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/Address-matching/env_address_matching/lib/python3.10/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o908.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1390.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1390.0 (TID 4376) (10.6.130.30 executor driver): java.io.IOException: No space left on device\n\tat java.base/java.io.FileOutputStream.writeBytes(Native Method)\n\tat java.base/java.io.FileOutputStream.write(FileOutputStream.java:354)\n\tat org.apache.spark.storage.TimeTrackingOutputStream.write(TimeTrackingOutputStream.java:59)\n\tat org.apache.spark.io.MutableCheckedOutputStream.write(MutableCheckedOutputStream.scala:43)\n\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)\n\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:127)\n\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:225)\n\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:178)\n\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)\n\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:127)\n\tat java.base/java.io.DataOutputStream.write(DataOutputStream.java:107)\n\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.writeToStream(UnsafeRow.java:519)\n\tat org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$1.writeValue(UnsafeRowSerializer.scala:69)\n\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:312)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:171)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\nCaused by: java.io.IOException: No space left on device\n\tat java.base/java.io.FileOutputStream.writeBytes(Native Method)\n\tat java.base/java.io.FileOutputStream.write(FileOutputStream.java:354)\n\tat org.apache.spark.storage.TimeTrackingOutputStream.write(TimeTrackingOutputStream.java:59)\n\tat org.apache.spark.io.MutableCheckedOutputStream.write(MutableCheckedOutputStream.scala:43)\n\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)\n\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:127)\n\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:225)\n\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:178)\n\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)\n\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:127)\n\tat java.base/java.io.DataOutputStream.write(DataOutputStream.java:107)\n\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.writeToStream(UnsafeRow.java:519)\n\tat org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$1.writeValue(UnsafeRowSerializer.scala:69)\n\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:312)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:171)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    }
   ],
   "source": [
    "evaluated_minilm_cosine.filter(\"evaluation == True\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluated_gpt3_cosine = calculate_cosine_similarity(\n",
    "    gpt_test_dense_vector, gpt3_train_dense_vector\n",
    ")\n",
    "evaluated_gpt3_cosine = filter_3_best(evaluated_gpt3_cosine)\n",
    "evaluated_gpt3_cosine = evaluated_gpt3_cosine.withColumn(\n",
    "    \"evaluation\", evaluated_gpt3_cosine.uuid_idt == evaluated_gpt3_cosine.uuid_idt2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/13 12:39:16 ERROR TaskMemoryManager: error while calling spill() on org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@523c1daa\n",
      "java.io.IOException: No space left on device\n",
      "\tat java.base/java.io.FileOutputStream.writeBytes(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.write(FileOutputStream.java:354)\n",
      "\tat org.apache.spark.storage.TimeTrackingOutputStream.write(TimeTrackingOutputStream.java:59)\n",
      "\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)\n",
      "\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:127)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:225)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:178)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:323)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.write(UnsafeSorterSpillWriter.java:136)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.spillIterator(UnsafeExternalSorter.java:576)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.spill(UnsafeExternalSorter.java:231)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.trySpillAndAcquire(TaskMemoryManager.java:227)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.acquireExecutionMemory(TaskMemoryManager.java:190)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.allocatePage(TaskMemoryManager.java:317)\n",
      "\tat org.apache.spark.memory.MemoryConsumer.allocatePage(MemoryConsumer.java:117)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.acquireNewPageIfNecessary(UnsafeExternalSorter.java:433)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.allocateMemoryForRecordIfNecessary(UnsafeExternalSorter.java:452)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.insertRecord(UnsafeExternalSorter.java:487)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.insertRow(UnsafeExternalRowSorter.java:138)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:226)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "24/05/13 12:39:16 ERROR TaskMemoryManager: error while calling spill() on org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@4d3a8dbf\n",
      "java.io.IOException: No space left on device\n",
      "\tat java.base/java.io.FileOutputStream.writeBytes(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.write(FileOutputStream.java:354)\n",
      "\tat org.apache.spark.storage.TimeTrackingOutputStream.write(TimeTrackingOutputStream.java:59)\n",
      "\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)\n",
      "\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:127)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:225)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:178)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:323)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.write(UnsafeSorterSpillWriter.java:136)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.spillIterator(UnsafeExternalSorter.java:576)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.spill(UnsafeExternalSorter.java:231)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.trySpillAndAcquire(TaskMemoryManager.java:227)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.acquireExecutionMemory(TaskMemoryManager.java:190)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.allocatePage(TaskMemoryManager.java:317)\n",
      "\tat org.apache.spark.memory.MemoryConsumer.allocatePage(MemoryConsumer.java:117)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.acquireNewPageIfNecessary(UnsafeExternalSorter.java:433)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.allocateMemoryForRecordIfNecessary(UnsafeExternalSorter.java:452)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.insertRecord(UnsafeExternalSorter.java:487)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.insertRow(UnsafeExternalRowSorter.java:138)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:226)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "24/05/13 12:39:16 ERROR Executor: Exception in task 3.0 in stage 1002.0 (TID 3147)\n",
      "org.apache.spark.memory.SparkOutOfMemoryError: error while calling spill() on org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@4d3a8dbf : No space left on device\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.trySpillAndAcquire(TaskMemoryManager.java:253)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.acquireExecutionMemory(TaskMemoryManager.java:190)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.allocatePage(TaskMemoryManager.java:317)\n",
      "\tat org.apache.spark.memory.MemoryConsumer.allocatePage(MemoryConsumer.java:117)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.acquireNewPageIfNecessary(UnsafeExternalSorter.java:433)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.allocateMemoryForRecordIfNecessary(UnsafeExternalSorter.java:452)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.insertRecord(UnsafeExternalSorter.java:487)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.insertRow(UnsafeExternalRowSorter.java:138)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:226)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "24/05/13 12:39:16 ERROR Executor: Exception in task 2.0 in stage 1002.0 (TID 3146)\n",
      "org.apache.spark.memory.SparkOutOfMemoryError: error while calling spill() on org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@523c1daa : No space left on device\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.trySpillAndAcquire(TaskMemoryManager.java:253)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.acquireExecutionMemory(TaskMemoryManager.java:190)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.allocatePage(TaskMemoryManager.java:317)\n",
      "\tat org.apache.spark.memory.MemoryConsumer.allocatePage(MemoryConsumer.java:117)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.acquireNewPageIfNecessary(UnsafeExternalSorter.java:433)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.allocateMemoryForRecordIfNecessary(UnsafeExternalSorter.java:452)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.insertRecord(UnsafeExternalSorter.java:487)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.insertRow(UnsafeExternalRowSorter.java:138)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:226)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "24/05/13 12:39:16 WARN TaskSetManager: Lost task 2.0 in stage 1002.0 (TID 3146) (10.6.130.30 executor driver): org.apache.spark.memory.SparkOutOfMemoryError: error while calling spill() on org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@523c1daa : No space left on device\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.trySpillAndAcquire(TaskMemoryManager.java:253)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.acquireExecutionMemory(TaskMemoryManager.java:190)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.allocatePage(TaskMemoryManager.java:317)\n",
      "\tat org.apache.spark.memory.MemoryConsumer.allocatePage(MemoryConsumer.java:117)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.acquireNewPageIfNecessary(UnsafeExternalSorter.java:433)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.allocateMemoryForRecordIfNecessary(UnsafeExternalSorter.java:452)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.insertRecord(UnsafeExternalSorter.java:487)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.insertRow(UnsafeExternalRowSorter.java:138)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:226)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "24/05/13 12:39:16 ERROR TaskSetManager: Task 2 in stage 1002.0 failed 1 times; aborting job\n",
      "24/05/13 12:39:16 WARN TaskSetManager: Lost task 3.0 in stage 1002.0 (TID 3147) (10.6.130.30 executor driver): org.apache.spark.memory.SparkOutOfMemoryError: error while calling spill() on org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@4d3a8dbf : No space left on device\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.trySpillAndAcquire(TaskMemoryManager.java:253)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.acquireExecutionMemory(TaskMemoryManager.java:190)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.allocatePage(TaskMemoryManager.java:317)\n",
      "\tat org.apache.spark.memory.MemoryConsumer.allocatePage(MemoryConsumer.java:117)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.acquireNewPageIfNecessary(UnsafeExternalSorter.java:433)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.allocateMemoryForRecordIfNecessary(UnsafeExternalSorter.java:452)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.insertRecord(UnsafeExternalSorter.java:487)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.insertRow(UnsafeExternalRowSorter.java:138)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:226)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "24/05/13 12:39:16 WARN TaskSetManager: Lost task 8.0 in stage 1002.0 (TID 3152) (10.6.130.30 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 2 in stage 1002.0 failed 1 times, most recent failure: Lost task 2.0 in stage 1002.0 (TID 3146) (10.6.130.30 executor driver): org.apache.spark.memory.SparkOutOfMemoryError: error while calling spill() on org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@523c1daa : No space left on device\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.trySpillAndAcquire(TaskMemoryManager.java:253)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.acquireExecutionMemory(TaskMemoryManager.java:190)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.allocatePage(TaskMemoryManager.java:317)\n",
      "\tat org.apache.spark.memory.MemoryConsumer.allocatePage(MemoryConsumer.java:117)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.acquireNewPageIfNecessary(UnsafeExternalSorter.java:433)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.allocateMemoryForRecordIfNecessary(UnsafeExternalSorter.java:452)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.insertRecord(UnsafeExternalSorter.java:487)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.insertRow(UnsafeExternalRowSorter.java:138)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:226)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:)\n",
      "[Stage 1002:>                                                      (0 + 6) / 16]\r"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1019.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 1002.0 failed 1 times, most recent failure: Lost task 2.0 in stage 1002.0 (TID 3146) (10.6.130.30 executor driver): org.apache.spark.memory.SparkOutOfMemoryError: error while calling spill() on org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@523c1daa : No space left on device\n\tat org.apache.spark.memory.TaskMemoryManager.trySpillAndAcquire(TaskMemoryManager.java:253)\n\tat org.apache.spark.memory.TaskMemoryManager.acquireExecutionMemory(TaskMemoryManager.java:190)\n\tat org.apache.spark.memory.TaskMemoryManager.allocatePage(TaskMemoryManager.java:317)\n\tat org.apache.spark.memory.MemoryConsumer.allocatePage(MemoryConsumer.java:117)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.acquireNewPageIfNecessary(UnsafeExternalSorter.java:433)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.allocateMemoryForRecordIfNecessary(UnsafeExternalSorter.java:452)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.insertRecord(UnsafeExternalSorter.java:487)\n\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.insertRow(UnsafeExternalRowSorter.java:138)\n\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:226)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\nCaused by: org.apache.spark.memory.SparkOutOfMemoryError: error while calling spill() on org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@523c1daa : No space left on device\n\tat org.apache.spark.memory.TaskMemoryManager.trySpillAndAcquire(TaskMemoryManager.java:253)\n\tat org.apache.spark.memory.TaskMemoryManager.acquireExecutionMemory(TaskMemoryManager.java:190)\n\tat org.apache.spark.memory.TaskMemoryManager.allocatePage(TaskMemoryManager.java:317)\n\tat org.apache.spark.memory.MemoryConsumer.allocatePage(MemoryConsumer.java:117)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.acquireNewPageIfNecessary(UnsafeExternalSorter.java:433)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.allocateMemoryForRecordIfNecessary(UnsafeExternalSorter.java:452)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.insertRecord(UnsafeExternalSorter.java:487)\n\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.insertRow(UnsafeExternalRowSorter.java:138)\n\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:226)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mevaluated_gpt3_cosine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mevaluation == \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTrue\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Address-matching/env_address_matching/lib/python3.10/site-packages/pyspark/sql/dataframe.py:1238\u001b[0m, in \u001b[0;36mDataFrame.count\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcount\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m   1216\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns the number of rows in this :class:`DataFrame`.\u001b[39;00m\n\u001b[1;32m   1217\u001b[0m \n\u001b[1;32m   1218\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1236\u001b[0m \u001b[38;5;124;03m    3\u001b[39;00m\n\u001b[1;32m   1237\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1238\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/Address-matching/env_address_matching/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/Address-matching/env_address_matching/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/Address-matching/env_address_matching/lib/python3.10/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1019.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 1002.0 failed 1 times, most recent failure: Lost task 2.0 in stage 1002.0 (TID 3146) (10.6.130.30 executor driver): org.apache.spark.memory.SparkOutOfMemoryError: error while calling spill() on org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@523c1daa : No space left on device\n\tat org.apache.spark.memory.TaskMemoryManager.trySpillAndAcquire(TaskMemoryManager.java:253)\n\tat org.apache.spark.memory.TaskMemoryManager.acquireExecutionMemory(TaskMemoryManager.java:190)\n\tat org.apache.spark.memory.TaskMemoryManager.allocatePage(TaskMemoryManager.java:317)\n\tat org.apache.spark.memory.MemoryConsumer.allocatePage(MemoryConsumer.java:117)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.acquireNewPageIfNecessary(UnsafeExternalSorter.java:433)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.allocateMemoryForRecordIfNecessary(UnsafeExternalSorter.java:452)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.insertRecord(UnsafeExternalSorter.java:487)\n\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.insertRow(UnsafeExternalRowSorter.java:138)\n\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:226)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\nCaused by: org.apache.spark.memory.SparkOutOfMemoryError: error while calling spill() on org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@523c1daa : No space left on device\n\tat org.apache.spark.memory.TaskMemoryManager.trySpillAndAcquire(TaskMemoryManager.java:253)\n\tat org.apache.spark.memory.TaskMemoryManager.acquireExecutionMemory(TaskMemoryManager.java:190)\n\tat org.apache.spark.memory.TaskMemoryManager.allocatePage(TaskMemoryManager.java:317)\n\tat org.apache.spark.memory.MemoryConsumer.allocatePage(MemoryConsumer.java:117)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.acquireNewPageIfNecessary(UnsafeExternalSorter.java:433)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.allocateMemoryForRecordIfNecessary(UnsafeExternalSorter.java:452)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.insertRecord(UnsafeExternalSorter.java:487)\n\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.insertRow(UnsafeExternalRowSorter.java:138)\n\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:226)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/13 12:39:18 WARN TaskSetManager: Lost task 7.0 in stage 1002.0 (TID 3151) (10.6.130.30 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 2 in stage 1002.0 failed 1 times, most recent failure: Lost task 2.0 in stage 1002.0 (TID 3146) (10.6.130.30 executor driver): org.apache.spark.memory.SparkOutOfMemoryError: error while calling spill() on org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@523c1daa : No space left on device\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.trySpillAndAcquire(TaskMemoryManager.java:253)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.acquireExecutionMemory(TaskMemoryManager.java:190)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.allocatePage(TaskMemoryManager.java:317)\n",
      "\tat org.apache.spark.memory.MemoryConsumer.allocatePage(MemoryConsumer.java:117)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.acquireNewPageIfNecessary(UnsafeExternalSorter.java:433)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.allocateMemoryForRecordIfNecessary(UnsafeExternalSorter.java:452)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.insertRecord(UnsafeExternalSorter.java:487)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.insertRow(UnsafeExternalRowSorter.java:138)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:226)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/05/13 12:39:19 WARN PythonUDFRunner: Incomplete task 4.0 in stage 1002 (TID 3148) interrupted: Attempting to kill Python Worker\n",
      "24/05/13 12:39:19 WARN PythonUDFRunner: Incomplete task 0.0 in stage 1002 (TID 3144) interrupted: Attempting to kill Python Worker\n",
      "24/05/13 12:39:19 WARN PythonUDFRunner: Incomplete task 5.0 in stage 1002 (TID 3149) interrupted: Attempting to kill Python Worker\n",
      "24/05/13 12:39:19 WARN TaskSetManager: Lost task 4.0 in stage 1002.0 (TID 3148) (10.6.130.30 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 2 in stage 1002.0 failed 1 times, most recent failure: Lost task 2.0 in stage 1002.0 (TID 3146) (10.6.130.30 executor driver): org.apache.spark.memory.SparkOutOfMemoryError: error while calling spill() on org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@523c1daa : No space left on device\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.trySpillAndAcquire(TaskMemoryManager.java:253)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.acquireExecutionMemory(TaskMemoryManager.java:190)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.allocatePage(TaskMemoryManager.java:317)\n",
      "\tat org.apache.spark.memory.MemoryConsumer.allocatePage(MemoryConsumer.java:117)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.acquireNewPageIfNecessary(UnsafeExternalSorter.java:433)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.allocateMemoryForRecordIfNecessary(UnsafeExternalSorter.java:452)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.insertRecord(UnsafeExternalSorter.java:487)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.insertRow(UnsafeExternalRowSorter.java:138)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:226)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/05/13 12:39:19 WARN TaskSetManager: Lost task 5.0 in stage 1002.0 (TID 3149) (10.6.130.30 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 2 in stage 1002.0 failed 1 times, most recent failure: Lost task 2.0 in stage 1002.0 (TID 3146) (10.6.130.30 executor driver): org.apache.spark.memory.SparkOutOfMemoryError: error while calling spill() on org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@523c1daa : No space left on device\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.trySpillAndAcquire(TaskMemoryManager.java:253)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.acquireExecutionMemory(TaskMemoryManager.java:190)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.allocatePage(TaskMemoryManager.java:317)\n",
      "\tat org.apache.spark.memory.MemoryConsumer.allocatePage(MemoryConsumer.java:117)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.acquireNewPageIfNecessary(UnsafeExternalSorter.java:433)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.allocateMemoryForRecordIfNecessary(UnsafeExternalSorter.java:452)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.insertRecord(UnsafeExternalSorter.java:487)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.insertRow(UnsafeExternalRowSorter.java:138)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:226)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/05/13 12:39:19 WARN PythonUDFRunner: Incomplete task 1.0 in stage 1002 (TID 3145) interrupted: Attempting to kill Python Worker\n",
      "24/05/13 12:39:19 WARN PythonUDFRunner: Incomplete task 6.0 in stage 1002 (TID 3150) interrupted: Attempting to kill Python Worker\n",
      "24/05/13 12:39:19 WARN TaskSetManager: Lost task 0.0 in stage 1002.0 (TID 3144) (10.6.130.30 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 2 in stage 1002.0 failed 1 times, most recent failure: Lost task 2.0 in stage 1002.0 (TID 3146) (10.6.130.30 executor driver): org.apache.spark.memory.SparkOutOfMemoryError: error while calling spill() on org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@523c1daa : No space left on device\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.trySpillAndAcquire(TaskMemoryManager.java:253)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.acquireExecutionMemory(TaskMemoryManager.java:190)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.allocatePage(TaskMemoryManager.java:317)\n",
      "\tat org.apache.spark.memory.MemoryConsumer.allocatePage(MemoryConsumer.java:117)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.acquireNewPageIfNecessary(UnsafeExternalSorter.java:433)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.allocateMemoryForRecordIfNecessary(UnsafeExternalSorter.java:452)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.insertRecord(UnsafeExternalSorter.java:487)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.insertRow(UnsafeExternalRowSorter.java:138)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:226)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/05/13 12:39:20 WARN TaskSetManager: Lost task 6.0 in stage 1002.0 (TID 3150) (10.6.130.30 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 2 in stage 1002.0 failed 1 times, most recent failure: Lost task 2.0 in stage 1002.0 (TID 3146) (10.6.130.30 executor driver): org.apache.spark.memory.SparkOutOfMemoryError: error while calling spill() on org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@523c1daa : No space left on device\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.trySpillAndAcquire(TaskMemoryManager.java:253)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.acquireExecutionMemory(TaskMemoryManager.java:190)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.allocatePage(TaskMemoryManager.java:317)\n",
      "\tat org.apache.spark.memory.MemoryConsumer.allocatePage(MemoryConsumer.java:117)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.acquireNewPageIfNecessary(UnsafeExternalSorter.java:433)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.allocateMemoryForRecordIfNecessary(UnsafeExternalSorter.java:452)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.insertRecord(UnsafeExternalSorter.java:487)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.insertRow(UnsafeExternalRowSorter.java:138)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:226)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/05/13 12:39:20 WARN TaskSetManager: Lost task 1.0 in stage 1002.0 (TID 3145) (10.6.130.30 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 2 in stage 1002.0 failed 1 times, most recent failure: Lost task 2.0 in stage 1002.0 (TID 3146) (10.6.130.30 executor driver): org.apache.spark.memory.SparkOutOfMemoryError: error while calling spill() on org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@523c1daa : No space left on device\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.trySpillAndAcquire(TaskMemoryManager.java:253)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.acquireExecutionMemory(TaskMemoryManager.java:190)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.allocatePage(TaskMemoryManager.java:317)\n",
      "\tat org.apache.spark.memory.MemoryConsumer.allocatePage(MemoryConsumer.java:117)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.acquireNewPageIfNecessary(UnsafeExternalSorter.java:433)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.allocateMemoryForRecordIfNecessary(UnsafeExternalSorter.java:452)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.insertRecord(UnsafeExternalSorter.java:487)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.insertRow(UnsafeExternalRowSorter.java:138)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:226)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:375)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:)\n"
     ]
    }
   ],
   "source": [
    "evaluated_gpt3_cosine.filter(\"evaluation == 'True'\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accertion_ratio(evaluated_gpt3_cosine))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similitud de direcciones con Chroma DB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "\n",
    "# Conéctate al cliente de ChromaDB y crea la colección\n",
    "chroma_client = chromadb.Client()\n",
    "collection = chroma_client.create_collection(name=\"address_matching\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import DenseVector\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "\n",
    "def extract_to_chroma(dataframes):\n",
    "    embeddings = []\n",
    "    documents = []\n",
    "    metadata = []\n",
    "    ids = []\n",
    "    start = 0\n",
    "\n",
    "    for index in range(len(dataframes)):\n",
    "        type_df = \"test\" if index == 1 else \"train\"\n",
    "\n",
    "        # Extraer los datos restantes como listas\n",
    "        embeddings_df = dataframes[index].select(\"embedding\").collect()\n",
    "        documents_df = dataframes[index].select(\"direccion\").collect()\n",
    "        metadata_df = dataframes[index].select(\"uuid_idt\").collect()\n",
    "        ids_df = list(range(start, len(embeddings_df) + start))\n",
    "\n",
    "        # print(type(embeddings_df[0].embedding))\n",
    "        # Extraer los embeddings como listas\n",
    "        embeddings_list = []\n",
    "        for row in embeddings_df:\n",
    "            if type(row.embedding) != list:\n",
    "                embeddings_list.append(row.embedding.toArray().tolist())\n",
    "            else:\n",
    "                embeddings_list.append(row.embedding)\n",
    "        embeddings_df = embeddings_list\n",
    "        # embeddings_df = [row.embedding.toArray().tolist() for row in embeddings_df]\n",
    "\n",
    "        # Extraer los documentos como listas\n",
    "        documents_df = [row.direccion for row in documents_df]\n",
    "\n",
    "        # Extraer los metadatos como listas\n",
    "        metadata_df = [\n",
    "            {\"uuid_idt\": row.uuid_idt, \"type_df\": type_df} for row in metadata_df\n",
    "        ]\n",
    "\n",
    "        ids_df = [str(id_) for id_ in ids_df]\n",
    "\n",
    "        embeddings += embeddings_df\n",
    "        documents += documents_df\n",
    "        metadata += metadata_df\n",
    "        ids += ids_df\n",
    "        start = len(ids_df)\n",
    "\n",
    "    return ids, documents, embeddings, metadata\n",
    "\n",
    "\n",
    "def extract_to_chroma1(dataframe: DataFrame, type_df: str):\n",
    "    \"\"\"Extrac the uuids, directions and embeddings of each row of a dataframe.\n",
    "\n",
    "    Args:\n",
    "        dataframe: Dataframe with the data.\n",
    "        type_df: Type of the dataframe. Train or test.\n",
    "\n",
    "    Returns:\n",
    "        embeddings: List with the numeric representation of each direction.\n",
    "        documents: List that contains the directions.\n",
    "        metadata: List with relevant information about the direction\n",
    "          (uuid, latitud, longitud, type_df).\n",
    "        ids: List with as many ids as addresses.\n",
    "    \"\"\"\n",
    "    ids = []\n",
    "    documents = []\n",
    "    embeddings = []\n",
    "    metadata = []\n",
    "\n",
    "    # Recopilar los datos del DataFrame\n",
    "    rows = dataframe.select(\n",
    "        \"uuid_idt\", \"latitud\", \"longitud\", \"direccion\", \"embedding\"\n",
    "    ).collect()\n",
    "    index = 0\n",
    "\n",
    "    for row in rows:\n",
    "        # Extraer los datos de cada fila\n",
    "        uuid_idt = str(row.uuid_idt)\n",
    "        direccion = str(row.direccion)\n",
    "        embedding = (\n",
    "            row.embedding.toArray().tolist()\n",
    "            if isinstance(row.embedding, DenseVector)\n",
    "            else row.embedding\n",
    "        )\n",
    "\n",
    "        # Agregar los datos a las listas correspondientes\n",
    "        ids.append(str(index))\n",
    "        documents.append(direccion)\n",
    "        embeddings.append(embedding)\n",
    "        metadata.append(\n",
    "            {\n",
    "                \"uuid_idt\": uuid_idt,\n",
    "                \"latitud\": float(row.latitud),\n",
    "                \"longitud\": float(row.longitud),\n",
    "                \"type_df\": type_df,\n",
    "            }\n",
    "        )\n",
    "        index += 1\n",
    "    return embeddings, documents, metadata, ids\n",
    "\n",
    "\n",
    "def chunk_data(data, chunk_size: int):\n",
    "    \"\"\"\n",
    "    Divide a list of data into batches of a specific size.\n",
    "\n",
    "    Args:\n",
    "        data: The list of data to be divided into batches.\n",
    "        chunk_size : The size of each batch.\n",
    "\n",
    "    Yields:\n",
    "        list: A batch of data of size `chunk_size`.\n",
    "    \"\"\"\n",
    "    for i in range(0, len(data), chunk_size):\n",
    "        yield data[i : i + chunk_size]\n",
    "\n",
    "\n",
    "def extract_data(\n",
    "    test_result_collection: dict, train_result_collection: dict, dataframe\n",
    "):\n",
    "    type(test_result_collection)\n",
    "    \"\"\"Extract data from the test and train result collections.\n",
    "\n",
    "    Args:\n",
    "        test_result_collection (dict): The result collection from the test data.\n",
    "        train_result_collection (dict): The result collection from the train data.\n",
    "        dataframe (DataFrame): The DataFrame to write the extracted data.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A DataFrame containing the extracted data with the following columns:\n",
    "            - uuid_idt_test\n",
    "            - latitud_test\n",
    "            - longitud_test\n",
    "            - direccion_test\n",
    "            - uuid_idt_train\n",
    "            - latitud_train\n",
    "            - longitud_train\n",
    "            - direccion_train\n",
    "    \"\"\"\n",
    "    test_metadata = test_result_collection[\"metadatas\"][0]\n",
    "    uuid_test_result = test_metadata[\"uuid_idt\"]\n",
    "    latitud_test_result = test_metadata[\"latitud\"]\n",
    "    longitud_test_result = test_metadata[\"longitud\"]\n",
    "    direccion_test_result = test_result_collection[\"documents\"][0]\n",
    "    test_data = (\n",
    "        uuid_test_result,\n",
    "        latitud_test_result,\n",
    "        longitud_test_result,\n",
    "        direccion_test_result,\n",
    "    )\n",
    "\n",
    "    # Datos de entrenamiento\n",
    "    train_metadata = train_result_collection[\"metadatas\"][0]\n",
    "    train_data = [\n",
    "        (metadata[\"uuid_idt\"], metadata[\"latitud\"], metadata[\"longitud\"], direccion)\n",
    "        for metadata, direccion in zip(\n",
    "            train_metadata, train_result_collection[\"documents\"][0]\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # Crear DataFrames temporales y unirlos\n",
    "    temp_data = [(test_data + train_row) for train_row in train_data]\n",
    "    temp_df = spark.createDataFrame(temp_data, schema=schema)\n",
    "    dataframe = dataframe.union(temp_df)\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "\"\"\" def extract_data(test_result_collection, train_result_collection):\n",
    "  global evaluated_no_match_dirs\n",
    "  uuid_test_result = test_result_collection[\"metadatas\"][0][\"uuid_idt\"]\n",
    "  latitud_test_result = test_result_collection[\"metadatas\"][0][\"latitud\"]\n",
    "  longitud_test_result = test_result_collection[\"metadatas\"][0][\"longitud\"]\n",
    "  direccion_test_result = test_result_collection[\"documents\"][0]\n",
    "  \n",
    "  test_data = (uuid_test_result, latitud_test_result, longitud_test_result, direccion_test_result)\n",
    "  \n",
    "  train_data = []\n",
    "  for index in range(len(train_result_collection[\"ids\"][0])):\n",
    "    metadata = train_result_collection[\"metadatas\"][0]\n",
    "    uuid_train_result = metadata[index][\"uuid_idt\"]\n",
    "    latitud_train_result = metadata[index][\"latitud\"]\n",
    "    longitud_train_result = metadata[index][\"longitud\"]\n",
    "    direccion_train_result = train_result_collection[\"documents\"][0][index]\n",
    "    \n",
    "    train_data.append((uuid_train_result, latitud_train_result, longitud_train_result, direccion_train_result))\n",
    "\n",
    "  # Creación de DataFrame para cada dato de entrenamiento y unión con el DataFrame del conjunto de prueba\n",
    "  df_rows = [(test_data + result) for result in train_data]\n",
    "  df = spark.createDataFrame(df_rows, schema)\n",
    "  \n",
    "  # Uniones de DataFrames\n",
    "  evaluated_no_match_dirs = evaluated_no_match_dirs.union(df) \"\"\"\n",
    "\n",
    "\n",
    "def evaluation(\n",
    "    collection_test: chromadb.api.models.Collection.Collection,\n",
    "    collection_train: chromadb.api.models.Collection.Collection,\n",
    "    max_id: int,\n",
    "):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        collection_test: Collection with the data to search for similar.\n",
    "        collection_train: Collection to query the n best embeddings.\n",
    "        max_id: Max id to iterate.\n",
    "\n",
    "    Returns: \n",
    "        match: Number of hits.\n",
    "        no_match: Numbers of no hits.\n",
    "        evaluated_dirs: Dataframe with the made comparations.\n",
    "    \"\"\"\n",
    "    evaluated_dirs = spark.createDataFrame([], schema)\n",
    "\n",
    "    counter = 0\n",
    "    match = 0\n",
    "    no_match = 0\n",
    "    while counter < max_id:\n",
    "        # Select from test df\n",
    "        result = collection_test.get(\n",
    "            ids=[str(counter)], include=[\"embeddings\", \"metadatas\", \"documents\"]\n",
    "        )\n",
    "\n",
    "        # Embedding and uuid from test df\n",
    "        embedding_to_evaluate = result[\"embeddings\"][0]\n",
    "        uuid_to_evaluate = result[\"metadatas\"][0][\"uuid_idt\"]\n",
    "\n",
    "        # Generate best similarities\n",
    "        best_similarities = collection_train.query(\n",
    "            query_embeddings=[embedding_to_evaluate], n_results=3\n",
    "        )\n",
    "\n",
    "        best_uuids = [u[\"uuid_idt\"] for i in best_similarities[\"metadatas\"] for u in i]\n",
    "\n",
    "        if uuid_to_evaluate in best_uuids:\n",
    "            match += 1\n",
    "        else:\n",
    "            no_match += 1\n",
    "            evaluated_dirs = extract_data(result, best_similarities, evaluated_dirs)\n",
    "        counter += 1\n",
    "    return match, no_match, evaluated_dirs\n",
    "\n",
    "\n",
    "def add_to_collection(\n",
    "    collection: chromadb.api.models.Collection.Collection, \n",
    "    embeddings_list: list, \n",
    "    documents_list: list, \n",
    "    metadata_list: list, \n",
    "    ids_list: list\n",
    "):\n",
    "    \"\"\"\n",
    "    Add elements to a collection.\n",
    "\n",
    "    Args:\n",
    "        collection: The collection to which the elements will be added.\n",
    "        embeddings_list (list): A list of embeddings.\n",
    "        documents_list (list): A list of documents.\n",
    "        metadata_list (list): A list of metadata.\n",
    "        ids_list (list): A list of IDs.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    batch_size = 41666\n",
    "    embeddings_chunks = list(chunk_data(embeddings_list, batch_size))\n",
    "    documents_chunks = list(chunk_data(documents_list, batch_size))\n",
    "    metadata_chunks = list(chunk_data(metadata_list, batch_size))\n",
    "    ids_chunks = list(chunk_data(ids_list, batch_size))\n",
    "\n",
    "    # Agrega cada lote por separado a la colección\n",
    "    for embeddings, documents, metadata, ids in zip(\n",
    "        embeddings_chunks, documents_chunks, metadata_chunks, ids_chunks\n",
    "    ):\n",
    "        collection.add(\n",
    "            embeddings=embeddings, documents=documents, metadatas=metadata, ids=ids\n",
    "        )\n",
    "\n",
    "\n",
    "schema = \"uuid_idt_test: string, \\\n",
    "        latitud_test: float, \\\n",
    "        longitud_test: float, \\\n",
    "        direccion_test: string, \\\n",
    "        uuid_idt_train: string, \\\n",
    "        latitud_train: float, \\\n",
    "        longitud_train: float, \\\n",
    "        direccion_train: string\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "ids_list, documents_list, embeddings_list, metadata_list = extract_to_chroma1(\n",
    "    test_embeddings_word2vec, \"test\"\n",
    ")\n",
    "collection_test_w2v = chroma_client.create_collection(name=\"test_df_w2v\")\n",
    "add_to_collection(\n",
    "    collection_test_w2v, ids_list, documents_list, embeddings_list, metadata_list\n",
    ")  # ?min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "ids_list, documents_list, embeddings_list, metadata_list = extract_to_chroma1(\n",
    "    train_embeddings_word2vec, \"train\"\n",
    ")\n",
    "collection_train_w2v = chroma_client.create_collection(name=\"train_df_w2v\")\n",
    "add_to_collection(\n",
    "    collection_train_w2v, ids_list, documents_list, embeddings_list, metadata_list\n",
    ")  # 21min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15841 67160\n"
     ]
    }
   ],
   "source": [
    "matches_count, no_matches_count = evaluation(\n",
    "    collection_test_w2v, collection_train_w2v, test_embeddings_word2vec.count() - 1\n",
    ")\n",
    "print(match, no_match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ids': [['90351', '33999', '130209', '91576', '140145']], 'distances': [[0.0, 0.19808949530124664, 0.20216217637062073, 0.20269343256950378, 0.22468826174736023]], 'metadatas': [[{'hnsw:space': 'cosine', 'type': 'test', 'uuid_idt': '6BD93CCB-3251-11E8-9BFD-480FCF5217B3'}, {'hnsw:space': 'cosine', 'type': 'train', 'uuid_idt': '683E27B8-3251-11E8-B413-480FCF5217B3'}, {'hnsw:space': 'cosine', 'type': 'test', 'uuid_idt': '6091C9B3-3251-11E8-BB6A-480FCF5217B3'}, {'hnsw:space': 'cosine', 'type': 'test', 'uuid_idt': '737D7422-1C73-476A-815F-292B1C2C11DD'}, {'hnsw:space': 'cosine', 'type': 'test', 'uuid_idt': '6273B1E0-3251-11E8-A556-480FCF5217B3'}]], 'embeddings': None, 'documents': [['CAMIN BOQUERON VALEL GUERRA 189 SAN CRISTOBAL DE LA LAGUNA', 'CG CARRETERA BOQUERON VALEL GUERRA 29 SAN CRISTOBAL DE LA LAGUNA', 'EXTRARRADIO VINO AVLLE GUERRA 357 SAN CRISTOBAL DE LA LAGUNA', 'DEIF BOQUERON VALLE GUERRA 85 SAN CRISTOBAL DE LA LAGUNA', 'CALLE BOQUERON VALLE GUERRA 214 SAN CRISTOBAL DE LA LAGUNA']], 'uris': None, 'data': None}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' # print(uuids)\\nif u1 in uuids:\\n  yes+=1\\n  print(result)\\n  print(\"--------\")\\n  print(results_q)\\nelse:\\n  no+=1\\ncounter+=1\\n\\nprint(yes)\\nprint(no) '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\" max_id = w2v_cosine.count() + word2vec_result.count() - 1\n",
    "counter = w2v_cosine.count()\n",
    "yes = 0\n",
    "no = 0\n",
    "while counter < max_id:\n",
    "  # Select from train df\n",
    "  result = collection.get(\n",
    "    ids=[str(counter)],\n",
    "    include=[\"embeddings\", \"metadatas\", \"documents\"]\n",
    "  )\n",
    "\n",
    "  # print(result)\n",
    "  # print(result[\"embeddings\"][0])\n",
    "  # print(result[\"metadatas\"][0][\"uuid_idt\"])\n",
    "  \n",
    "  # Embedding from train df\n",
    "  e1 = result[\"embeddings\"][0]\n",
    "  u1 = result[\"metadatas\"][0][\"uuid_idt\"]\n",
    "\n",
    "  # Generate best similarities\n",
    "  # TODO: filtrar antes por aquellos embeddings que son de \"train\"\n",
    "  results_q = collection.query(\n",
    "    query_embeddings=[e1],\n",
    "    n_results=2,\n",
    "  )\n",
    "\n",
    "  uuids = [u[\"uuid_idt\"] for i in results_q[\"metadatas\"] for u in i]\n",
    "  # print(uuids)\n",
    "\n",
    "  if u1 in uuids:\n",
    "    yes+=1\n",
    "    print(result)\n",
    "    print(\"--------\")\n",
    "    print(results_q)\n",
    "  else:\n",
    "    no+=1\n",
    "  counter+=1\n",
    "\n",
    "print(yes)\n",
    "print(no) \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_client.delete_collection(\"test_df\")\n",
    "chroma_client.delete_collection(\"train_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_test_gpt = chroma_client.create_collection(name=\"test_df\")\n",
    "collection_train_gpt = chroma_client.create_collection(name=\"train_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "ids_list, documents_list, embeddings_list, metadata_list = extract_to_chroma1(\n",
    "    gpt_test_list, \"test\"\n",
    ")\n",
    "add_to_collection(\n",
    "    collection_test_gpt, ids_list, documents_list, embeddings_list, metadata_list\n",
    ")  # xmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "ids_list, documents_list, embeddings_list, metadata_list = extract_to_chroma1(\n",
    "    gpt_train_list, \"train\"\n",
    ")\n",
    "add_to_collection(\n",
    "    collection_train_gpt, ids_list, documents_list, embeddings_list, metadata_list\n",
    ")  # xmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3320 3320\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1479:====================================================> (31 + 1) / 32]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13959 13957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(gpt_test_list.count(), collection_test_gpt.count())\n",
    "print(gpt_train_list.count(), collection_train_gpt.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3078 241\n"
     ]
    }
   ],
   "source": [
    "matches_count, no_matches_count, best_results_df = evaluation(\n",
    "    collection_test_gpt, \n",
    "    collection_train_gpt, \n",
    "    collection_test_gpt.count() - 1\n",
    ")\n",
    "print(match, no_match)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MiniLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" from pyspark.sql.functions import udf\n",
    "\n",
    "@udf(returnType=ArrayType(FloatType()))\n",
    "def minilm_embedding_list(direccion):\n",
    "    \"\"\"Calcula el embedding de la dirección dada y lo convierte a una lista para evitar la serialización de Spark.\"\"\"\n",
    "    # Sentences are encoded by calling model.encode()\n",
    "    embedding = model.encode(direccion)\n",
    "    # Convertir el embedding a una lista de Python para evitar la serialización\n",
    "    return embedding.tolist()\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "minilm_result = train_df.withColumn(\"embedding\", minilm_embedding_list(\"direccion\"))\n",
    "minilm_cosine = test_df.withColumn(\"embedding\", minilm_embedding_list(\"direccion\")) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_client.delete_collection(\"test_df_minilm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_client.delete_collection(\"train_df_minilm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_test_minilm = chroma_client.create_collection(name=\"test_df_minilm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_train_minilm = chroma_client.create_collection(name=\"train_df_minilm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3622:=================================================>      (7 + 1) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(minilm_train_embedding_list.count())\n",
    "print(minilm_test_embedding_list.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "embeddings_list, documents_list, metadata_list, ids_list = extract_to_chroma1(\n",
    "    minilm_test_embedding_list, \"test\"\n",
    ")\n",
    "add_to_collection(\n",
    "    collection_test_minilm, embeddings_list, documents_list, metadata_list, ids_list\n",
    ")  # 9min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "embeddings_list, documents_list, metadata_list, ids_list = extract_to_chroma1(\n",
    "    minilm_train_embedding_list, \"train\"\n",
    ")\n",
    "add_to_collection(\n",
    "    collection_train_minilm, embeddings_list, documents_list, metadata_list, ids_list\n",
    ")  # 11min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4735 386\n"
     ]
    }
   ],
   "source": [
    "matches_count, no_matches_count, best_results_df = evaluation(\n",
    "    collection_test_minilm, \n",
    "    collection_train_minilm, \n",
    "    collection_test_minilm.count() - 1\n",
    ")\n",
    "print(match, no_match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/06 12:54:46 WARN DAGScheduler: Broadcasting large task binary with size 6.3 MiB\n",
      "24/05/06 12:54:47 WARN DAGScheduler: Broadcasting large task binary with size 6.3 MiB\n",
      "24/05/06 12:54:47 WARN DAGScheduler: Broadcasting large task binary with size 6.3 MiB\n",
      "24/05/06 12:54:48 WARN DAGScheduler: Broadcasting large task binary with size 6.3 MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+------------+-------------+---------------------------------------------------------------------+------------------------------------+-------------+--------------+----------------------------------------------------------+\n",
      "|uuid_idt_test                       |latitud_test|longitud_test|direccion_test                                                       |uuid_idt_train                      |latitud_train|longitud_train|direccion_train                                           |\n",
      "+------------------------------------+------------+-------------+---------------------------------------------------------------------+------------------------------------+-------------+--------------+----------------------------------------------------------+\n",
      "|67842392-3251-11E8-B06D-480FCF5217B3|28.478521   |-16.308836   |CALLE ESC AGUIAR SOTO RES DRAGO FASE II 30 SAN CRISTOBAL DE LA LAGUNA|60F6009A-3251-11E8-B7C6-480FCF5217B3|28.49091     |-16.322086    |CALLE ESPINAL 19 SAN CRISTOBAL DE LA LAGUNA               |\n",
      "|67842392-3251-11E8-B06D-480FCF5217B3|28.478521   |-16.308836   |CALLE ESC AGUIAR SOTO RES DRAGO FASE II 30 SAN CRISTOBAL DE LA LAGUNA|67B2EA0B-3251-11E8-BD97-480FCF5217B3|28.53544     |-16.361206    |CALLE CERCADO SEÑOR TEJINA 28 SAN CRISTOBAL DE LA LAGUNA  |\n",
      "|67842392-3251-11E8-B06D-480FCF5217B3|28.478521   |-16.308836   |CALLE ESC AGUIAR SOTO RES DRAGO FASE II 30 SAN CRISTOBAL DE LA LAGUNA|67B2EA0B-3251-11E8-BD97-480FCF5217B3|28.53544     |-16.361206    |CALLE ESPINAL TEJINA 22 SAN CRISTOBAL DE LA LAGUNA        |\n",
      "|716F9C2F-3251-11E8-A212-480FCF5217B3|28.490538   |-16.31851    |CALLE SALIDA 9 SAN CRISTOBAL DE LA LAGUNA                            |6E35DA0F-3251-11E8-B965-480FCF5217B3|28.487091    |-16.320686    |CALLE SALOMON 9 SAN CRISTOBAL DE LA LAGUNA                |\n",
      "|716F9C2F-3251-11E8-A212-480FCF5217B3|28.490538   |-16.31851    |CALLE SALIDA 9 SAN CRISTOBAL DE LA LAGUNA                            |C052D2BA-0AD1-11EA-8401-7756D0C25261|28.46116     |-16.30177     |CALLE SALIDA CHUMBERAS 9 SAN CRISTOBAL DE LA LAGUNA       |\n",
      "|716F9C2F-3251-11E8-A212-480FCF5217B3|28.490538   |-16.31851    |CALLE SALIDA 9 SAN CRISTOBAL DE LA LAGUNA                            |7319F37D-3251-11E8-B70D-480FCF5217B3|28.485228    |-16.320099    |CALLE SALIDA 7 SAN CRISTOBAL DE LA LAGUNA                 |\n",
      "|747F47F5-3251-11E8-B2E7-480FCF5217B3|28.473421   |-16.304382   |CARRETERA CURVA GRACIA 7 SAN CRISTOBAL DE LA LAGUNA                  |692A201D-3251-11E8-B4A5-480FCF5217B3|28.4899      |-16.372274    |CARRETERA CTRA GRAL ORTIGAL 211 SAN CRISTOBAL DE LA LAGUNA|\n",
      "|747F47F5-3251-11E8-B2E7-480FCF5217B3|28.473421   |-16.304382   |CARRETERA CURVA GRACIA 7 SAN CRISTOBAL DE LA LAGUNA                  |846EA4AB-841B-11EA-A806-0768FB2B34E1|28.5511      |-16.34873     |CARRETERA CTRA GRAL BAJAMAR 15 SAN CRISTOBAL DE LA LAGUNA |\n",
      "|747F47F5-3251-11E8-B2E7-480FCF5217B3|28.473421   |-16.304382   |CARRETERA CURVA GRACIA 7 SAN CRISTOBAL DE LA LAGUNA                  |C08AB1BC-0AD1-11EA-8401-7756D0C25261|28.45878     |-16.30604     |CARRETERA CTRA GRAL SUR 1 SAN CRISTOBAL DE LA LAGUNA      |\n",
      "|C48075ED-3EDA-11EB-845E-5FC45ADE36E3|28.47643    |-16.30047    |CALLE BARRANQUILLO ACENTEJO FINCA ESPAÑ 4 SAN CRISTOBAL DE LA LAGUNA |6D656973-3251-11E8-B901-480FCF5217B3|28.489985    |-16.328224    |CALLE ACEVIÑO SAN LAZARO 4 SAN CRISTOBAL DE LA LAGUNA     |\n",
      "+------------------------------------+------------+-------------+---------------------------------------------------------------------+------------------------------------+-------------+--------------+----------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1855:==================>                                   (11 + 9) / 32]\r"
     ]
    }
   ],
   "source": [
    "evaluated_dirs.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+---------+----------+---------+-----------------+-----+------+--------------------------+----------------------------------------------------------+\n",
      "|uuid_idt                            |latitud  |longitud  |tvia     |nvia             |numer|codmun|nommun                    |direccion                                                 |\n",
      "+------------------------------------+---------+----------+---------+-----------------+-----+------+--------------------------+----------------------------------------------------------+\n",
      "|692A201D-3251-11E8-B4A5-480FCF5217B3|28.489901|-16.372274|CARRETERA|CTRA GRAL ORTIGAL|211  |38023 |San Cristóbal de La Laguna|CARRETERA CTRA GRAL ORTIGAL 211 SAN CRISTOBAL DE LA LAGUNA|\n",
      "+------------------------------------+---------+----------+---------+-----------------+-----+------+--------------------------+----------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "addresses_df.filter(\n",
    "    \"direccion == 'CARRETERA CTRA GRAL ORTIGAL 211 SAN CRISTOBAL DE LA LAGUNA'\"\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TODO: \n",
    "- probar con proporción 90 - 10\n",
    "1. ver si las direcciones mal clasificadas están próximas a estar bien clasificadas. Agregar las coordenadas al df y en el caso de estar mal clasificadas, ver por cuanto fallan. Extraer 100 de cada caso (bien y mal clasificado)\n",
    "- Usar 200 mil direcciones para calcular los embeddings usando la API de OpenAI dada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "- Probar con frecuencias de 10, 30, 50, 100 dependiendo si los resultados mejoran mucho o no\n",
    "- Sacar el csv solo con los caso de no match\n",
    "- Sacar metricas con la distancia de haversine, media, máxima, etc, con los 3 o con el mejor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, udf\n",
    "from haversine import haversine\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "# Definir una función UDF (User Defined Function) para aplicar haversine\n",
    "haversine_udf = udf(\n",
    "    lambda lat1, lon1, lat2, lon2: haversine((lat1, lon1), (lat2, lon2)),\n",
    "    returnType=DoubleType(),\n",
    ")\n",
    "\n",
    "# Aplicar la función UDF a las columnas del DataFrame\n",
    "evaluated_dirs = best_results_df.withColumn(\n",
    "    \"haversine_distance\",\n",
    "    haversine_udf(\n",
    "        col(\"test_df.latitud\"),\n",
    "        col(\"test_df.longitud\"),\n",
    "        col(\"train_df.latitud\"),\n",
    "        col(\"train_df.longitud\"),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluated_dirs.coalesce(1).write.csv(\"../data/proccesed_data/.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluated_dirs.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluated_dirs = evaluated_dirs.repartition(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluated_dirs.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 745:>                                                       (0 + 8) / 16]\r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "\n",
    "mean_value = round(evaluated_dirs.agg(avg(\"haversine_distance\")).collect()[0][0], 2)\n",
    "# max_value = round(evaluated_dirs.agg({\"haversine_distance\": \"max\"}).collect()[0][0], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(mean_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "modelo = 'Word2Vec'\n",
    "# modelo = 'GTP'\n",
    "# modelo = \"MiniLM\"\n",
    "tecnology = 'Spark'\n",
    "# tecnology = \"Chroma DB\"\n",
    "hit_rate = round(matches_count / test_df.count(), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.93\n"
     ]
    }
   ],
   "source": [
    "print(hit_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(matches_count)\n",
    "print(test_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file '../data/proccesed_data/results.csv' created successfully.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "\n",
    "def write_model_results(cabecera: list, data: list):\n",
    "    \"\"\"Create or write the data into a csv file.\"\"\"\n",
    "    csv_file_path = f\"../data/proccesed_data/results.csv\"\n",
    "    escribir_cabecera = not os.path.exists(csv_file_path)\n",
    "\n",
    "    with open(csv_file_path, mode=\"a\", newline=\"\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        if escribir_cabecera:\n",
    "            writer.writerow(cabecera)\n",
    "        writer.writerows(data)\n",
    "\n",
    "    print(f\"CSV file '{csv_file_path}' created successfully.\")\n",
    "\n",
    "\n",
    "cabecera = [\n",
    "    \"Model\",\n",
    "    \"Tecnology\",\n",
    "    \"Municipaly\",\n",
    "    \"Train dataset size\",\n",
    "    \"Test dataframe size\",\n",
    "    \"Aumented data\",\n",
    "    \"Frecuency\",\n",
    "    \"Hit rate\",\n",
    "    \"Mean Haversine distance\",\n",
    "    # \"Max Haversine distance\",\n",
    "]\n",
    "\n",
    "data = [\n",
    "    [\n",
    "        modelo,\n",
    "        tecnology,\n",
    "        MUNICIPIO,\n",
    "        train_df.count(),\n",
    "        test_df.count(),\n",
    "        AUMENTED_DATA,\n",
    "        MIN_FRENCUENCY,\n",
    "        hit_rate,\n",
    "        mean_value,\n",
    "        # max_value,\n",
    "    ]\n",
    "]\n",
    "\n",
    "write_model_results(cabecera, data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
