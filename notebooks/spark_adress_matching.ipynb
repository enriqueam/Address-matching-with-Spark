{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Address Matching with Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iniciar Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.session import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"Address Matching\")\n",
    "    .config(\"spark.executor.memory\", \"2g\")\n",
    "    .config(\"spark.driver.memory\", \"20g\")\n",
    "    .config(\"spark.driver.maxResultSize\", \"4g\")\n",
    "    .config(\"spark.local.dir\", \"/spark-tmp\") \\\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = spark.sparkContext.getConf()\n",
    "for key, value in conf.getAll():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lectura de ficheros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El dataset continene exactamente 24 columnas, gran parte de ellas son utilizadas por el ISTAC para el propio trabajo que realizan. En este proyecto, se utilizarán las siguientes columnas pues contienen la información más relevante:\n",
    "- **uuid_idt**: contiene un identificador alfanumérico único para cada dirección.\n",
    "- **latitud**\n",
    "- **longitud**\n",
    "- **tvia**: contiene el tipo de vía, es decir, si es una calle, avenida, carretera, etc.\n",
    "- **nvia**: contiene el nombre de la vía.\n",
    "- **numer**: contiene el número de la vía.\n",
    "- **codmun**: está compuesto por un código numérico que identifica a un municipio, es decir, el código postal.\n",
    "- **nommun**: contiene el nombre del municipio.\n",
    "- **direccion**: contiene la dirección completa.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import upper\n",
    "\n",
    "# Leer el archivo CSV y cargarlo en un DataFrame\n",
    "file = \"../data/raw_data/TFM_Direcciones.tab\"\n",
    "first_df = (\n",
    "    spark.read.option(\"delimiter\", \"\\t\")\n",
    "    .option(\"encoding\", \"latin1\")  # Cambiar la codificación a UTF-8\n",
    "    .csv(file, header=True, inferSchema=True)\n",
    ")\n",
    "\n",
    "# Seleccionar solo las columnas deseadas\n",
    "selected_columns = [\n",
    "    \"uuid_idt\",\n",
    "    \"latitud\",\n",
    "    \"longitud\",\n",
    "    \"tvia\",\n",
    "    \"nvia\",\n",
    "    \"numer\",\n",
    "    \"codmun\",\n",
    "    \"nommun\",\n",
    "    \"direccion\",\n",
    "]\n",
    "\n",
    "first_df = first_df.select(selected_columns)\n",
    "first_df = first_df.withColumn(\"nommun\", upper(first_df[\"nommun\"]))\n",
    "\n",
    "# Mostrar el DataFrame\n",
    "first_df.show()\n",
    "first_df.schema\n",
    "print(first_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"../data/raw_data/data-09022024.csv\"\n",
    "second_df = spark.read.option(\"header\", True).csv(file)\n",
    "\n",
    "selected_columns = [\"uuid_idt\", \"latitud\", \"longitud\", \"codmun\", \"nommun\", \"direccion\"]\n",
    "\n",
    "second_df = second_df.select(selected_columns)\n",
    "second_df = second_df.select(\n",
    "    upper(\"uuid_idt\").alias(\"uuid_idt\"),\n",
    "    \"latitud\",\n",
    "    \"longitud\",\n",
    "    \"codmun\",\n",
    "    upper(\"nommun\").alias(\"nommun\"),\n",
    "    \"direccion\",\n",
    ")\n",
    "\n",
    "municipalities = first_df.select(\"nommun\").distinct()\n",
    "second_df = second_df.join(municipalities, \"nommun\", \"inner\")\n",
    "\n",
    "second_df.show()\n",
    "print(second_df.count())\n",
    "print(second_df.count() + first_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count\n",
    "\n",
    "\n",
    "def uuid_frecuency(dataframe):\n",
    "    \"\"\"Frequency of the number of addresses associated by uuid_idt.\"\"\"\n",
    "    uuid_counts = dataframe.groupBy(\"uuid_idt\").count()\n",
    "    values_under_10 = (\n",
    "        uuid_counts\n",
    "        .filter(\"count <= 9\")\n",
    "        .groupBy(\"count\")\n",
    "        .agg(count(\"*\").alias(\"Frecuencia\"))\n",
    "        .orderBy(\"count\")\n",
    "    )\n",
    "    values_under_10 = values_under_10.withColumnRenamed(\n",
    "        \"count\", \"Número de direcciones asociadas\"\n",
    "    )\n",
    "\n",
    "    values_over_10 = dataframe.groupBy(dataframe.uuid_idt).count().filter(\"count > 9\")\n",
    "    values_over_10 = spark.createDataFrame(\n",
    "        [[\"10 o más\", values_over_10.count()]],\n",
    "        [\"Número de direcciones asociadas\", \"Frecuencia\"],\n",
    "    )\n",
    "\n",
    "    values = values_under_10.union(values_over_10)\n",
    "    values.show()\n",
    "\n",
    "\n",
    "uuid_frecuency(first_df)\n",
    "uuid_frecuency(second_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unión de los dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_uuid_first_df = first_df.select(\"uuid_idt\").distinct()\n",
    "unique_uuid_second_df = second_df.select(\"uuid_idt\").distinct()\n",
    "uuid_comunes = unique_uuid_first_df.join(unique_uuid_second_df, \"uuid_idt\", \"inner\")\n",
    "\n",
    "addresses_df = first_df.unionByName(\n",
    "    second_df.join(uuid_comunes, \"uuid_idt\", \"inner\"), allowMissingColumns=True\n",
    ")\n",
    "\n",
    "addresses_df.show()\n",
    "\n",
    "print(\"Tamaño del dataframe ampliado: \", addresses_df.count())\n",
    "print(\n",
    "    \"Número de uuid_idt nuevos del segundo dataframe: \",\n",
    "    unique_uuid_second_df.subtract(uuid_comunes).count(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "addresses_df = addresses_df.withColumn(\n",
    "    \"latitud\", col(\"latitud\").cast(\"float\")\n",
    ").withColumn(\"longitud\", col(\"longitud\").cast(\"float\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uuid_frecuency(addresses_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partición del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "addresses_df = addresses_df.repartition(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limpieza del dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tras visualizar el dataset, se observa que hay columnas que tienen valores desconocidos, representados con '_U', y valores nulos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Initial dataset size: {addresses_df.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Valores nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "\n",
    "addresses_df.select(\n",
    "    [count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in addresses_df.columns]\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "addresses_df = addresses_df.na.fill(\"_U\")\n",
    "\n",
    "addresses_df.select(\n",
    "    [count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in addresses_df.columns]\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracción del prefijo para eleminar valores '_U'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tras analizar el dataset, se observa que para una dirección, el tipo de vía (tvia) puede aparecer vacío, en cambio, en la columna 'direccion' aparece este valor. Por lo que se procederá a extraer el prefijo de la columna 'direccion' para rellenar los valores vacíos de la columna 'tvia' evitando así la perdida de información."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"Unknown tvia before prefix extraction: {addresses_df.filter(addresses_df.tvia == '_U').count()}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvia_types = addresses_df.select(\"tvia\").distinct().collect()\n",
    "tvia_types = {t.tvia for t in tvia_types if t.tvia != \"_U\" and not t.tvia.isnumeric()}\n",
    "print(tvia_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_extract, when\n",
    "\n",
    "condition = (addresses_df.tvia == \"_U\") & (\n",
    "    regexp_extract(\"direccion\", r\"^(\\S+)\", 1).isin(tvia_types)\n",
    ")\n",
    "addresses_df = addresses_df.withColumn(\n",
    "    \"tvia\",\n",
    "    when(condition, regexp_extract(\"direccion\", r\"^(\\S+)\", 1)).otherwise(\n",
    "        addresses_df.tvia\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"Unknown tvia after prefix extraction: {addresses_df.filter(addresses_df.tvia == '_U').count()}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede observar que los tipos de vías que siguen siendo desconocidos es debido a que desde la columna de dirección no se ha podido extraer un prefijo que aparezca en la lista de los tipos de vías que tenemos en el dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limpieza de entradas con valor '_U'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tras realizar la limpieza del dataset y extraer el tipo de via de la columna 'direccion', se procederá a eliminar las entradas restantes que contengan valores '_U' en las columnas 'tvia' pues no aportan información relevante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"Unknown tvia before duplicated null cleaning: {addresses_df.filter(addresses_df.tvia == '_U').count()}\"\n",
    ")\n",
    "print(addresses_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "addresses_df = addresses_df.filter(addresses_df.tvia != \"_U\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"Unknown tvia after duplicated null cleaning: {addresses_df.filter(addresses_df.tvia == '_U').count()}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Tamaño final del dataframe: {addresses_df.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtros de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUMENTED_DATA = False\n",
    "MIN_FRENCUENCY = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aumento de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frecuencia de uuid_idt antes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uuid_frecuency(addresses_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "addresses_df.filter(\"uuid_idt == '015AF5E4-05FD-4636-B80E-1835A087D3DC'\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aumento de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se procederá a realizar un aumento de datos si es especificado para crear direcciones artificiales, a partir de las existentes, con el fin de reducir el número de uuid_idt que tienen pocas direcciones asociadas. Lo cambios a realizar a las nuevas direcciones consistirá en: \n",
    "- Añadir errores ortográficos intercambiando el dos letras adyacentes de una palabra.\n",
    "- Cambiar el tipo de vía por uno aleatorio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "import random\n",
    "\n",
    "\n",
    "# Define la función UDF\n",
    "def switch_letters(direcction: str) -> str:\n",
    "    \"\"\"Intercambia letras adyacentes en una palabra.\n",
    "\n",
    "    Args:\n",
    "        direction: string with the direction\n",
    "    \"\"\"\n",
    "    words = direcction.split()\n",
    "    word_candidates = [word for word in words if word.isalpha() and len(word) >= 2]\n",
    "    if not word_candidates:\n",
    "        return direcction\n",
    "    word = random.choice(word_candidates)\n",
    "    pos = random.randint(0, len(word) - 2)\n",
    "    return \" \".join(\n",
    "        w if w != word else w[:pos] + w[pos + 1] + w[pos] + w[pos + 2 :] for w in words\n",
    "    )\n",
    "\n",
    "\n",
    "def update_direction_via(tvia: str, direction: str) -> str:\n",
    "    \"\"\"Actualiza el tipo de vía de la dirección\"\"\"\n",
    "    old_tvia = direction.split(\" \")[0]\n",
    "    if tvia.upper() in tvia_types:\n",
    "        direction = direction.replace(old_tvia, tvia)\n",
    "    return direction\n",
    "\n",
    "\n",
    "def select_via_type(tvia: str) -> list:\n",
    "    \"\"\"Selecciona tipos de vía alternativos sin duplicados\"\"\"\n",
    "    return random.choice(\n",
    "        [t for t in tvia_types if t.title() != tvia and t != tvia.lower()]\n",
    "    )\n",
    "\n",
    "\n",
    "# Registra la función UDF con Spark\n",
    "spark.udf.register(\"switch_letters\", switch_letters, StringType())\n",
    "spark.udf.register(\"select_via_type\", select_via_type, StringType())\n",
    "spark.udf.register(\"update_direction_via\", update_direction_via, StringType())\n",
    "\n",
    "if AUMENTED_DATA:\n",
    "    print(\"enter\")\n",
    "    unique_filtered_uuids = (\n",
    "        addresses_df.groupBy(\"uuid_idt\")\n",
    "        .count()\n",
    "        .filter(f\"count <= {MIN_FRENCUENCY}\")\n",
    "        .select(\"uuid_idt\")\n",
    "        .sample(0.5)\n",
    "    )\n",
    "\n",
    "    to_extend = addresses_df.join(unique_filtered_uuids, \"uuid_idt\")\n",
    "\n",
    "    extended_ds = (\n",
    "        to_extend.withColumn(\"tvia\", udf(select_via_type, StringType())(\"tvia\"))\n",
    "        .withColumn(\n",
    "            \"direccion\", udf(update_direction_via, StringType())(\"tvia\", \"direccion\")\n",
    "        )\n",
    "        .withColumn(\"direccion\", udf(switch_letters, StringType())(\"direccion\"))\n",
    "    )\n",
    "\n",
    "    extended_ds.show(truncate=False)\n",
    "    print(extended_ds.count(), addresses_df.count())\n",
    "    addresses_df = addresses_df.union(extended_ds)\n",
    "    print(addresses_df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frecuencia de uuid_idt después"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uuid_frecuency(addresses_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtro\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con el fin de reducir el problema, se aplica un filtro por el municipio con más direccioens asociadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MUNICIPIO = \"SAN CRISTÓBAL DE LA LAGUNA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MUNICIPIO:\n",
    "    filtered_uuids = (\n",
    "        addresses_df.groupBy(\"uuid_idt\")\n",
    "        .count()\n",
    "        .filter(f\"count >= {MIN_FRENCUENCY}\")\n",
    "        .select(\"uuid_idt\")\n",
    "    )\n",
    "    addresses_df = addresses_df.join(filtered_uuids, \"uuid_idt\").filter(\n",
    "        f\"nommun == '{MUNICIPIO}'\"\n",
    "    )\n",
    "addresses_df = addresses_df.select(\"uuid_idt\", \"latitud\", \"longitud\", \"direccion\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## División de dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener los uuids únicos y convertirlos a una lista de Python\n",
    "unique_uuids = [row[0] for row in addresses_df.select(\"uuid_idt\").distinct().collect()]\n",
    "\n",
    "# Crear DataFrame del 80%\n",
    "train_df = addresses_df.sampleBy(\n",
    "    \"uuid_idt\", fractions={uuid: 0.8 for uuid in unique_uuids}, seed=42\n",
    ")\n",
    "\n",
    "# Crear DataFrame del porcentaje restante restando el DataFrame del 80% al original\n",
    "test_df = addresses_df.subtract(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(addresses_df.count())\n",
    "print(train_df.count())\n",
    "print(test_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(addresses_df.select(\"uuid_idt\").distinct().count())\n",
    "print(train_df.select(\"uuid_idt\").distinct().count())\n",
    "print(test_df.select(\"uuid_idt\").distinct().count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uuid_frecuency(train_df)\n",
    "uuid_frecuency(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representación gráfica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"direccion\", outputCol=\"words\")\n",
    "tokenized_sentences = tokenizer.transform(addresses_df).select(\"words\").collect()\n",
    "tokenized_sentences = [sentence.words for sentence in tokenized_sentences]\n",
    "print(tokenized_sentences[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counter = dict()\n",
    "for sentence in tokenized_sentences:\n",
    "    for word in sentence:\n",
    "        if word not in word_counter:\n",
    "            word_counter[word] = 1\n",
    "        else:\n",
    "            word_counter[word] += 1\n",
    "\n",
    "word_counter = spark.createDataFrame(word_counter.items(), [\"word\", \"count\"]).orderBy(\n",
    "    \"count\", ascending=False\n",
    ")\n",
    "\n",
    "# Creo la columa frecuencia\n",
    "word_counter = word_counter.withColumn(\n",
    "    \"frequency\", word_counter[\"count\"] / word_counter.count()\n",
    ")\n",
    "word_counter.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot\n",
    "\n",
    "# Imprimir las 20 palabras más frecuentes\n",
    "word_counter.toPandas().head(15).plot.bar(x=\"word\", y=\"frequency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representación con embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representación mediante _Word2Vec_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Word2Vec, Tokenizer\n",
    "\n",
    "# Tokenizar el texto\n",
    "tokenizer = Tokenizer(inputCol=\"direccion\", outputCol=\"words\")\n",
    "\n",
    "train_tokens = tokenizer.transform(train_df)\n",
    "test_tokens = tokenizer.transform(test_df)\n",
    "\n",
    "# Entrenar el modelo Word2Vec\n",
    "word2Vec = Word2Vec(vectorSize=200, minCount=0, inputCol=\"words\", outputCol=\"embedding\")\n",
    "word2vec_model = word2Vec.fit(train_tokens)\n",
    "\n",
    "train_embeddings_word2vec  = word2vec_model.transform(train_tokens)\n",
    "test_embeddings_word2vec  = word2vec_model.transform(test_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representación con _GPT_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lectura de la variable de entorno donde se almacena la API key de OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not api_key:\n",
    "    raise ValueError(\"No API key found in environment variables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import ArrayType, FloatType\n",
    "from pyspark.sql.functions import udf\n",
    "from openai import OpenAI\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def normalize_l2(x):\n",
    "    x = np.array(x)\n",
    "    if x.ndim == 1:\n",
    "        norm = np.linalg.norm(x)\n",
    "        if norm == 0:\n",
    "            return x.tolist()  # Convertir a lista si es un vector unidimensional\n",
    "        return (x / norm).tolist()\n",
    "    else:\n",
    "        norm = np.linalg.norm(x, 2, axis=1, keepdims=True)\n",
    "        return (x / norm).tolist()\n",
    "    \n",
    "\n",
    "def get_embedding(text, model=\"text-embedding-3-small\"):\n",
    "    client = OpenAI(api_key=api_key)\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    result = client.embeddings.create(input=[text], model=model).data[0].embedding[:256]\n",
    "    return normalize_l2(result)\n",
    "\n",
    "\n",
    "# Definir la función UDF y especificar el tipo de dato de retorno como ArrayType(FloatType())\n",
    "embedding_udf = udf(lambda text: get_embedding(text), ArrayType(FloatType()))\n",
    "\n",
    "test_embeddings_gpt = test_df.withColumn(\"embedding\", embedding_udf(\"direccion\"))\n",
    "train_embeddings_gpt = train_df.withColumn(\"embedding\", embedding_udf(\"direccion\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representación con _MiniLM_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import ArrayType, FloatType\n",
    "from pyspark.sql.functions import udf\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "\n",
    "@udf(returnType=ArrayType(FloatType()))\n",
    "def minilm_embedding_list(direccion: str):\n",
    "    \"\"\"Calcula el embedding de la direccion dada y lo convierte a una lista para evitar la serelización de Spark.\n",
    "\n",
    "    Args:\n",
    "        direccion: Direction to be represeted by the embedding model.\n",
    "\n",
    "    Returns:\n",
    "        A list with de embedding of the direction\n",
    "    \"\"\"\n",
    "    embedding = model.encode(direccion)\n",
    "    return embedding.tolist()\n",
    "\n",
    "\n",
    "train_embeddings_minilm = train_df.withColumn(\n",
    "    \"embedding\", minilm_embedding_list(\"direccion\")\n",
    ")\n",
    "test_embeddings_minilm = test_df.withColumn(\n",
    "    \"embedding\", minilm_embedding_list(\"direccion\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similitud de direcciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distancia del coseno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def cosine_similarity(v1, v2):\n",
    "    v1_np = np.array(v1)\n",
    "    v2_np = np.array(v2)\n",
    "    cos_sim = np.dot(v1_np, v2_np) / (np.linalg.norm(v1_np) * np.linalg.norm(v2_np))\n",
    "    return float(cos_sim)\n",
    "\n",
    "\n",
    "cosine_similarity_udf = udf(cosine_similarity, DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col, row_number, expr, max as spark_max\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "\n",
    "def get_top_matches(\n",
    "    test_df: DataFrame,\n",
    "    train_df: DataFrame,\n",
    "    embedding_col: str,\n",
    "    address_col: str,\n",
    "    top_n: int = 3,\n",
    ") -> DataFrame:\n",
    "    \"\"\"Return the top N results of the cosine similarity computation.\n",
    "    \n",
    "    Args:\n",
    "        test_df: DataFrame containing the addresses to evaluate.\n",
    "        train_df: DataFrame containing the addresses used for training.\n",
    "        embedding_col: Name of the column containing the precomputed embeddings.\n",
    "        address_col: Name of the column containing the addresses.\n",
    "        top_n: Number of top results to return for each address in test_df.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame containing the addresses from test_df and their top N matches from train_df based on cosine\n",
    "    \"\"\"\n",
    "    test_df = test_df.alias(\"test_df\")\n",
    "    train_df = train_df.alias(\"train_df\")\n",
    "\n",
    "    cross_df = test_df.crossJoin(train_df)\n",
    "    cross_df = cross_df.withColumn(\n",
    "        \"cosine_similarity\",\n",
    "        cosine_similarity_udf(\n",
    "            col(f\"test_df.{embedding_col}\"), col(f\"train_df.{embedding_col}\")\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    window = Window.partitionBy(col(f\"test_df.{address_col}\")).orderBy(\n",
    "        col(\"cosine_similarity\").desc()\n",
    "    )\n",
    "\n",
    "    cross_df = cross_df.withColumn(\"rank\", row_number().over(window))\n",
    "    top_n_df = cross_df.filter(col(\"rank\") <= top_n)\n",
    "    return top_n_df\n",
    "\n",
    "\n",
    "def evaluate_and_count_matches(\n",
    "    top_n_df: DataFrame, test_id_col: str, train_id_col: str, address_col: str\n",
    ") -> DataFrame:\n",
    "    \"\"\"Evaluate the result of a pair of test direction data and the best direction result.\n",
    "    \n",
    "    Given a Dataframe with set of rows with the test direction and a result direction, \n",
    "    confirm if it is a match or not creating a new column 'evaluation'.\n",
    "    \n",
    "    Args:\n",
    "        top_n_df:\n",
    "        test_id_col:\n",
    "        train_id_col:\n",
    "        address_col:\n",
    "\n",
    "    Returns: The number of matches\n",
    "    \"\"\"\n",
    "    top_n_df = top_n_df.withColumn(\"evaluation\", col(test_id_col) == col(train_id_col))\n",
    "    matches_by_address = top_n_df.groupBy(f\"test_df.{address_col}\").agg(\n",
    "        spark_max(expr(\"case when evaluation = true then 1 else 0 end\")).alias(\n",
    "            \"has_match\"\n",
    "        )\n",
    "    )\n",
    "    matches = matches_by_address.filter(\"has_match = 1\")\n",
    "    return matches.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_embeddings_word2vec.count())\n",
    "print(test_embeddings_word2vec.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener las mejores coincidencias\n",
    "best_results_df = get_top_matches(\n",
    "    test_embeddings_word2vec,\n",
    "    train_embeddings_word2vec,\n",
    "    \"embedding\",\n",
    "    \"direccion\",\n",
    "    top_n=3,\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Tiempo de ejecución: {elapsed_time} segundos\")\n",
    "\n",
    "matches_count = evaluate_and_count_matches(\n",
    "    best_results_df, \"test_df.uuid_idt\", \"train_df.uuid_idt\", \"direccion\"\n",
    ")\n",
    "print(f\"Number of addresses with matches: {matches_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MiniLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_results_df = get_top_matches(\n",
    "    test_embeddings_minilm,\n",
    "    train_embeddings_minilm,\n",
    "    \"embedding\",\n",
    "    \"direccion\",\n",
    "    top_n=3,\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Tiempo de ejecución: {elapsed_time} segundos\")\n",
    "\n",
    "matches_count = evaluate_and_count_matches(\n",
    "    best_results_df, \"test_df.uuid_idt\", \"train_df.uuid_idt\", \"direccion\"\n",
    ")\n",
    "print(f\"Number of addresses with matches: {matches_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_results_df = get_top_matches(\n",
    "    test_embeddings_gpt,\n",
    "    train_embeddings_gpt,\n",
    "    \"embedding\",\n",
    "    \"direccion\",\n",
    "    top_n=3,\n",
    ")\n",
    "matches_count = evaluate_and_count_matches(\n",
    "    best_results_df, \"test_df.uuid_idt\", \"train_df.uuid_idt\", \"direccion\"\n",
    ")\n",
    "print(f\"Number of addresses with matches: {matches_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similitud de direcciones con Chroma DB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "\n",
    "chroma_client = chromadb.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import DenseVector\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "\n",
    "def extract_to_chroma(dataframe: DataFrame, type_df: str):\n",
    "    \"\"\"Extrac the uuids, directions and embeddings of each row of a dataframe.\n",
    "\n",
    "    Args:\n",
    "        dataframe: Dataframe with the data.\n",
    "        type_df: Type of the dataframe. Train or test.\n",
    "\n",
    "    Returns:\n",
    "        embeddings: List with the numeric representation of each direction.\n",
    "        documents: List that contains the directions.\n",
    "        metadata: List with relevant information about the direction\n",
    "          (uuid, latitud, longitud, type_df).\n",
    "        ids: List with as many ids as addresses.\n",
    "    \"\"\"\n",
    "    ids = []\n",
    "    documents = []\n",
    "    embeddings = []\n",
    "    metadata = []\n",
    "\n",
    "    # Recopilar los datos del DataFrame\n",
    "    rows = dataframe.select(\n",
    "        \"uuid_idt\", \"latitud\", \"longitud\", \"direccion\", \"embedding\"\n",
    "    ).collect()\n",
    "    index = 0\n",
    "\n",
    "    for row in rows:\n",
    "        # Extraer los datos de cada fila\n",
    "        uuid_idt = str(row.uuid_idt)\n",
    "        direccion = str(row.direccion)\n",
    "        embedding = (\n",
    "            row.embedding.toArray().tolist()\n",
    "            if isinstance(row.embedding, DenseVector)\n",
    "            else row.embedding\n",
    "        )\n",
    "\n",
    "        ids.append(str(index))\n",
    "        documents.append(direccion)\n",
    "        embeddings.append(embedding)\n",
    "        metadata.append(\n",
    "            {\n",
    "                \"uuid_idt\": uuid_idt,\n",
    "                \"latitud\": float(row.latitud),\n",
    "                \"longitud\": float(row.longitud),\n",
    "                \"type_df\": type_df,\n",
    "            }\n",
    "        )\n",
    "        index += 1\n",
    "    return embeddings, documents, metadata, ids\n",
    "\n",
    "\n",
    "def extract_data(\n",
    "    test_result_collection: dict, train_result_collection: dict, dataframe\n",
    "):\n",
    "    \"\"\"Extract data from the test and train result collections to a dataframe.\n",
    "\n",
    "    Args:\n",
    "        test_result_collection (dict): The result collection from the test data.\n",
    "        train_result_collection (dict): The result collection from the train data.\n",
    "        dataframe (DataFrame): The DataFrame to write the extracted data.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A DataFrame containing the extracted data with the following columns:\n",
    "            - uuid_idt_test\n",
    "            - latitud_test\n",
    "            - longitud_test\n",
    "            - direccion_test\n",
    "            - uuid_idt_train\n",
    "            - latitud_train\n",
    "            - longitud_train\n",
    "            - direccion_train\n",
    "    \"\"\"\n",
    "    test_metadata = test_result_collection[\"metadatas\"][0]\n",
    "    uuid_test_result = test_metadata[\"uuid_idt\"]\n",
    "    latitud_test_result = test_metadata[\"latitud\"]\n",
    "    longitud_test_result = test_metadata[\"longitud\"]\n",
    "    direccion_test_result = test_result_collection[\"documents\"][0]\n",
    "    test_data = (\n",
    "        uuid_test_result,\n",
    "        latitud_test_result,\n",
    "        longitud_test_result,\n",
    "        direccion_test_result,\n",
    "    )\n",
    "\n",
    "    # Datos de entrenamiento\n",
    "    train_metadata = train_result_collection[\"metadatas\"][0]\n",
    "    train_data = [\n",
    "        (metadata[\"uuid_idt\"], metadata[\"latitud\"], metadata[\"longitud\"], direccion)\n",
    "        for metadata, direccion in zip(\n",
    "            train_metadata, train_result_collection[\"documents\"][0]\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # Crear DataFrames temporales y unirlos\n",
    "    temp_data = [(test_data + train_row) for train_row in train_data]\n",
    "    temp_df = spark.createDataFrame(temp_data, schema=schema)\n",
    "    dataframe = dataframe.union(temp_df)\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "def evaluation_collections(\n",
    "    collection_test: chromadb.api.models.Collection.Collection,\n",
    "    collection_train: chromadb.api.models.Collection.Collection,\n",
    "):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        collection_test: Collection with the data to search for similar.\n",
    "        collection_train: Collection to query the n best embeddings.\n",
    "\n",
    "    Returns: \n",
    "        match: Number of hits.\n",
    "        no_match: Numbers of no hits.\n",
    "        evaluated_dirs: Dataframe with the made comparations.\n",
    "    \"\"\"\n",
    "    evaluated_dirs = spark.createDataFrame([], schema)\n",
    "\n",
    "    counter = 0\n",
    "    match = 0\n",
    "    no_match = 0\n",
    "    while counter < collection_test.count():\n",
    "        data_to_evaluate = collection_test.get(\n",
    "            ids=[str(counter)], include=[\"embeddings\", \"metadatas\", \"documents\"]\n",
    "        )\n",
    "\n",
    "        embedding_to_evaluate = data_to_evaluate[\"embeddings\"][0]\n",
    "        uuid_to_evaluate = data_to_evaluate[\"metadatas\"][0][\"uuid_idt\"]\n",
    "\n",
    "        best_similarities = collection_train.query(\n",
    "            query_embeddings=[embedding_to_evaluate], n_results=3\n",
    "        )\n",
    "\n",
    "        best_uuids = [u[\"uuid_idt\"] for i in best_similarities[\"metadatas\"] for u in i]\n",
    "\n",
    "        if uuid_to_evaluate in best_uuids:\n",
    "            match += 1\n",
    "        else:\n",
    "            no_match += 1\n",
    "            evaluated_dirs = extract_data(data_to_evaluate, best_similarities, evaluated_dirs)\n",
    "        counter += 1\n",
    "    return match, no_match, evaluated_dirs\n",
    "\n",
    "\n",
    "def chunk_data(data, chunk_size: int):\n",
    "    \"\"\"\n",
    "    Divide a list of data into batches of a specific size.\n",
    "\n",
    "    Args:\n",
    "        data: The list of data to be divided into batches.\n",
    "        chunk_size : The size of each batch.\n",
    "\n",
    "    Yields:\n",
    "        list: A batch of data of size `chunk_size`.\n",
    "    \"\"\"\n",
    "    for i in range(0, len(data), chunk_size):\n",
    "        yield data[i : i + chunk_size]\n",
    "\n",
    "\n",
    "def add_to_collection(\n",
    "    collection: chromadb.api.models.Collection.Collection, \n",
    "    embeddings_list: list, \n",
    "    documents_list: list, \n",
    "    metadata_list: list, \n",
    "    ids_list: list\n",
    "):\n",
    "    \"\"\"\n",
    "    Add elements to a collection.\n",
    "\n",
    "    Args:\n",
    "        collection: The collection to which the elements will be added.\n",
    "        embeddings_list (list): A list of embeddings.\n",
    "        documents_list (list): A list of documents.\n",
    "        metadata_list (list): A list of metadata.\n",
    "        ids_list (list): A list of IDs.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    batch_size = 41666\n",
    "    embeddings_chunks = list(chunk_data(embeddings_list, batch_size))\n",
    "    documents_chunks = list(chunk_data(documents_list, batch_size))\n",
    "    metadata_chunks = list(chunk_data(metadata_list, batch_size))\n",
    "    ids_chunks = list(chunk_data(ids_list, batch_size))\n",
    "\n",
    "    # Agrega cada lote por separado a la colección\n",
    "    for embeddings, documents, metadata, ids in zip(\n",
    "        embeddings_chunks, documents_chunks, metadata_chunks, ids_chunks\n",
    "    ):\n",
    "        collection.add(\n",
    "            embeddings=embeddings, documents=documents, metadatas=metadata, ids=ids\n",
    "        )\n",
    "\n",
    "\n",
    "schema = \"uuid_idt_test: string, \\\n",
    "        latitud_test: float, \\\n",
    "        longitud_test: float, \\\n",
    "        direccion_test: string, \\\n",
    "        uuid_idt_train: string, \\\n",
    "        latitud_train: float, \\\n",
    "        longitud_train: float, \\\n",
    "        direccion_train: string\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_list, documents_list, embeddings_list, metadata_list = extract_to_chroma(\n",
    "    test_embeddings_word2vec, \"test\"\n",
    ")\n",
    "collection_test_w2v = chroma_client.create_collection(name=\"test_df_w2v\")\n",
    "add_to_collection(\n",
    "    collection_test_w2v, ids_list, documents_list, embeddings_list, metadata_list\n",
    ")\n",
    "\n",
    "ids_list, documents_list, embeddings_list, metadata_list = extract_to_chroma(\n",
    "    train_embeddings_word2vec, \"train\"\n",
    ")\n",
    "collection_train_w2v = chroma_client.create_collection(name=\"train_df_w2v\")\n",
    "add_to_collection(\n",
    "    collection_train_w2v, ids_list, documents_list, embeddings_list, metadata_list\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Tiempo de ejecución: {elapsed_time} segundos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_count, no_matches_count, best_results_df = evaluation_collections(\n",
    "    collection_test_w2v, \n",
    "    collection_train_w2v, \n",
    ")\n",
    "print(matches_count, no_matches_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_client.delete_collection(\"test_df\")\n",
    "chroma_client.delete_collection(\"train_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_test_gpt = chroma_client.create_collection(name=\"test_df\")\n",
    "collection_train_gpt = chroma_client.create_collection(name=\"train_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_list, documents_list, embeddings_list, metadata_list = extract_to_chroma(\n",
    "    test_embeddings_gpt, \"test\"\n",
    ")\n",
    "add_to_collection(\n",
    "    collection_test_gpt, ids_list, documents_list, embeddings_list, metadata_list\n",
    ")\n",
    "\n",
    "ids_list, documents_list, embeddings_list, metadata_list = extract_to_chroma(\n",
    "    train_embeddings_gpt, \"train\"\n",
    ")\n",
    "add_to_collection(\n",
    "    collection_train_gpt, ids_list, documents_list, embeddings_list, metadata_list\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_count, no_matches_count, best_results_df = evaluation_collections(\n",
    "    collection_test_gpt, \n",
    "    collection_train_gpt, \n",
    ")\n",
    "print(matches_count, no_matches_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MiniLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_client.delete_collection(\"test_df_minilm\")\n",
    "chroma_client.delete_collection(\"train_df_minilm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_test_minilm = chroma_client.create_collection(name=\"test_df_minilm\")\n",
    "collection_train_minilm = chroma_client.create_collection(name=\"train_df_minilm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_list, documents_list, metadata_list, ids_list = extract_to_chroma(\n",
    "    test_embeddings_minilm, \"test\"\n",
    ")\n",
    "add_to_collection(\n",
    "    collection_test_minilm, embeddings_list, documents_list, metadata_list, ids_list\n",
    ")\n",
    "\n",
    "embeddings_list, documents_list, metadata_list, ids_list = extract_to_chroma(\n",
    "    train_embeddings_minilm, \"train\"\n",
    ")\n",
    "add_to_collection(\n",
    "    collection_train_minilm, embeddings_list, documents_list, metadata_list, ids_list\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Tiempo de ejecución: {elapsed_time} segundos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_count, no_matches_count, best_results_df = evaluation_collections(\n",
    "    collection_test_minilm, \n",
    "    collection_train_minilm\n",
    ")\n",
    "print(matches_count, no_matches_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(f\"Tiempo de ejecución: {elapsed_time} segundos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se implementa la distancia de Haversine, por la cual se podrá saber que tan lejos está la dirección de evaluación con respecto a respecto a la media de las N mejroes direcciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelo = 'Word2Vec'\n",
    "# modelo = 'GTP'\n",
    "modelo = \"MiniLM\"\n",
    "\n",
    "# tecnology = 'Spark'\n",
    "tecnology = \"Chroma DB\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, udf\n",
    "from haversine import haversine\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "haversine_udf = udf(\n",
    "    lambda lat1, lon1, lat2, lon2: haversine((lat1, lon1), (lat2, lon2)),\n",
    "    returnType=DoubleType(),\n",
    ")\n",
    "\n",
    "\n",
    "if tecnology == \"Chroma DB\":\n",
    "    evaluated_dirs = best_results_df.withColumn(\n",
    "        \"haversine_distance\",\n",
    "        haversine_udf(\n",
    "            col(\"latitud_test\"),\n",
    "            col(\"longitud_test\"),\n",
    "            col(\"latitud_train\"),\n",
    "            col(\"longitud_train\"),\n",
    "        ),\n",
    "    )\n",
    "elif tecnology == \"Spark\":\n",
    "    evaluated_dirs = best_results_df.withColumn(\n",
    "        \"haversine_distance\",\n",
    "        haversine_udf(\n",
    "            col(\"test_df.latitud\"),\n",
    "            col(\"test_df.longitud\"),\n",
    "            col(\"train_df.latitud\"),\n",
    "            col(\"train_df.longitud\"),\n",
    "        ),\n",
    "    )\n",
    "else:\n",
    "    print(\"Error, check the tecnology used\")\n",
    "    \n",
    "evaluated_dirs = evaluated_dirs.repartition(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exporta el dataframe con la evaluación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluated_dirs.coalesce(1).write.csv(\"../data/proccesed_data/.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "mean_value = 0\n",
    "mean_value = round(evaluated_dirs.agg(avg(\"haversine_distance\")).collect()[0][0], 2)\n",
    "max_value = round(evaluated_dirs.agg({\"haversine_distance\": \"max\"}).collect()[0][0], 2)\n",
    "hit_rate = round(matches_count / test_df.count(), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "\n",
    "def write_model_results(cabecera: list, data: list):\n",
    "    \"\"\"Create or write the data into a csv file.\"\"\"\n",
    "    csv_file_path = f\"../data/proccesed_data/results.csv\"\n",
    "    escribir_cabecera = not os.path.exists(csv_file_path)\n",
    "\n",
    "    with open(csv_file_path, mode=\"a\", newline=\"\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        if escribir_cabecera:\n",
    "            writer.writerow(cabecera)\n",
    "        writer.writerows(data)\n",
    "\n",
    "    print(f\"CSV file '{csv_file_path}' created successfully.\")\n",
    "\n",
    "\n",
    "cabecera = [\n",
    "    \"Model\",\n",
    "    \"Tecnology\",\n",
    "    \"Municipaly\",\n",
    "    \"Train dataset size\",\n",
    "    \"Test dataframe size\",\n",
    "    \"Aumented data\",\n",
    "    \"Frecuency\",\n",
    "    \"Hit rate\",\n",
    "    \"Mean Haversine distance\",\n",
    "    \"Max Haversine distance\",\n",
    "]\n",
    "\n",
    "data = [\n",
    "    [\n",
    "        modelo,\n",
    "        tecnology,\n",
    "        MUNICIPIO,\n",
    "        train_df.count(),\n",
    "        test_df.count(),\n",
    "        AUMENTED_DATA,\n",
    "        MIN_FRENCUENCY,\n",
    "        hit_rate,\n",
    "        mean_value,\n",
    "        max_value,\n",
    "    ]\n",
    "]\n",
    "\n",
    "write_model_results(cabecera, data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
